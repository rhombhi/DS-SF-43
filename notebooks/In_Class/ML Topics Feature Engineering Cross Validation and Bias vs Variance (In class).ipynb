{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Topics: Feature Engineering, Cross Validation, and Bias vs Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "- <b>Feature engineering</b>: transform a dirty dataset into a machine learning ready dataset.\n",
    "- Learn the importance of <b>cross validation</b>: why and how it's used.\n",
    "- The <b>bias vs variance</b> trade-off, aka the eternal dilemma of machine learning.\n",
    "- Make and interpret learning and validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering.\n",
    "[\"Feature engineering is the process of using domain knowledge of the data to create features \n",
    "that make machine learning algorithms work\"](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "\n",
    "We are creating new features from old ones.\n",
    "<br><br>\n",
    "Our job: transform the [titanic dataset](https://www.kaggle.com/c/titanic) into one that can be used for machine learning, specifically predicting whether or not a passenger survives the titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the the dataset\n",
    "\n",
    "path = \"../../data/titanic.csv\"\n",
    "\n",
    "titanic = \n",
    "\n",
    "#lowercase column names\n",
    "\n",
    "\n",
    "#Set passengerid column as index\n",
    "\n",
    "\n",
    "#view data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data dictionary:</b>\n",
    "\n",
    "PassengerID: A column added by Kaggle to identify each row and make submissions easier\n",
    "\n",
    "Survived: Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n",
    "\n",
    "Pclass:\tThe class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n",
    "\n",
    "Sex: The passenger’s sex\n",
    "\n",
    "Age: The passenger’s age in years\n",
    "\n",
    "SibSp: The number of siblings or spouses the passenger had aboard the Titanic\n",
    "\n",
    "Parch: The number of parents or children the passenger had aboard the Titanic\n",
    "\n",
    "Ticket: The passenger’s ticket number\n",
    "\n",
    "Fare: The fare the passenger paid\n",
    "\n",
    "Cabin: The passenger’s cabin number\n",
    "\n",
    "Embarked— The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspection time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three columns will null values, what should we do with them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up: fixing the age column.\n",
    "\n",
    "We're not going to drop it because we don't want to reduce the size of the dataset by a significant amount. We're going to use a technique called \"[imputation](https://machinelearningmastery.com/handle-missing-data-python/)\" to get around this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the null values of the age column using the median age.\n",
    "\n",
    "\n",
    "#Confirm there are no nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about cabin? It gets the drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the cabin column from the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View uniques\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution is to drop rows that null because there are only two null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see which columns has nulls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with null values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's at the details of the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .info() on titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've dealt with the missing data issue, now we need to handle the text data. Our objective here is turn words into numbers.\n",
    "\n",
    "What do you think that means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First order of business, deciding which of the string/object columns to keep and drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of object dtype columns\n",
    "object_columns = [\"name\", \"sex\", \"ticket\", \"embarked\"]\n",
    "\n",
    "#Look at titanic data with just the columns in object_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which ones do we drop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name and ticket get the ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop name and ticket columns from the data\n",
    "\n",
    "\n",
    "\n",
    "#View data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we now have two string columns in sex and embarked. Let's turn them in numbers by making dummy variables.\n",
    "<br><br>\n",
    "1. Convert male to 0 and female to 1 in the sex column.\n",
    "2. Make dummy variables from the embarked column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary where the keys are male and female and the values are 0 and 1\n",
    "gender_dict =\n",
    "\n",
    "\n",
    "#Map dictionary onto the sex column and reassign it to sex.\n",
    "\n",
    "titanic[\"sex\"] ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.get_dummies to make dummy variables from the embarked column\n",
    "\n",
    "#Pass embarked column, then set prefix to \"emb\" and call .head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the issue here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curse of dimensionality!\n",
    "\n",
    "We don't need all three columns. We didn't make a separate column for male and female, so why should we do that for C, Q, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make dummy variables from the embarked column, but this time set drop_first = True\n",
    "#Assign dummies to variable called emb_dums\n",
    "\n",
    "\n",
    "\n",
    "#Look at emb_dums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine this dataframe of dummy variables with our original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Drop embarked column\n",
    "\n",
    "\n",
    "#2. Concatenate the titanic and emb_dums dataframes and overwrite titanic variable\n",
    "\n",
    "\n",
    "#3. View new concatenated dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if all variables are numeric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Our dataset is now ready for machine learning.\n",
    "<br><br>\n",
    "\n",
    "But time for a quick exercise. Write a function to that takes an uncleaned version of the titantic dataset, applys the feature engineering techniques we used above, and outputs a clean machine learning ready dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function goes here\n",
    "def titanic_fe(df):\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to see if function on reimported titanic dataset\n",
    "\n",
    "titanic2 = pd.read_csv(\"../data/titanic.csv\")\n",
    "\n",
    "#Pass in titanic2 into titanti_fe function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to do some machine learning but first let's discuss the null accuracy\n",
    "<br><br>\n",
    "The null accuracy aka the bench mark of our model's performance. It is the maximum percentage of the target variable distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .value_counts(normalize=True) on survived column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null accuracy is 61.75%. That means we have to create a model that classifies the data at a better rate than 61.75%.\n",
    "\n",
    "If we didn't build a model and just said everyone died, then we'd be 61.75% without even going through the trouble of building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test sets and cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import train_test_split and cross_val_score functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split our titanic dataset into two sets: training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First extract features and target variables\n",
    "\n",
    "X = \n",
    "y = \n",
    "\n",
    "#Input X and y into the train_test_split function, set test_size to .25, random_state = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X_train = The features of the data we use to fit the model\n",
    "\n",
    "- X_test = The features of the data we use to make and test predictions with\n",
    "\n",
    "- y_train = The target variable of the data we use to fit the model\n",
    "\n",
    "- y_test = The target variable of the data we use to make and test predicitions with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a decision tree model on X_train and y_train. Do not specify max_depth\n",
    "\n",
    "\n",
    "\n",
    "#Evaluate the model by scoring the X_train and y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We got a high score! Or did we????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Huge drop in accuracy score. How come?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring back to the model plotting function for the purpose of visualizing an overfit model against a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some fake data again\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#Generate fake data that is 400 x 2.\n",
    "data = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.20, random_state = 34)\n",
    "#Assign features to XX\n",
    "\n",
    "#Assign target variable to yy\n",
    "\n",
    "\n",
    "#Set style and size\n",
    "\n",
    "\n",
    "\n",
    "#Plot features and use to yy to color-encode,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split on XX and yy\n",
    "\n",
    "\n",
    "#fit model on the training set \n",
    "\n",
    "\n",
    "#Evaluate model on training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Perfect model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in plot_decision_boundary function\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, alpha=0.4, s = 50, cmap=\"rainbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the model and the testing data\n",
    "\n",
    "#Pass in pre-trained model that was trained on the training set\n",
    "#Pass in the testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that look to you? Where in the plot is the model overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see how well the model classifies the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model on testing data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that whole process again to see if we get different scores\n",
    "\n",
    "<br><br>\n",
    "Run this code several times and observe the changes in the testing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different train/test split but with no random_state set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25)\n",
    "\n",
    "#Fit model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "training_score = model.score(X_train, y_train)\n",
    "\n",
    "print (\"The training score is {:.3f} percent\".format(training_score*100))\n",
    "\n",
    "testing_score = model.score(X_test, y_test)\n",
    "print (\"The testing score is {:.3f}\".format(testing_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make this a for loop\n",
    "\n",
    "#Intialize list that we'll use for our testing scores\n",
    "testscorelist = []\n",
    "\n",
    "#Iterate over range 10\n",
    "\n",
    "testscorelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "<br><br>\n",
    "\n",
    "\"Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\" \n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<b>K-Fold Cross Validation</b>\n",
    "![Image](https://i.stack.imgur.com/1fXzJ.png)\n",
    "\n",
    "<br><br>\n",
    "\"[In K Fold cross validation](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f), the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing’s fixed and it can take any value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use cross_val_score function to perform KFold cross validation five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call cross_val_score, input empty DT model, X, y, set cv = 5 and scoring = accuracy\n",
    "cv_scores = \n",
    "\n",
    "#Call cv_scores\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see theres a degree of variance in the output, which makes deriving the mean crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the average score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Class exercise:</b>\n",
    "\n",
    "Test to see the relationship between max_depth and the average cv_score. What happens when you increase or decrease max_depth. \n",
    "\n",
    "Whats you're done playing around with that, then make a line plot of depth values from 1 - 20 and the average cross validated score for each corresponding depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best depth value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model with the best depth value and evaluate it on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, \n",
    "                                                    test_size = .25,\n",
    "                                                   random_state = 42)\n",
    "#Fit model with depth 6 and random_state = 42\n",
    "\n",
    "\n",
    "#Score model on test set\n",
    "testscore = \n",
    "\n",
    "print (\"The test score is {:.3f} percent\".format(testscore*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compare to the null accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract null accuracy from testscore\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports confusion_matrix and accuracy score funcions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate accuracy_score using sklearn\n",
    "\n",
    "#Make predictions on test set\n",
    "preds = \n",
    "\n",
    "#Call accuracy_score on y_test and preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in y_test and preds into confusion_matrix function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best depth is one that is not too small but not too large.\n",
    "\n",
    "We need to find the depth that strikes the right balance between <b>bias</b> and <b>variance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias vs Variance\n",
    "<br><br>\n",
    "<b>Bias:</b> The simplifying assumptions made by the model to make the target function easier to approximate.\n",
    "\n",
    "<b>Variance:</b> The amount that the estimate of the target function will change given different training data.\n",
    "\n",
    "From: https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\n",
    "<br><br>\n",
    "[Legendary data science blog post](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<b>Bias error:</b> The difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Bias measures how far off in general these models' predictions are from the correct value.\n",
    "\n",
    "<b>Variance error:</b> The error due to variance is taken as the variability of a model prediction for a given data point. Imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphic illustration of bias vs variance:\n",
    "\n",
    "![b v v](https://i.stack.imgur.com/r7QFy.png)\n",
    "\n",
    "Credit: Scott Fortmann-Roe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see here? How would you interpret this graphic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Depicting bias vs variance with validation and learning curves</b>\n",
    "\n",
    "Validation Curve:\n",
    "\n",
    "![Lc](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "\n",
    "<br><br>\n",
    "Learning Curve:\n",
    "![lc](https://chrisalbon.com/images/machine_learning_flashcards/Learning_Curve_print.png)\n",
    "<br><br>\n",
    "[\"Graph that compares the performance of a model on training and testing data over a varying number of training instances\"](http://www.ritchieng.com/machinelearning-learning-curve/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph 1: Plot validation curve of model complexity versus error rates for training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25,\n",
    "                                                    random_state = 38)\n",
    "\n",
    "#2. Initialize lists of errors for train and test sets\n",
    "\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "#3. Set range of depth values from 1 to 20\n",
    "depths = range(1,21)\n",
    "#4a. Iterate over depth values.\n",
    "#4b. Fit a DT model for each depth model.\n",
    "#4c. Evaluate the model on both the train and test sets.\n",
    "#4d. Append scores to train_errors and test_errors\n",
    "\n",
    "\n",
    "    \n",
    "#5. Make two line plots. Plot depths vs train_errors and plot depths vs test_errors\n",
    "#Give them different colors and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to validation plot code from Chris Albon: https://chrisalbon.com/machine_learning/model_evaluation/plot_the_validation_curve/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph 2: Plot learning curve of training sizes vs training and testing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Chris Albon\n",
    "\n",
    "#1. Import learning_curve from sklearn\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#2. Create CV training and test scores for various training set sizes\n",
    "#Use max_depth = 5 for DT model\n",
    "train_sizes, train_scores, test_scores = learning_curve(DecisionTreeClassifier(max_depth=5), \n",
    "                                                        X, \n",
    "                                                        y,\n",
    "                                                        # Number of folds in cross-validation\n",
    "                                                        cv=5,\n",
    "                                                        # Evaluation metric\n",
    "                                                        scoring='accuracy', \n",
    "                                                        # 30 different sizes of the training set\n",
    "                                                        train_sizes=np.linspace(0.01, 1.0, 30))\n",
    "\n",
    "#3.Train and test_scores are 30x5. We need to compute average of each 5-fold cv\n",
    "train_scores = train_scores.mean(axis =1)\n",
    "test_scores = test_scores.mean(axis = 1)\n",
    "\n",
    "#4. Draw lines\n",
    "plt.plot(train_sizes, train_scores, color=\"r\",  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores, color=\"g\", label=\"Testing score\")\n",
    "\n",
    "#5. Create plot\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training Set Size\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.ylim(0.6, 1.1)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus!!\n",
    "<br><br>\n",
    "Let's see the most important features and visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit DT model on X and y with max_depth 4\n",
    "\n",
    "dt = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets put that in a dataframe\n",
    "fi = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "#Export the decision tree graph viz object. We have to export and the re-import it\n",
    "export_graphviz(dt, out_file='titanic.dot', \n",
    "                    feature_names=X.columns, \n",
    "                    class_names=[\"dead\", \"alive\"])\n",
    "with open(\"titanic.dot\") as f: \n",
    "        dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class work \n",
    "\n",
    "For the rest of class, work on improving your model as much as possible. \n",
    "\n",
    "- See what happens when you drop different features\n",
    "- Try different combinations of them\n",
    "- Try making pclass into a dummy variables instead of a continuous one\n",
    "- Make predictions of \"fake passengers\". Input a bunch of features to see what happens.\n",
    "- Once you've made the best possible model, make some more validation and learning curves.\n",
    "- You're also welcome to try the iris dataset or the churn rate dataset as well or make your own data with sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "Bias vs variance:\n",
    "\n",
    "https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\n",
    "\n",
    "http://www.machinelearningtutorial.net/2017/01/26/the-bias-variance-tradeoff/\n",
    "\n",
    "https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Cross validation:\n",
    "\n",
    "https://stats.stackexchange.com/questions/1826/cross-validation-in-plain-english\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/\n",
    "\n",
    "https://www.openml.org/a/estimation-procedures/1\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Titanic dataset projects:\n",
    "\n",
    "https://www.kaggle.com/maielld1/titanic-dataquest-tutorial\n",
    "\n",
    "https://github.com/agconti/kaggle-titanic\n",
    "\n",
    "https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
