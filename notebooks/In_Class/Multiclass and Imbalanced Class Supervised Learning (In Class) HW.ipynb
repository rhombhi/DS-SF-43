{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Sklearn Tools and Multi-Class and Imbalanced Class Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Goals: </b>\n",
    "\n",
    "- Learn how to use an array of advanced tools in sklearn such as Pipelines, GridSearch, and more\n",
    "- Work on a supervised classification dataset with more than two classes, specifically the famous MNIST digits dataset.\n",
    "- Work on a supervised classification dataset with imbalanced classes, specifically the credit card fraud dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Sklearn tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in titanic data\n",
    "\n",
    "path = \"../../data/titanic.csv\"\n",
    "\n",
    "titanic = pd.read_csv(path)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding aka dummy variables with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One hot encoding:*** Transforming categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = titanic.drop(\"Survived\", axis = 1)\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train test split with the titanic data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use LabelEncoder to turn the object values into numbers. Instead turning each unique value into a column a la dummy variables, this tool returns a single column and replaces the objects/strings with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize LabelEncoder object\n",
    "\n",
    "le = LabelEncoder()\n",
    "# le.fit_transform(X_train.Sex)\n",
    "\n",
    "#Use le on the sex column\n",
    "# sex_encoded = le.transform(X_test.Sex)\n",
    "sex_encoded = le.fit_transform(X_train.Sex)\n",
    "sex_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns male and female into a 1s and 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using this is that we can use the LabelEncoder object (le) to transform other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass in the Sex column on the testing dataset into the le object\n",
    "\n",
    "le.transform(X_test.Sex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350      male\n",
       "176      male\n",
       "723      male\n",
       "404    female\n",
       "306    female\n",
       "665      male\n",
       "607      male\n",
       "151    female\n",
       "265      male\n",
       "865    female\n",
       "408      male\n",
       "78       male\n",
       "200      male\n",
       "40     female\n",
       "709      male\n",
       "785      male\n",
       "876      male\n",
       "64       male\n",
       "852    female\n",
       "429      male\n",
       "193      male\n",
       "437    female\n",
       "35       male\n",
       "654    female\n",
       "284      male\n",
       "101      male\n",
       "641    female\n",
       "744      male\n",
       "601      male\n",
       "56     female\n",
       "        ...  \n",
       "649    female\n",
       "499      male\n",
       "820    female\n",
       "431    female\n",
       "619      male\n",
       "708    female\n",
       "809    female\n",
       "5        male\n",
       "25     female\n",
       "468      male\n",
       "736    female\n",
       "314      male\n",
       "856    female\n",
       "553      male\n",
       "635    female\n",
       "53     female\n",
       "329    female\n",
       "175      male\n",
       "472    female\n",
       "880    female\n",
       "883      male\n",
       "427    female\n",
       "1      female\n",
       "346    female\n",
       "327    female\n",
       "411      male\n",
       "330    female\n",
       "558    female\n",
       "799    female\n",
       "33       male\n",
       "Name: Sex, Length: 295, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roahuja/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "X_train.Embarked.fillna(\"unknown\", inplace=True)\n",
    "X_test.Embarked.fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try this on the Embarked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 2, 0, 0, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0,\n",
       "       2, 2, 2, 1, 2, 0, 2, 2, 2, 1, 0, 0, 0, 2, 2, 1, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2,\n",
       "       1, 2, 2, 0, 1, 2, 2, 2, 0, 1, 2, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, 1,\n",
       "       2, 0, 1, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       0, 0, 2, 1, 0, 2, 1, 2, 2, 0, 2, 2, 0, 1, 1, 2, 2, 2, 0, 0, 2, 0,\n",
       "       2, 2, 0, 2, 0, 1, 0, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "       1, 1, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 3,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "       2, 0, 2, 2, 2, 2, 1, 0, 2, 2, 0, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 2, 2, 2, 2, 2, 0, 1, 2, 2, 2, 0, 2,\n",
       "       2, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 1, 2, 2, 2, 1, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0,\n",
       "       2, 1, 0, 2, 2, 2, 2, 0, 2, 2, 2, 0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 1,\n",
       "       2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 2,\n",
       "       2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1,\n",
       "       0, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0,\n",
       "       0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2,\n",
       "       2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 2, 2, 2, 2, 1, 2, 2, 0, 0, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 0, 0, 2, 0,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 2, 1, 3, 2, 0, 2, 2, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 0,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Intialize LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "#Pass Embarked column into le object\n",
    "emb_encoded = le.fit_transform(X_train.Embarked)\n",
    "\n",
    "#Look at first twenty rows\n",
    "emb_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'Q', 'S', 'unknown'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call .classes_ to see the original object values\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2,\n",
       "       2, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 2,\n",
       "       2, 0, 2, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 0, 1, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 0, 2, 2, 1, 2, 0, 2, 2,\n",
       "       2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 0, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 1, 2, 0, 2, 0, 1, 2, 2, 0,\n",
       "       2, 0, 2, 2, 0, 2, 0, 2, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2,\n",
       "       2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 1, 2, 2,\n",
       "       2, 1, 2, 2, 2, 2, 2, 0, 2, 2, 1, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the embarked class of the testing dataset\n",
    "emb_encoded_test = le.transform(X_test.Embarked)\n",
    "\n",
    "emb_encoded_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350    S\n",
       "176    S\n",
       "723    S\n",
       "404    S\n",
       "306    C\n",
       "665    S\n",
       "607    S\n",
       "151    S\n",
       "265    S\n",
       "865    S\n",
       "408    S\n",
       "78     S\n",
       "200    S\n",
       "40     S\n",
       "709    C\n",
       "785    S\n",
       "876    S\n",
       "64     C\n",
       "852    C\n",
       "429    S\n",
       "193    S\n",
       "437    S\n",
       "35     S\n",
       "654    Q\n",
       "284    S\n",
       "101    S\n",
       "641    C\n",
       "744    S\n",
       "601    S\n",
       "56     S\n",
       "      ..\n",
       "649    S\n",
       "499    S\n",
       "820    S\n",
       "431    S\n",
       "619    S\n",
       "708    S\n",
       "809    S\n",
       "5      Q\n",
       "25     S\n",
       "468    Q\n",
       "736    S\n",
       "314    S\n",
       "856    S\n",
       "553    C\n",
       "635    S\n",
       "53     S\n",
       "329    C\n",
       "175    S\n",
       "472    S\n",
       "880    S\n",
       "883    S\n",
       "427    S\n",
       "1      C\n",
       "346    S\n",
       "327    S\n",
       "411    Q\n",
       "330    Q\n",
       "558    S\n",
       "799    S\n",
       "33     S\n",
       "Name: Embarked, Length: 295, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at original X_test.Embarked\n",
    "\n",
    "X_test.Embarked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(596, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_encoded.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the OneHotEncoder object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C    Q    S  unknown\n",
       "0  0.0  1.0  0.0      0.0\n",
       "1  0.0  0.0  1.0      0.0\n",
       "2  0.0  0.0  1.0      0.0\n",
       "3  1.0  0.0  0.0      0.0\n",
       "4  1.0  0.0  0.0      0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Intialize object\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "\n",
    "\n",
    "#Fit and transform using the emb_encoded variable\n",
    "\n",
    "emb_onehot = onehot.fit_transform(emb_encoded.reshape(-1,1))\n",
    "\n",
    "\n",
    "#Look at emb_onehot\n",
    "\n",
    "# type(emb_onehot)\n",
    "# emb_onehot.toarray()\n",
    "pd.DataFrame(emb_onehot.toarray(), columns=le.classes_).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Q</th>\n",
       "      <th>S</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C    Q    S  unknown\n",
       "0  0.0  0.0  1.0      0.0\n",
       "1  0.0  0.0  1.0      0.0\n",
       "2  0.0  0.0  1.0      0.0\n",
       "3  0.0  0.0  1.0      0.0\n",
       "4  1.0  0.0  0.0      0.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform emb_encoded_test using onehot object\n",
    "\n",
    "emb_onehot_test = onehot.transform(emb_encoded_test.reshape(-1, 1))\n",
    "\n",
    "emb_onehot_test_df = pd.DataFrame(emb_onehot_test.toarray(), columns=le.classes_)\n",
    "emb_onehot_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the LabelBinarizer to do this as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Intialize LabelBinarizer\n",
    "lb = LabelBinarizer() \n",
    "\n",
    "\n",
    "#Fit and transform on the Embarked column of the training dataset\n",
    "\n",
    "bin_data = lb.fit_transform(X_train.Embarked)\n",
    "\n",
    "#Fill nans with unknown\n",
    "bin_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C', 'Q', 'S', 'unknown'], dtype='<U7')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the class or column values\n",
    "\n",
    "lb.classes_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform the testing data using lb\n",
    "\n",
    "bin_data_test = lb.transform(X_test.Embarked)\n",
    "bin_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be asking yourself \"Why use this instead of pd.get_dummies?\"\n",
    "\n",
    "That's because testing data or any other new you want to use may not have the same values in their categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new dataset from X_test where there is no C value in the Embarked column\n",
    "X_test2 = X_test[X_test.Embarked != \"C\"]\n",
    "\n",
    "\n",
    "#Transform the Embarked column from  X_test2 using the LabelBinarizer\n",
    "\n",
    "lb.transform(X_test2.Embarked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a 0 for every value under the C column. Using pd.get_dummies we would have three columns instead of four. This is important because when you fit you model using the training data and then make predictions using the testing data, the model won't work if your testing and training data don't have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion:\n",
    "\n",
    "![e](https://chrisalbon.com/images/machine_learning_flashcards/One-Hot_Encoding_print.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm that tests every combination of model parameters to find the best one.\n",
    "\n",
    "Let's use GridSearch to find the best K value for a KNN model and Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in spotify data and assign X/y\n",
    "spotify = pd.read_csv(\"../../data/spotify_data.csv\", index_col=[0])\n",
    "\n",
    "X = spotify.drop(\"target\", axis = 1)\n",
    "y = spotify.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': [1,\n",
       "  3,\n",
       "  5,\n",
       "  7,\n",
       "  9,\n",
       "  11,\n",
       "  13,\n",
       "  15,\n",
       "  17,\n",
       "  19,\n",
       "  21,\n",
       "  23,\n",
       "  25,\n",
       "  27,\n",
       "  29,\n",
       "  31,\n",
       "  33,\n",
       "  35,\n",
       "  37,\n",
       "  39,\n",
       "  41,\n",
       "  43,\n",
       "  45,\n",
       "  47,\n",
       "  49]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize parameter grid\n",
    "\n",
    "#Range of neighbors to test\n",
    "\n",
    "neighnors_range = list(range(1, 50, 2))\n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_knn = {}\n",
    "param_grid_knn['n_neighbors']=neighnors_range\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, scoring='accuracy', cv=5)\n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "grid_knn.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.691125433812593"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.56668, std: 0.03445, params: {'n_neighbors': 1},\n",
       " mean: 0.58404, std: 0.04995, params: {'n_neighbors': 3},\n",
       " mean: 0.58007, std: 0.05479, params: {'n_neighbors': 5},\n",
       " mean: 0.57759, std: 0.05078, params: {'n_neighbors': 7},\n",
       " mean: 0.58999, std: 0.05879, params: {'n_neighbors': 9},\n",
       " mean: 0.60337, std: 0.06452, params: {'n_neighbors': 11},\n",
       " mean: 0.59891, std: 0.05833, params: {'n_neighbors': 13},\n",
       " mean: 0.60089, std: 0.05880, params: {'n_neighbors': 15},\n",
       " mean: 0.59841, std: 0.05767, params: {'n_neighbors': 17},\n",
       " mean: 0.60139, std: 0.05741, params: {'n_neighbors': 19},\n",
       " mean: 0.60783, std: 0.05533, params: {'n_neighbors': 21},\n",
       " mean: 0.60337, std: 0.05728, params: {'n_neighbors': 23},\n",
       " mean: 0.60635, std: 0.05986, params: {'n_neighbors': 25},\n",
       " mean: 0.60486, std: 0.06027, params: {'n_neighbors': 27},\n",
       " mean: 0.61180, std: 0.05100, params: {'n_neighbors': 29},\n",
       " mean: 0.60040, std: 0.05144, params: {'n_neighbors': 31},\n",
       " mean: 0.60288, std: 0.04893, params: {'n_neighbors': 33},\n",
       " mean: 0.60040, std: 0.04946, params: {'n_neighbors': 35},\n",
       " mean: 0.60238, std: 0.05149, params: {'n_neighbors': 37},\n",
       " mean: 0.60436, std: 0.04751, params: {'n_neighbors': 39},\n",
       " mean: 0.60040, std: 0.04503, params: {'n_neighbors': 41},\n",
       " mean: 0.59941, std: 0.05189, params: {'n_neighbors': 43},\n",
       " mean: 0.60288, std: 0.04688, params: {'n_neighbors': 45},\n",
       " mean: 0.60288, std: 0.04882, params: {'n_neighbors': 47},\n",
       " mean: 0.60436, std: 0.04793, params: {'n_neighbors': 49}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View scores\n",
    "\n",
    "grid_knn.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-2ef7a269de24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_knn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "# np.mean(grid_knn.grid_scores_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6117997025285077"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Whats the best cross validated accuracy score\n",
    "\n",
    "grid_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 29}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the best parameters\n",
    "\n",
    "grid_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=29, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_knn.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple technique gives us the best K value.\n",
    "\n",
    "We can use the best model from grid_knn to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Input \n",
    "x = [[0.2, .15, 0.68, 0.05, 0.328]]\n",
    "\n",
    "#Make prediction \n",
    "\n",
    "grid_knn.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13793103, 0.86206897]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Proability\n",
    "\n",
    "grid_knn.predict_proba(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Range of neighbors to test\n",
    "depths_range = range(2, 20)\n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_dt = {}\n",
    "param_grid_dt['max_depth']=list(depths_range)\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6608824987605354"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score for DT model\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameter for DT model\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make prediction\n",
    "\n",
    "grid_dt.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our grids have been one-dimensional, now let's try using multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       " 'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Param grid with test different split criteria as well.\n",
    "param_grid_dt = {'min_samples_split' : list(range(2, 30 ,2)), \"max_depth\": list(depths_range)}\n",
    "\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy')\n",
    "\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'min_samples_split': 12}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameter\n",
    "\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6623698562221121"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 2},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 4},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 6},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 8},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 10},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 12},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 14},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 16},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 18},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 20},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 22},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 24},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 26},\n",
       " mean: 0.63907, std: 0.06683, params: {'max_depth': 2, 'min_samples_split': 28},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 2},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 4},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 6},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 8},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 10},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 12},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 14},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 16},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 18},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 20},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 22},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 24},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 26},\n",
       " mean: 0.64303, std: 0.06100, params: {'max_depth': 3, 'min_samples_split': 28},\n",
       " mean: 0.66187, std: 0.04178, params: {'max_depth': 4, 'min_samples_split': 2},\n",
       " mean: 0.66187, std: 0.04178, params: {'max_depth': 4, 'min_samples_split': 4},\n",
       " mean: 0.66088, std: 0.04292, params: {'max_depth': 4, 'min_samples_split': 6},\n",
       " mean: 0.66138, std: 0.04233, params: {'max_depth': 4, 'min_samples_split': 8},\n",
       " mean: 0.66138, std: 0.04233, params: {'max_depth': 4, 'min_samples_split': 10},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 12},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 14},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 16},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 18},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 20},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 22},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 24},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 26},\n",
       " mean: 0.66237, std: 0.04116, params: {'max_depth': 4, 'min_samples_split': 28},\n",
       " mean: 0.64998, std: 0.02408, params: {'max_depth': 5, 'min_samples_split': 2},\n",
       " mean: 0.64898, std: 0.02564, params: {'max_depth': 5, 'min_samples_split': 4},\n",
       " mean: 0.64998, std: 0.02408, params: {'max_depth': 5, 'min_samples_split': 6},\n",
       " mean: 0.64898, std: 0.02628, params: {'max_depth': 5, 'min_samples_split': 8},\n",
       " mean: 0.65047, std: 0.02387, params: {'max_depth': 5, 'min_samples_split': 10},\n",
       " mean: 0.64898, std: 0.02628, params: {'max_depth': 5, 'min_samples_split': 12},\n",
       " mean: 0.64849, std: 0.02644, params: {'max_depth': 5, 'min_samples_split': 14},\n",
       " mean: 0.64998, std: 0.02408, params: {'max_depth': 5, 'min_samples_split': 16},\n",
       " mean: 0.64601, std: 0.02747, params: {'max_depth': 5, 'min_samples_split': 18},\n",
       " mean: 0.64601, std: 0.02747, params: {'max_depth': 5, 'min_samples_split': 20},\n",
       " mean: 0.64601, std: 0.02747, params: {'max_depth': 5, 'min_samples_split': 22},\n",
       " mean: 0.64601, std: 0.02747, params: {'max_depth': 5, 'min_samples_split': 24},\n",
       " mean: 0.64403, std: 0.02930, params: {'max_depth': 5, 'min_samples_split': 26},\n",
       " mean: 0.64353, std: 0.02992, params: {'max_depth': 5, 'min_samples_split': 28},\n",
       " mean: 0.63956, std: 0.03290, params: {'max_depth': 6, 'min_samples_split': 2},\n",
       " mean: 0.64254, std: 0.02898, params: {'max_depth': 6, 'min_samples_split': 4},\n",
       " mean: 0.64155, std: 0.03225, params: {'max_depth': 6, 'min_samples_split': 6},\n",
       " mean: 0.64155, std: 0.03071, params: {'max_depth': 6, 'min_samples_split': 8},\n",
       " mean: 0.64006, std: 0.03205, params: {'max_depth': 6, 'min_samples_split': 10},\n",
       " mean: 0.64155, std: 0.03128, params: {'max_depth': 6, 'min_samples_split': 12},\n",
       " mean: 0.64105, std: 0.02841, params: {'max_depth': 6, 'min_samples_split': 14},\n",
       " mean: 0.64105, std: 0.02841, params: {'max_depth': 6, 'min_samples_split': 16},\n",
       " mean: 0.63461, std: 0.03088, params: {'max_depth': 6, 'min_samples_split': 18},\n",
       " mean: 0.63510, std: 0.03052, params: {'max_depth': 6, 'min_samples_split': 20},\n",
       " mean: 0.63609, std: 0.03056, params: {'max_depth': 6, 'min_samples_split': 22},\n",
       " mean: 0.63907, std: 0.02891, params: {'max_depth': 6, 'min_samples_split': 24},\n",
       " mean: 0.63808, std: 0.02874, params: {'max_depth': 6, 'min_samples_split': 26},\n",
       " mean: 0.63808, std: 0.02988, params: {'max_depth': 6, 'min_samples_split': 28},\n",
       " mean: 0.63659, std: 0.03205, params: {'max_depth': 7, 'min_samples_split': 2},\n",
       " mean: 0.63758, std: 0.03043, params: {'max_depth': 7, 'min_samples_split': 4},\n",
       " mean: 0.64056, std: 0.03266, params: {'max_depth': 7, 'min_samples_split': 6},\n",
       " mean: 0.63560, std: 0.03126, params: {'max_depth': 7, 'min_samples_split': 8},\n",
       " mean: 0.63312, std: 0.03572, params: {'max_depth': 7, 'min_samples_split': 10},\n",
       " mean: 0.63411, std: 0.03598, params: {'max_depth': 7, 'min_samples_split': 12},\n",
       " mean: 0.63312, std: 0.03282, params: {'max_depth': 7, 'min_samples_split': 14},\n",
       " mean: 0.63262, std: 0.03562, params: {'max_depth': 7, 'min_samples_split': 16},\n",
       " mean: 0.62866, std: 0.03711, params: {'max_depth': 7, 'min_samples_split': 18},\n",
       " mean: 0.63014, std: 0.03723, params: {'max_depth': 7, 'min_samples_split': 20},\n",
       " mean: 0.63064, std: 0.03861, params: {'max_depth': 7, 'min_samples_split': 22},\n",
       " mean: 0.63510, std: 0.03255, params: {'max_depth': 7, 'min_samples_split': 24},\n",
       " mean: 0.63411, std: 0.03348, params: {'max_depth': 7, 'min_samples_split': 26},\n",
       " mean: 0.63213, std: 0.03615, params: {'max_depth': 7, 'min_samples_split': 28},\n",
       " mean: 0.61230, std: 0.03840, params: {'max_depth': 8, 'min_samples_split': 2},\n",
       " mean: 0.61725, std: 0.03415, params: {'max_depth': 8, 'min_samples_split': 4},\n",
       " mean: 0.61230, std: 0.03266, params: {'max_depth': 8, 'min_samples_split': 6},\n",
       " mean: 0.61577, std: 0.03111, params: {'max_depth': 8, 'min_samples_split': 8},\n",
       " mean: 0.61031, std: 0.03998, params: {'max_depth': 8, 'min_samples_split': 10},\n",
       " mean: 0.61527, std: 0.03719, params: {'max_depth': 8, 'min_samples_split': 12},\n",
       " mean: 0.61477, std: 0.03514, params: {'max_depth': 8, 'min_samples_split': 14},\n",
       " mean: 0.61626, std: 0.03968, params: {'max_depth': 8, 'min_samples_split': 16},\n",
       " mean: 0.61230, std: 0.04164, params: {'max_depth': 8, 'min_samples_split': 18},\n",
       " mean: 0.61279, std: 0.04119, params: {'max_depth': 8, 'min_samples_split': 20},\n",
       " mean: 0.61477, std: 0.04371, params: {'max_depth': 8, 'min_samples_split': 22},\n",
       " mean: 0.61924, std: 0.03934, params: {'max_depth': 8, 'min_samples_split': 24},\n",
       " mean: 0.61874, std: 0.04012, params: {'max_depth': 8, 'min_samples_split': 26},\n",
       " mean: 0.61775, std: 0.03979, params: {'max_depth': 8, 'min_samples_split': 28},\n",
       " mean: 0.63114, std: 0.02528, params: {'max_depth': 9, 'min_samples_split': 2},\n",
       " mean: 0.62866, std: 0.02607, params: {'max_depth': 9, 'min_samples_split': 4},\n",
       " mean: 0.62618, std: 0.02762, params: {'max_depth': 9, 'min_samples_split': 6},\n",
       " mean: 0.61924, std: 0.02996, params: {'max_depth': 9, 'min_samples_split': 8},\n",
       " mean: 0.63064, std: 0.02893, params: {'max_depth': 9, 'min_samples_split': 10},\n",
       " mean: 0.62023, std: 0.03211, params: {'max_depth': 9, 'min_samples_split': 12},\n",
       " mean: 0.61725, std: 0.03598, params: {'max_depth': 9, 'min_samples_split': 14},\n",
       " mean: 0.62271, std: 0.03845, params: {'max_depth': 9, 'min_samples_split': 16},\n",
       " mean: 0.61477, std: 0.04116, params: {'max_depth': 9, 'min_samples_split': 18},\n",
       " mean: 0.61725, std: 0.03951, params: {'max_depth': 9, 'min_samples_split': 20},\n",
       " mean: 0.61279, std: 0.04158, params: {'max_depth': 9, 'min_samples_split': 22},\n",
       " mean: 0.61775, std: 0.03785, params: {'max_depth': 9, 'min_samples_split': 24},\n",
       " mean: 0.61180, std: 0.03631, params: {'max_depth': 9, 'min_samples_split': 26},\n",
       " mean: 0.61130, std: 0.03709, params: {'max_depth': 9, 'min_samples_split': 28},\n",
       " mean: 0.61477, std: 0.02484, params: {'max_depth': 10, 'min_samples_split': 2},\n",
       " mean: 0.61180, std: 0.02326, params: {'max_depth': 10, 'min_samples_split': 4},\n",
       " mean: 0.61378, std: 0.02178, params: {'max_depth': 10, 'min_samples_split': 6},\n",
       " mean: 0.61230, std: 0.02728, params: {'max_depth': 10, 'min_samples_split': 8},\n",
       " mean: 0.62023, std: 0.02322, params: {'max_depth': 10, 'min_samples_split': 10},\n",
       " mean: 0.61081, std: 0.03039, params: {'max_depth': 10, 'min_samples_split': 12},\n",
       " mean: 0.61180, std: 0.02977, params: {'max_depth': 10, 'min_samples_split': 14},\n",
       " mean: 0.61279, std: 0.03578, params: {'max_depth': 10, 'min_samples_split': 16},\n",
       " mean: 0.61180, std: 0.03734, params: {'max_depth': 10, 'min_samples_split': 18},\n",
       " mean: 0.61428, std: 0.03781, params: {'max_depth': 10, 'min_samples_split': 20},\n",
       " mean: 0.61031, std: 0.03907, params: {'max_depth': 10, 'min_samples_split': 22},\n",
       " mean: 0.62072, std: 0.03892, params: {'max_depth': 10, 'min_samples_split': 24},\n",
       " mean: 0.61577, std: 0.03862, params: {'max_depth': 10, 'min_samples_split': 26},\n",
       " mean: 0.61477, std: 0.03957, params: {'max_depth': 10, 'min_samples_split': 28},\n",
       " mean: 0.60932, std: 0.02281, params: {'max_depth': 11, 'min_samples_split': 2},\n",
       " mean: 0.60734, std: 0.02406, params: {'max_depth': 11, 'min_samples_split': 4},\n",
       " mean: 0.60982, std: 0.02671, params: {'max_depth': 11, 'min_samples_split': 6},\n",
       " mean: 0.61180, std: 0.02086, params: {'max_depth': 11, 'min_samples_split': 8},\n",
       " mean: 0.60932, std: 0.02801, params: {'max_depth': 11, 'min_samples_split': 10},\n",
       " mean: 0.60436, std: 0.02915, params: {'max_depth': 11, 'min_samples_split': 12},\n",
       " mean: 0.60734, std: 0.03047, params: {'max_depth': 11, 'min_samples_split': 14},\n",
       " mean: 0.61230, std: 0.03261, params: {'max_depth': 11, 'min_samples_split': 16},\n",
       " mean: 0.60932, std: 0.03631, params: {'max_depth': 11, 'min_samples_split': 18},\n",
       " mean: 0.60982, std: 0.03583, params: {'max_depth': 11, 'min_samples_split': 20},\n",
       " mean: 0.60734, std: 0.03579, params: {'max_depth': 11, 'min_samples_split': 22},\n",
       " mean: 0.61527, std: 0.03744, params: {'max_depth': 11, 'min_samples_split': 24},\n",
       " mean: 0.61031, std: 0.03692, params: {'max_depth': 11, 'min_samples_split': 26},\n",
       " mean: 0.61031, std: 0.03425, params: {'max_depth': 11, 'min_samples_split': 28},\n",
       " mean: 0.59147, std: 0.03022, params: {'max_depth': 12, 'min_samples_split': 2},\n",
       " mean: 0.60089, std: 0.02703, params: {'max_depth': 12, 'min_samples_split': 4},\n",
       " mean: 0.60288, std: 0.02731, params: {'max_depth': 12, 'min_samples_split': 6},\n",
       " mean: 0.60040, std: 0.02825, params: {'max_depth': 12, 'min_samples_split': 8},\n",
       " mean: 0.59990, std: 0.02436, params: {'max_depth': 12, 'min_samples_split': 10},\n",
       " mean: 0.59494, std: 0.02341, params: {'max_depth': 12, 'min_samples_split': 12},\n",
       " mean: 0.59891, std: 0.02951, params: {'max_depth': 12, 'min_samples_split': 14},\n",
       " mean: 0.60238, std: 0.03301, params: {'max_depth': 12, 'min_samples_split': 16},\n",
       " mean: 0.60089, std: 0.03751, params: {'max_depth': 12, 'min_samples_split': 18},\n",
       " mean: 0.59990, std: 0.03480, params: {'max_depth': 12, 'min_samples_split': 20},\n",
       " mean: 0.59891, std: 0.03441, params: {'max_depth': 12, 'min_samples_split': 22},\n",
       " mean: 0.60982, std: 0.03534, params: {'max_depth': 12, 'min_samples_split': 24},\n",
       " mean: 0.60535, std: 0.03493, params: {'max_depth': 12, 'min_samples_split': 26},\n",
       " mean: 0.60486, std: 0.03345, params: {'max_depth': 12, 'min_samples_split': 28},\n",
       " mean: 0.59395, std: 0.02298, params: {'max_depth': 13, 'min_samples_split': 2},\n",
       " mean: 0.59941, std: 0.02126, params: {'max_depth': 13, 'min_samples_split': 4},\n",
       " mean: 0.59593, std: 0.01967, params: {'max_depth': 13, 'min_samples_split': 6},\n",
       " mean: 0.59544, std: 0.02122, params: {'max_depth': 13, 'min_samples_split': 8},\n",
       " mean: 0.59593, std: 0.02734, params: {'max_depth': 13, 'min_samples_split': 10},\n",
       " mean: 0.59197, std: 0.02474, params: {'max_depth': 13, 'min_samples_split': 12},\n",
       " mean: 0.59395, std: 0.02882, params: {'max_depth': 13, 'min_samples_split': 14},\n",
       " mean: 0.60040, std: 0.03241, params: {'max_depth': 13, 'min_samples_split': 16},\n",
       " mean: 0.59891, std: 0.03467, params: {'max_depth': 13, 'min_samples_split': 18},\n",
       " mean: 0.60089, std: 0.03379, params: {'max_depth': 13, 'min_samples_split': 20},\n",
       " mean: 0.59395, std: 0.03306, params: {'max_depth': 13, 'min_samples_split': 22},\n",
       " mean: 0.60734, std: 0.03633, params: {'max_depth': 13, 'min_samples_split': 24},\n",
       " mean: 0.60089, std: 0.03370, params: {'max_depth': 13, 'min_samples_split': 26},\n",
       " mean: 0.60387, std: 0.03335, params: {'max_depth': 13, 'min_samples_split': 28},\n",
       " mean: 0.59098, std: 0.03705, params: {'max_depth': 14, 'min_samples_split': 2},\n",
       " mean: 0.59544, std: 0.03262, params: {'max_depth': 14, 'min_samples_split': 4},\n",
       " mean: 0.58850, std: 0.02479, params: {'max_depth': 14, 'min_samples_split': 6},\n",
       " mean: 0.59841, std: 0.02708, params: {'max_depth': 14, 'min_samples_split': 8},\n",
       " mean: 0.59891, std: 0.02358, params: {'max_depth': 14, 'min_samples_split': 10},\n",
       " mean: 0.59693, std: 0.02760, params: {'max_depth': 14, 'min_samples_split': 12},\n",
       " mean: 0.59048, std: 0.02643, params: {'max_depth': 14, 'min_samples_split': 14},\n",
       " mean: 0.59742, std: 0.03723, params: {'max_depth': 14, 'min_samples_split': 16},\n",
       " mean: 0.59395, std: 0.03762, params: {'max_depth': 14, 'min_samples_split': 18},\n",
       " mean: 0.59841, std: 0.03544, params: {'max_depth': 14, 'min_samples_split': 20},\n",
       " mean: 0.59296, std: 0.03626, params: {'max_depth': 14, 'min_samples_split': 22},\n",
       " mean: 0.60635, std: 0.03662, params: {'max_depth': 14, 'min_samples_split': 24},\n",
       " mean: 0.60139, std: 0.03510, params: {'max_depth': 14, 'min_samples_split': 26},\n",
       " mean: 0.60089, std: 0.03416, params: {'max_depth': 14, 'min_samples_split': 28},\n",
       " mean: 0.59246, std: 0.02665, params: {'max_depth': 15, 'min_samples_split': 2},\n",
       " mean: 0.59346, std: 0.02643, params: {'max_depth': 15, 'min_samples_split': 4},\n",
       " mean: 0.59296, std: 0.02182, params: {'max_depth': 15, 'min_samples_split': 6},\n",
       " mean: 0.59395, std: 0.01961, params: {'max_depth': 15, 'min_samples_split': 8},\n",
       " mean: 0.59246, std: 0.02690, params: {'max_depth': 15, 'min_samples_split': 10},\n",
       " mean: 0.58999, std: 0.02589, params: {'max_depth': 15, 'min_samples_split': 12},\n",
       " mean: 0.58949, std: 0.02881, params: {'max_depth': 15, 'min_samples_split': 14},\n",
       " mean: 0.59147, std: 0.03498, params: {'max_depth': 15, 'min_samples_split': 16},\n",
       " mean: 0.59593, std: 0.03816, params: {'max_depth': 15, 'min_samples_split': 18},\n",
       " mean: 0.59891, std: 0.03321, params: {'max_depth': 15, 'min_samples_split': 20},\n",
       " mean: 0.59098, std: 0.03627, params: {'max_depth': 15, 'min_samples_split': 22},\n",
       " mean: 0.60288, std: 0.03346, params: {'max_depth': 15, 'min_samples_split': 24},\n",
       " mean: 0.59990, std: 0.03956, params: {'max_depth': 15, 'min_samples_split': 26},\n",
       " mean: 0.60139, std: 0.03614, params: {'max_depth': 15, 'min_samples_split': 28},\n",
       " mean: 0.59296, std: 0.02087, params: {'max_depth': 16, 'min_samples_split': 2},\n",
       " mean: 0.58949, std: 0.03387, params: {'max_depth': 16, 'min_samples_split': 4},\n",
       " mean: 0.59296, std: 0.02849, params: {'max_depth': 16, 'min_samples_split': 6},\n",
       " mean: 0.58354, std: 0.02773, params: {'max_depth': 16, 'min_samples_split': 8},\n",
       " mean: 0.58354, std: 0.02867, params: {'max_depth': 16, 'min_samples_split': 10},\n",
       " mean: 0.58751, std: 0.02963, params: {'max_depth': 16, 'min_samples_split': 12},\n",
       " mean: 0.58800, std: 0.02778, params: {'max_depth': 16, 'min_samples_split': 14},\n",
       " mean: 0.59246, std: 0.03732, params: {'max_depth': 16, 'min_samples_split': 16},\n",
       " mean: 0.59445, std: 0.03532, params: {'max_depth': 16, 'min_samples_split': 18},\n",
       " mean: 0.59494, std: 0.03025, params: {'max_depth': 16, 'min_samples_split': 20},\n",
       " mean: 0.59296, std: 0.03519, params: {'max_depth': 16, 'min_samples_split': 22},\n",
       " mean: 0.60337, std: 0.03562, params: {'max_depth': 16, 'min_samples_split': 24},\n",
       " mean: 0.59792, std: 0.03575, params: {'max_depth': 16, 'min_samples_split': 26},\n",
       " mean: 0.60040, std: 0.03514, params: {'max_depth': 16, 'min_samples_split': 28},\n",
       " mean: 0.58106, std: 0.02840, params: {'max_depth': 17, 'min_samples_split': 2},\n",
       " mean: 0.58949, std: 0.02173, params: {'max_depth': 17, 'min_samples_split': 4},\n",
       " mean: 0.58751, std: 0.02628, params: {'max_depth': 17, 'min_samples_split': 6},\n",
       " mean: 0.58354, std: 0.02508, params: {'max_depth': 17, 'min_samples_split': 8},\n",
       " mean: 0.58651, std: 0.02387, params: {'max_depth': 17, 'min_samples_split': 10},\n",
       " mean: 0.58850, std: 0.02263, params: {'max_depth': 17, 'min_samples_split': 12},\n",
       " mean: 0.58949, std: 0.02955, params: {'max_depth': 17, 'min_samples_split': 14},\n",
       " mean: 0.59197, std: 0.03424, params: {'max_depth': 17, 'min_samples_split': 16},\n",
       " mean: 0.59197, std: 0.03706, params: {'max_depth': 17, 'min_samples_split': 18},\n",
       " mean: 0.59544, std: 0.03258, params: {'max_depth': 17, 'min_samples_split': 20},\n",
       " mean: 0.59098, std: 0.03376, params: {'max_depth': 17, 'min_samples_split': 22},\n",
       " mean: 0.60188, std: 0.03555, params: {'max_depth': 17, 'min_samples_split': 24},\n",
       " mean: 0.59990, std: 0.03625, params: {'max_depth': 17, 'min_samples_split': 26},\n",
       " mean: 0.59941, std: 0.03561, params: {'max_depth': 17, 'min_samples_split': 28},\n",
       " mean: 0.58751, std: 0.02660, params: {'max_depth': 18, 'min_samples_split': 2},\n",
       " mean: 0.58354, std: 0.02888, params: {'max_depth': 18, 'min_samples_split': 4},\n",
       " mean: 0.58304, std: 0.01730, params: {'max_depth': 18, 'min_samples_split': 6},\n",
       " mean: 0.58404, std: 0.02318, params: {'max_depth': 18, 'min_samples_split': 8},\n",
       " mean: 0.58800, std: 0.02606, params: {'max_depth': 18, 'min_samples_split': 10},\n",
       " mean: 0.58304, std: 0.02798, params: {'max_depth': 18, 'min_samples_split': 12},\n",
       " mean: 0.58751, std: 0.03167, params: {'max_depth': 18, 'min_samples_split': 14},\n",
       " mean: 0.59098, std: 0.03466, params: {'max_depth': 18, 'min_samples_split': 16},\n",
       " mean: 0.58999, std: 0.03859, params: {'max_depth': 18, 'min_samples_split': 18},\n",
       " mean: 0.59296, std: 0.03205, params: {'max_depth': 18, 'min_samples_split': 20},\n",
       " mean: 0.59147, std: 0.03298, params: {'max_depth': 18, 'min_samples_split': 22},\n",
       " mean: 0.60139, std: 0.03563, params: {'max_depth': 18, 'min_samples_split': 24},\n",
       " mean: 0.59792, std: 0.03755, params: {'max_depth': 18, 'min_samples_split': 26},\n",
       " mean: 0.59941, std: 0.03561, params: {'max_depth': 18, 'min_samples_split': 28},\n",
       " mean: 0.58850, std: 0.02294, params: {'max_depth': 19, 'min_samples_split': 2},\n",
       " mean: 0.57362, std: 0.02546, params: {'max_depth': 19, 'min_samples_split': 4},\n",
       " mean: 0.58255, std: 0.01996, params: {'max_depth': 19, 'min_samples_split': 6},\n",
       " mean: 0.59197, std: 0.02196, params: {'max_depth': 19, 'min_samples_split': 8},\n",
       " mean: 0.58999, std: 0.02569, params: {'max_depth': 19, 'min_samples_split': 10},\n",
       " mean: 0.58850, std: 0.02217, params: {'max_depth': 19, 'min_samples_split': 12},\n",
       " mean: 0.58205, std: 0.02646, params: {'max_depth': 19, 'min_samples_split': 14},\n",
       " mean: 0.59346, std: 0.03180, params: {'max_depth': 19, 'min_samples_split': 16},\n",
       " mean: 0.59346, std: 0.03676, params: {'max_depth': 19, 'min_samples_split': 18},\n",
       " mean: 0.59296, std: 0.03350, params: {'max_depth': 19, 'min_samples_split': 20},\n",
       " mean: 0.58899, std: 0.03627, params: {'max_depth': 19, 'min_samples_split': 22},\n",
       " mean: 0.60089, std: 0.03797, params: {'max_depth': 19, 'min_samples_split': 24},\n",
       " mean: 0.59693, std: 0.03376, params: {'max_depth': 19, 'min_samples_split': 26},\n",
       " mean: 0.59941, std: 0.03561, params: {'max_depth': 19, 'min_samples_split': 28}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_dt.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many models did the grid search algorithm fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add another dimension\n",
    "param_grid_dt[\"max_features\"] = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.884172201156616\n"
     ]
    }
   ],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n",
    "                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4, 'max_features': 4, 'min_samples_split': 8}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameter\n",
    "\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6782350024789291"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.006991147994995\n"
     ]
    }
   ],
   "source": [
    "#Intialize RandomizedSearchCV grid with n_iter = 15\n",
    "grid_dt = RandomizedSearchCV(DecisionTreeClassifier(), param_grid_dt, n_iter=100, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced run time by  a whole a lot. But now let's see if we sacrificed performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6673277144273674"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check best score\n",
    "\n",
    "grid_dt.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'max_features': 3, 'min_samples_split': 24}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to using the KNN model.\n",
    "\n",
    "We know that we need to scale our data for the KNN algorithm right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data and fit it a Grid search function it.\n",
    "\n",
    "#Intialize scalar\n",
    "scale = StandardScaler()\n",
    "\n",
    "#Fit and transform scaler on the data\n",
    "Xs = scale.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass scaler and knn classifier objects into make_pipeline function\n",
    "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kneighborsclassifier__n_neighbors': [2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20]}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new param_grid\n",
    "neighbors_range = range(2, 21)\n",
    "param_grid_knn = {}\n",
    "param_grid_knn[\"kneighborsclassifier__n_neighbors\"] = list(neighbors_range)\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6063460585027268 {'kneighborsclassifier__n_neighbors': 17}\n"
     ]
    }
   ],
   "source": [
    "#Pass in pipe into GridSearchCV function, \n",
    "grid_knn_pipe = GridSearchCV(pipe, param_grid_knn, cv=5, scoring='accuracy')\n",
    "\n",
    "\n",
    "#Fit on original versions of data\n",
    "grid_knn_pipe.fit(X,y)\n",
    "\n",
    "\n",
    "#Best scores and params\n",
    "    print (grid_knn_pipe.best_score_, grid_knn_pipe.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5775851902808146"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the cross-validation process using Pipeline\n",
    "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in our classification lessons we've mainly modeling binary classification datasets aka either-or data. In this class we're going to work through the MNIST digits dataset and learn to work with and interpret models trained on multiple classesmultiple meaning more than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into the MNIST dataset, let's bring back the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width      species\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in iris data using seaborn\n",
    "# iris = sb.load_dataset(\"iris\")\n",
    "\n",
    "path = '../../data/iris.csv'\n",
    "\n",
    "iris = pd.read_csv(path)\n",
    "\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset is a mulit-class dataset because there are three uniques values in the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The class of species\n",
    "iris.species.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's model this data and using a confusion matrix to analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1. Assign X and y\n",
    "\n",
    "X = iris.drop('species', axis=1)\n",
    "\n",
    "y = iris.species\n",
    "\n",
    "#Step 2. train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=27)\n",
    "\n",
    "\n",
    "#Step 3. Fit KNN model with 3 neighbors on training data\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Step 4. Make predictions on the X_test using the model\n",
    "\n",
    "preds = knn.predict(X_test)\n",
    "\n",
    "#Step 5. Score predictions\n",
    "\n",
    "knn.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty decent model right? Now let's use the confusion matrix to better understand our predictions, particulary the wrong ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  0,  0],\n",
       "       [ 0, 15,  3],\n",
       "       [ 0,  3, 16]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Call pass in y_test and preds into confusion_matrix object\n",
    "\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_setosa</th>\n",
       "      <th>pred_versicolor</th>\n",
       "      <th>pred_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>actual_setosa</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_versicolor</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_virginica</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pred_setosa  pred_versicolor  pred_virginica\n",
       "actual_setosa               13                0               0\n",
       "actual_versicolor            0               15               3\n",
       "actual_virginica             0                3              16"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Column and index values for our confusion matrix dataframe\n",
    "cols = [\"pred_setosa\", \"pred_versicolor\", \"pred_virginica\"]\n",
    "index = [\"actual_setosa\", \"actual_versicolor\", \"actual_virginica\"]\n",
    "\n",
    "#Make dataframe out of confusion matrix\n",
    "\n",
    "cm_df = pd.DataFrame(cm, columns=cols, index=index)\n",
    "\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score told us that our model correctly classified 88% of the testing dataset.\n",
    "\n",
    "What does the confusion matrix tell us that the accuracy score doesn't? What is the accuracy if we ignore setosa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8378378378378378"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Non setosa accuracy score\n",
    "\n",
    "(15+16)/(15+16+3+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 13, 15, 16, represent the values that we correctly identified.\n",
    "\n",
    "The two 3 values represent the values that we incorrectly identified.\n",
    "\n",
    "<br>\n",
    "\n",
    "If we want to calculate the recall and precision scores then we need to designate one of the classes as true and the rest as false\n",
    "\n",
    "![ee](https://media.licdn.com/mpr/mpr/AAEAAQAAAAAAAAykAAAAJDUzZGVlZGM0LTUyNWMtNDNjZi1hNjkxLTdlZjEzY2VmMmM4OQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision = TP/TP+FP\n",
    "\n",
    "Recall = TP/TP+FN\n",
    "\n",
    "Good blog post to refresh your memory on these metrics:http://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The recall score for setaosa\n",
    "\n",
    "tp_set = cm_df.loc[\"actual_setosa\", \"pred_setosa\"]\n",
    "\n",
    "#Divide tp_set by the sum of values in the actual setosa row\n",
    "\n",
    "tp_set/float(cm_df.loc[\"actual_setosa\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The precision score for setosa\n",
    "\n",
    "\n",
    "\n",
    "#Divide tp_set by the sum of values in the predicted setosa column\n",
    "\n",
    "tp_set/float(cm_df.pred_setosa.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got perfect scores but thats boring, let's try this with versicolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roahuja/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The recall score for versicolor\n",
    "\n",
    "tp_ver = cm_df.loc[\"actual_versicolor\", \"pred_versicolor\"]\n",
    "\n",
    "#Divide tp_set by the sum of values in the actual versicolor row\n",
    "\n",
    "tp_ver/float(cm_df.ix[\"actual_versicolor\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The precision score for versicolor\n",
    "\n",
    "#Divide tp_set by the sum of values in the predicted versicolor column\n",
    "\n",
    "tp_ver/float(cm_df.pred_versicolor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-class confusion matrix explained:\n",
    "![s](https://3.bp.blogspot.com/-YpiS7AXxlgs/VEVrZGx5oaI/AAAAAAAAG1c/E8PdwoUamYw/s1600/multi-class-confusionmatrix.png)\n",
    "\n",
    "[Source](http://text-analytics101.rxnlp.com/2014/10/computing-precision-and-recall-for.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famous machine learning dataset that is frequently used over and over again in machine learning courses.\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hand written digits of the mnist dataset.\n",
    "![a](https://kuanhoong.files.wordpress.com/2016/01/mnistdigits.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset from sklearn\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_dict = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Optical Recognition of Handwritten Digits Data Set',\n",
       " '===================================================',\n",
       " '',\n",
       " 'Notes',\n",
       " '-----',\n",
       " 'Data Set Characteristics:',\n",
       " '    :Number of Instances: 5620',\n",
       " '    :Number of Attributes: 64',\n",
       " '    :Attribute Information: 8x8 image of integer pixels in the range 0..16.',\n",
       " '    :Missing Attribute Values: None',\n",
       " \"    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\",\n",
       " '    :Date: July; 1998',\n",
       " '',\n",
       " 'This is a copy of the test set of the UCI ML hand-written digits datasets',\n",
       " 'http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits',\n",
       " '',\n",
       " 'The data set contains images of hand-written digits: 10 classes where',\n",
       " 'each class refers to a digit.',\n",
       " '',\n",
       " 'Preprocessing programs made available by NIST were used to extract',\n",
       " 'normalized bitmaps of handwritten digits from a preprinted form. From a',\n",
       " 'total of 43 people, 30 contributed to the training set and different 13',\n",
       " 'to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of',\n",
       " '4x4 and the number of on pixels are counted in each block. This generates',\n",
       " 'an input matrix of 8x8 where each element is an integer in the range',\n",
       " '0..16. This reduces dimensionality and gives invariance to small',\n",
       " 'distortions.',\n",
       " '',\n",
       " 'For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.',\n",
       " 'T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.',\n",
       " 'L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,',\n",
       " '1994.',\n",
       " '',\n",
       " 'References',\n",
       " '----------',\n",
       " '  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their',\n",
       " '    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of',\n",
       " '    Graduate Studies in Science and Engineering, Bogazici University.',\n",
       " '  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.',\n",
       " '  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.',\n",
       " '    Linear dimensionalityreduction using relevance weighted LDA. School of',\n",
       " '    Electrical and Electronic Engineering Nanyang Technological University.',\n",
       " '    2005.',\n",
       " '  - Claudio Gentile. A New Approximate Maximal Margin Classification',\n",
       " '    Algorithm. NIPS. 2000.',\n",
       " '']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data description\n",
    "digits_dict[\"DESCR\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data\n",
    "\n",
    "data = digits_dict[\"data\"]\n",
    "\n",
    "target = digits_dict[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at target variable\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Data Shape (1797, 64)\n",
      "Label Data Shape (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Print to show there are 1797 images (8 by 8 images for a dimensionality of 64)\n",
    "print('Image Data Shape' , digits_dict.data.shape)\n",
    "# Print to show there are 1797 labels (integers from 09)\n",
    "print(\"Label Data Shape\", digits_dict.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  3., 15., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  7., 15., 16., 16.,  2.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View data of single digit\n",
    "\n",
    "data[1].reshape(8, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you tell what number this data is showing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABHsAAADvCAYAAABi6dioAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHS1JREFUeJzt3XuQpXddJvDny8xkQm4kEK4JGBCIBagExiAbgSJZ1iAIcXUhKKiU7li4siCsLrrsilsubpVbGC8YjFyCEgmCoFbkIiIiljGQm0DMRQi3EJIQICEJkGQyv/2jO7vDODN9enJ+fX7nzedT1TXdp08/53tO5sl0f/s9563WWgAAAACYhnssegAAAAAA5seyBwAAAGBCLHsAAAAAJsSyBwAAAGBCLHsAAAAAJsSyBwAAAGBCLHsAAAAAJsSyZwNU1c27vO2sqm/s8vGPbfAsB1ZVq6qj1/l1P1FVn1ud+R1Vda9eM8JGWfZuVtVDquqcqrpm9Wsf0HNG2CgT6OYpVXVuVd1YVV+sqtOr6qCec8JGmEA3v7+qPlFVN1TV9VX19qq6f885YSMsezd3+/q33pWv5/+z7NkArbVD7nxL8rkkP7jLZWetJ6uqNveZcp+3eVyS307y3CQPTFJJfmuj54B5W/ZuJrkjyTlJnrOA24ZuJtDNw5L8jyQPSPKYJI9M8uoFzAFzNYFu/lOSp7XWDk9ydJKrk/zOAuaAuZpAN++87ZOSHLWo258ay54BVNUJVXXe6m8Ar66q37yzZLtsRl9UVZ9K8onVy59RVf+y+puJ06rqH6vq+btk/kxVXV5VX6mqv6yqO0vzd6t/Xr666T1lhhFfkORPW2vnttZuyso3sM+tqgPn9iDAgEbvZmvtC6211yW5cL73HMa2BN38w9ba+1tr32itfTnJG5KcMNcHAQa0BN28prX2xV0uuiPJw+dy52Fgo3dzNe+AJKcl+c9zu+N3c5Y9Y7g9yc8luXeSJyX5wSQ/vdt1npnk8UmOq5Wnarwtyc8nuW9Wfivx+DuvWFWnJnnpas79k1yU5C2rn37y6p/Hrm56/6yqtq6WeNte5nt0Vn4TkiRprV2SZFOSb9+/uwtLY/Ruwt3VsnXzyUkuWd9dhKU0fDer6hFVdUOSryf5T0l+4y7cX1gWw3czyX9N8u4kl+33veRbWPYMoLX2kdbaR1trd7TWPpXk9UmestvV/ldr7YbW2jeSPCvJR1tr57TWbk/yf5J8dZfr/kySX2utXbH6+V9N8n21l+ckt9Zuba0d3lo7fy8jHpLkxt0uuynJoeu6o7BklqCbcLe0TN2sqmcm+Q+rmTBpy9DN1tq/rD6N635JXpXkiv28u7A0Ru9mVT0syY8n+Z936Y7yLSx7BlBVj6qq91TVtVX1taw8TerI3a72+V3ef9CuH7fWdib5wi6f/7Ykr1vdnt6Q5EtJdmTlucn74+asvP7Arg7NysIHJmsJugl3S8vSzap6UpIzk5zSWvv0XcmCZbAs3Vy9reuTvDXJn1dV3dU8GNkSdPN3kvy31tot+/n17IFlzxj+ICuvufHtrbXDsrLR3P0fnbbL+1/MLkWqqnvkW1/I6vNJfnJ1e3rn2z1baxfsljOrS5J89y6396isPMf5U/uRBctk9G7C3dXw3ayqJyR5Z5Ifa619eH8yYAkN383dbM7KD7XOlsfUjd7NE5P8dlVdk+Qzq5ddWFU/vB9ZrLLsGcOhSW5srd1cVY9O8h/XuP5fJHlCVf3A6gtrvSzJEbt8/nVJXllVxyZJVR1xZ1Faa7dm5SlZD1vHfG9J8sNV9b1VdUhWDtN7W2vtm+vIgGU0ejdTKy+UvnX1w61VtXVf14eJGLqbtXIWy3OSbG+tvW/Wr4MJGL2bP1JVD68V98/KU1P+0dEE3A0M3c0kxyR57OrbE1Yv+/4kf7mODHZj2TOGn0/y01V1c5LXZuXFsPaqrZxF4HlZOR369VnZun48ya2rn39rkt9N8s7Vw/QuTvK0XSL+R5K3rx5296zVF8y6uaq+Zy+3d2FWXoDrHUmuzcrfm5fs752FJTJ0N1cXPd9Yva1k5Tchu7++FkzR0N1M8otZeRHMP1q93s1VdcH+3llYIqN389uS/HVWXqLg4iS3JHnOft1TWC5Dd7O1dm1bOVveNVn5eTNJvuTggrumWvPMgWW3um29JskPttbOXfQ8wArdhDHpJoxJN2FMurmcHNmzpKrq6VV1r9Xf7P9KVk4f6beGsGC6CWPSTRiTbsKYdHP5WfYsrycn+XSS65KclOSHWmu3LXYkILoJo9JNGJNuwph0c8l5GhcAAADAhDiyBwAAAGBCNvcIPaC2tgNzcI/oDbHjyL6zP+ABX+ma/4VbDu+anyQHXnV71/x2+46u+T19M7fktnZrLXqO3S17L3s74Dv67r633qP/3+kbrj20a/6mLy/3mWlvylevb63dd9Fz7E43923n4X0fm2MefO3aV7qLrrn9sK75t122s2t+b7rZx21H9Z39Mff5Utf8r+zc1DU/Sb58ed/HaJm/n010c1nV5i4/4v8/Ox/W/3iRusKzxfZl1m52+ZtwYA7OE+qkHtEb4voffmLX/F94+dld8//7Bc/ump8kj3zZF7vm77im/zffvZzXPrDoEfZo2XvZ24Pe3HdR8oiDruuanyR/9poTu+YfceZyn3zhr9s7PrvoGfZEN/ft6yc+oWv+G057Tdf8JPn1L57cNf/q772pa35vutnHp1/c9/vZj/zE6V3zz77piK75SfJHTzm+a/4yfz+b6Oay2nTk/brmf+P37tk1P0kOeNqQf/WGMWs3PY0LAAAAYEIsewAAAAAmxLIHAAAAYEIsewAAAAAmxLIHAAAAYEIsewAAAAAmxLIHAAAAYEJmWvZU1clVdXlVfbKqXtF7KGA2uglj0k0Yk27CmHQT5m/NZU9VbUry2iRPT/KoJM+rqkf1HgzYN92EMekmjEk3YUy6CX3McmTP8Uk+2Vq7srV2W5Kzkzy771jADHQTxqSbMCbdhDHpJnQwy7LnqCSf3+Xjq1Yv+xZVtb2qzq+q82/PrfOaD9i7Nbupl7AQuglj0k0Yk25CB7Mse2oPl7V/dUFrZ7TWtrXWtm3J1rs+GbCWNbupl7AQuglj0k0Yk25CB7Mse65K8uBdPj46ydV9xgHWQTdhTLoJY9JNGJNuQgezLHs+muQRVfXQqjogyalJ/qLvWMAMdBPGpJswJt2EMekmdLB5rSu01nZU1c8leV+STUne2Fq7pPtkwD7pJoxJN2FMuglj0k3oY81lT5K01t6d5N2dZwHWSTdhTLoJY9JNGJNuwvzN8jQuAAAAAJaEZQ8AAADAhFj2AAAAAEyIZQ8AAADAhFj2AAAAAEyIZQ8AAADAhMx06vW7m194+dld80899Ktd8087/Oau+Unylxe+r2v+41/1oq75R55xbtd8ls9nbrp31/w3PeTDXfOT5A+e/KSu+Uec2TWeJbXzKcd1zf/wa3+/a/4Vt3eNT5I8+z4Xdc0/PQ/vmk8fV5x+fNf8Xz+x7/ezj/mtn+2a/4mX/F7X/CT5nScd0zX/kLdf2zUf9uTTL+r7b8Jtn9jZNT9JHp7Pdr+NuwNH9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwISsueypqjdW1XVV9YmNGAiYjW7CmHQTxqSbMCbdhD5mObLnzCQnd54DWL8zo5swojOjmzCiM6ObMKIzo5swd2sue1prf5fkKxswC7AOuglj0k0Yk27CmHQT+tg8r6Cq2p5ke5IcmIPmFQvcBXoJY9JNGJNuwph0E9Zvbi/Q3Fo7o7W2rbW2bUu2zisWuAv0EsakmzAm3YQx6Sasn7NxAQAAAEyIZQ8AAADAhMxy6vW3Jjk3ybFVdVVV/VT/sYC16CaMSTdhTLoJY9JN6GPNF2hurT1vIwYB1kc3YUy6CWPSTRiTbkIfnsYFAAAAMCGWPQAAAAATYtkDAAAAMCGWPQAAAAATYtkDAAAAMCGWPQAAAAATsuap10e048THd80/9dCLu+Y//eRTu+bf62OXdc1Pkuf8/Uld879y3B1d84/smk4PO59yXNf833/k73bNTw7unJ8c9vEDut8G7O7KU7Z2zX/19cd2zX/DB57aNT9JPvXc13XNP71rOr18x+lf65r/R796fNf8V37orV3zz77piK75SXLI28/rfhuwu033v1/X/Bf8+w90zX/bm/r+HJgkmx7d99/+3u645PJFj5DEkT0AAAAAk2LZAwAAADAhlj0AAAAAE2LZAwAAADAhlj0AAAAAE2LZAwAAADAhlj0AAAAAE2LZAwAAADAhay57qurBVfXBqrq0qi6pqpdsxGDAvukmjEk3YUy6CWPSTehj8wzX2ZHk5a21C6vq0CQXVNX7W2v/3Hk2YN90E8akmzAm3YQx6SZ0sOaRPa21L7bWLlx9/6YklyY5qvdgwL7pJoxJN2FMuglj0k3oY12v2VNVxyQ5Lsl5PYYB9o9uwph0E8akmzAm3YT5meVpXEmSqjokyZ8meWlr7Wt7+Pz2JNuT5MAcNLcBgX3bVzf1EhZHN2FMuglj0k2Yr5mO7KmqLVkp3lmttXfu6TqttTNaa9taa9u2ZOs8ZwT2Yq1u6iUshm7CmHQTxqSbMH+znI2rkrwhyaWttdf0HwmYhW7CmHQTxqSbMCbdhD5mObLnhCQvSHJiVV28+vYDnecC1qabMCbdhDHpJoxJN6GDNV+zp7X290lqA2YB1kE3YUy6CWPSTRiTbkIf6zobFwAAAABjs+wBAAAAmBDLHgAAAIAJsewBAAAAmBDLHgAAAIAJsewBAAAAmJA1T70+om/ep+/Yr7zuO7vm7/zYZV3zN8JHP/7tix6BwXzuVf+ma/6fv/A3uuY/csvBXfM3wlF/9eWu+Xd0TWdZHfu/r+ya/7bPndQ1/z0v7fv/liR56iU/2jX/gHy2az59dP9+8Lu+o2v8qYd+tWv+c67s2/0k2fyAvj9T7Ljm2q75LKdPv+jhXfNPu9e7uuZ/6Dfv2TU/SS5947au+fe4sW/3H/7zXeNn5sgeAAAAgAmx7AEAAACYEMseAAAAgAmx7AEAAACYEMseAAAAgAmx7AEAAACYEMseAAAAgAmx7AEAAACYkDWXPVV1YFV9pKr+qaouqapf3YjBgH3TTRiTbsKYdBPGpJvQx+YZrnNrkhNbazdX1ZYkf19V72mt/WPn2YB9000Yk27CmHQTxqSb0MGay57WWkty8+qHW1bfWs+hgLXpJoxJN2FMuglj0k3oY6bX7KmqTVV1cZLrkry/tXZe37GAWegmjEk3YUy6CWPSTZi/mZY9rbU7WmuPTXJ0kuOr6jG7X6eqtlfV+VV1/u25dd5zAnuwVjf1EhZDN2FMuglj0k2Yv3Wdjau1dkOSv01y8h4+d0ZrbVtrbduWbJ3TeMAs9tZNvYTF0k0Yk27CmHQT5meWs3Hdt6oOX33/nkn+bZLLeg8G7Jtuwph0E8akmzAm3YQ+Zjkb1wOTvLmqNmVlOfQnrbVz+o4FzEA3YUy6CWPSTRiTbkIHs5yN62NJjtuAWYB10E0Yk27CmHQTxqSb0Me6XrMHAAAAgLFZ9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwIRY9gAAAABMyJqnXh/RN4/ou6M669wnds1/ZD7SNX8jbL7XbV3zd9x4QNd85u8hr/qHrvkvPf2Huua/+6K/6pq/EW4/8qCu+X47sJw23f9+XfMvf8XDuub/1Ekf6Jq/Ee75/G90zb+jazrLaufHLuua/4zHfX/X/OPee3XX/CTJe/vGX3Tyg7rm77jm2q75d1df/cm+Pwteuv33uuY/+tztXfOPziVd85Pk0ye/vmv+d//Gz3bNH4Xv3QEAAAAmxLIHAAAAYEIsewAAAAAmxLIHAAAAYEIsewAAAAAmxLIHAAAAYEIsewAAAAAmxLIHAAAAYEJmXvZU1aaquqiqzuk5ELA+uglj0k0Yj17CmHQT5m89R/a8JMmlvQYB9ptuwph0E8ajlzAm3YQ5m2nZU1VHJ3lGktf3HQdYD92EMekmjEcvYUy6CX3MemTPaUl+McnOjrMA66ebMCbdhPHoJYxJN6GDNZc9VfXMJNe11i5Y43rbq+r8qjr/9tw6twGBPZulm3oJG083YTy+n4Ux6Sb0M8uRPSckeVZVfSbJ2UlOrKq37H6l1toZrbVtrbVtW7J1zmMCe7BmN/USFkI3YTy+n4Ux6SZ0suayp7X2S621o1trxyQ5NcnftNae330yYJ90E8akmzAevYQx6Sb0s56zcQEAAAAwuM3ruXJr7W+T/G2XSYD9ppswJt2E8egljEk3Yb4c2QMAAAAwIZY9AAAAABNi2QMAAAAwIZY9AAAAABNi2QMAAAAwIZY9AAAAABNi2QMAAAAwIZsXPcD+OPCrO7vmf893fqpr/o1d05PND7h/51tInvuoC7rm/8l7vq9rPkzRdY+7Z9f8B3yoazydXPrrD+ma/+mTX9c1v7fjf/m/dL+NI649t/ttwEbbcc21XfMvOvlBXfOT5MtvPLRr/rW/cu+u+Y98Ud//BndXW2/s+7PmFbff0jX/kiee1TX/1R87tmv+Rjjqjz/ZNf+Orumzc2QPAAAAwIRY9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwIRY9gAAAABMiGUPAAAAwIRsnuVKVfWZJDdl5ZTxO1pr23oOBcxGN2FMuglj0k0Yk27C/M207Fn11Nba9d0mAfaXbsKYdBPGpJswJt2EOfI0LgAAAIAJmXXZ05L8VVVdUFXbew4ErItuwph0E8akmzAm3YQ5m/VpXCe01q6uqvsleX9VXdZa+7tdr7Bayu1JcmAOmvOYwF7ss5t6CQujmzAm3YQx6SbM2UxH9rTWrl7987ok70py/B6uc0ZrbVtrbduWbJ3vlMAerdVNvYTF0E0Yk27CmHQT5m/NZU9VHVxVh975fpJ/l+QTvQcD9k03YUy6CWPSTRiTbkIfszyN6/5J3lVVd17/j1tr7+06FTAL3YQx6SaMSTdhTLoJHay57GmtXZnkuzdgFmAddBPGpJswJt2EMekm9OHU6wAAAAATYtkDAAAAMCGWPQAAAAATYtkDAAAAMCGWPQAAAAATYtkDAAAAMCGWPQAAAAATsnnRA+yPwy6/sWv+rxx9Ttf8H9/+sq75W075Utf8jfDQXzp30SMATMLD33xH1/xXbzu2a/4vH3l51/yPvPr0rvlJ8tQfe3bX/FvOelDX/CPO9G/yMrri9OO75j/ob6pr/jeP6P876T981Gu65p9yw4u65tPHQe86r2v+i991Qtf8nU85rmv+a//wd7vmJ8mjz93eNf/oay/pmj8KR/YAAAAATIhlDwAAAMCEWPYAAAAATIhlDwAAAMCEWPYAAAAATIhlDwAAAMCEWPYAAAAATIhlDwAAAMCEzLTsqarDq+odVXVZVV1aVU/sPRiwNt2EMekmjEk3YUy6CfO3ecbr/VaS97bWfqSqDkhyUMeZgNnpJoxJN2FMuglj0k2YszWXPVV1WJInJ/nJJGmt3Zbktr5jAWvRTRiTbsKYdBPGpJvQxyxP43pYki8leVNVXVRVr6+qg3e/UlVtr6rzq+r823Pr3AcF/pU1u6mXsBC6CWPSTRiTbkIHsyx7Nid5XJLTW2vHJbklySt2v1Jr7YzW2rbW2rYt2TrnMYE9WLObegkLoZswJt2EMekmdDDLsueqJFe11s5b/fgdWSkjsFi6CWPSTRiTbsKYdBM6WHPZ01q7Jsnnq+rY1YtOSvLPXacC1qSbMCbdhDHpJoxJN6GPWc/G9eIkZ62+MvqVSV7YbyRgHXQTxqSbMCbdhDHpJszZTMue1trFSbZ1ngVYJ92EMekmjEk3YUy6CfM3y2v2AAAAALAkLHsAAAAAJsSyBwAAAGBCLHsAAAAAJsSyBwAAAGBCLHsAAAAAJmSmU6+PZufHLuua/9zTX941/5Uvf2vX/NM+dVLX/CT56GM3db8N2NUd117XNf+plzy7a/4HH/3nXfOTZMf33dj3Bn6zbzx93ONDF3XN/9B33bNr/gef8sKu+Tte+ZWu+Un//j/0yT/dNf+IM7vG08mWG/p+r/biXzu7a/5GOOUfXtQ1/2E/enHXfNiTLdd/vWv+I7cc3DU/Se79lkO638bdgSN7AAAAACbEsgcAAABgQix7AAAAACbEsgcAAABgQix7AAAAACbEsgcAAABgQix7AAAAACbEsgcAAABgQtZc9lTVsVV18S5vX6uql27EcMDe6SaMSTdhTLoJY9JN6GPzWldorV2e5LFJUlWbknwhybs6zwWsQTdhTLoJY9JNGJNuQh/rfRrXSUk+1Vr7bI9hgP2mmzAm3YQx6SaMSTdhTtY8smc3pyZ5654+UVXbk2xPkgNz0F0cC1inPXZTL2HhdBPGpJswJt2EOZn5yJ6qOiDJs5K8fU+fb62d0Vrb1lrbtiVb5zUfsIZ9dVMvYXF0E8akmzAm3YT5Ws/TuJ6e5MLW2rW9hgH2i27CmHQTxqSbMCbdhDlaz7LnednLU7iAhdJNGJNuwph0E8akmzBHMy17quqgJE9L8s6+4wDroZswJt2EMekmjEk3Yf5meoHm1trXk9yn8yzAOukmjEk3YUy6CWPSTZi/9Z56HQAAAICBWfYAAAAATIhlDwAAAMCEWPYAAAAATIhlDwAAAMCEWPYAAAAATEi11uYfWvWlJJ9dx5ccmeT6uQ+yccy/WKPN/22ttfsueojd3Q17mSz/fTD/fOnmGJZ9/mT578No8+vmGMy/eKPdB90cg/kXa8T5Z+pml2XPelXV+a21bYueY3+Zf7GWff5RTeFxXfb7YH72ZNkf12WfP1n++7Ds849q2R9X8y/eFO7DiJb9cTX/Yi3z/J7GBQAAADAhlj0AAAAAEzLKsueMRQ9wF5l/sZZ9/lFN4XFd9vtgfvZk2R/XZZ8/Wf77sOzzj2rZH1fzL94U7sOIlv1xNf9iLe38Q7xmDwAAAADzMcqRPQAAAADMgWUPAAAAwIQsdNlTVSdX1eVV9cmqesUiZ1mvqnpwVX2wqi6tqkuq6iWLnml/VNWmqrqoqs5Z9CzrVVWHV9U7quqy1f8OT1z0TFOhm4unm+yJbi6ebrInurl4usme6Obi6ebiLOw1e6pqU5IrkjwtyVVJPprkea21f17IQOtUVQ9M8sDW2oVVdWiSC5Kcsizz36mqXpZkW5LDWmvPXPQ861FVb07y4dba66vqgCQHtdZuWPRcy043x6Cb7E43x6Cb7E43x6Cb7E43x6Cbi7PII3uOT/LJ1tqVrbXbkpyd5NkLnGddWmtfbK1duPr+TUkuTXLUYqdan6o6Oskzkrx+0bOsV1UdluTJSd6QJK2125apeIPTzQXTTfZCNxdMN9kL3Vww3WQvdHPBdHOxFrnsOSrJ53f5+Kos2V/eO1XVMUmOS3LeYidZt9OS/GKSnYseZD88LMmXkrxp9bDA11fVwYseaiJ0c/F0kz3RzcXTTfZENxdPN9kT3Vw83VygRS57ag+XLd154KvqkCR/muSlrbWvLXqeWVXVM5Nc11q7YNGz7KfNSR6X5PTW2nFJbkmyVM/DHZhuLpBusg+6uUC6yT7o5gLpJvugmwukm4u3yGXPVUkevMvHRye5ekGz7Jeq2pKV4p3VWnvnoudZpxOSPKuqPpOVQxpPrKq3LHakdbkqyVWttTu32+/IShm563RzsXSTvdHNxdJN9kY3F0s32RvdXCzdXLBFLns+muQRVfXQ1Rc7OjXJXyxwnnWpqsrK8/cuba29ZtHzrFdr7Zdaa0e31o7JymP/N6215y94rJm11q5J8vmqOnb1opOSLNWLlQ1MNxdIN9kH3Vwg3WQfdHOBdJN90M0F0s3F27yoG26t7aiqn0vyviSbkryxtXbJoubZDyckeUGSj1fVxauX/XJr7d0LnOnu5sVJzlr9n/eVSV644HkmQTeZA93sQDeZA93sQDeZA93sQDeZg6Xu5sJOvQ4AAADA/C3yaVwAAAAAzJllDwAAAMCEWPYAAAAATIhlDwAAAMCEWPYAAAAATIhlDwAAAMCEWPYAAAAATMj/BYyDekbhsJvQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c152828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use matplotlib to view the images\n",
    "plt.figure(figsize=(20, 4))\n",
    "for index, (image, label) in enumerate(zip(data[0:5], target[0:5])):\n",
    "    plt.subplot(1, 5, index+1)\n",
    "    plt.imshow(image.reshape(8,8))\n",
    "    plt.title('Target: %i' % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a logistic regression algorithm to model this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT: 0.8358831710709318\n",
      "LR: 0.933240611961057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Step 1. train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=.4, random_state=19)\n",
    "\n",
    "\n",
    "#Step 2. Fit DecisionTreeClassifier with max_depth 10 on training data\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "#Step 3. Make predictions on the X_test using the model\n",
    "\n",
    "preds1 = dt.predict(X_test)\n",
    "preds2 = lr.predict(X_test)\n",
    "\n",
    "#Step 4. Score predictions\n",
    "print(\"DT: \"+str(dt.score(X_test, y_test)))\n",
    "print(\"LR: \"+str(lr.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9\n",
       "0  61   0   0   0   1   0   0   0   0   0\n",
       "1   0  63   0   0   0   0   0   0   2   0\n",
       "2   0   0  77   0   0   0   0   0   1   0\n",
       "3   0   0   0  65   0   1   0   3   6   1\n",
       "4   0   1   0   0  66   0   0   1   0   1\n",
       "5   0   2   0   0   0  69   0   0   0   2\n",
       "6   0   0   0   0   1   1  72   0   1   0\n",
       "7   0   1   0   0   0   0   0  69   1   4\n",
       "8   0   5   0   1   0   1   0   0  61   0\n",
       "9   0   4   0   2   1   1   0   0   2  68"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create confusion matrix from predictions and y_test\n",
    "\n",
    "cm_digits = pd.DataFrame(confusion_matrix(y_test, preds2))\n",
    "\n",
    "cm_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1bd8e518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Heatmap version\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_digits, cmap='coolwarm', annot=True)\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"Actual Labels\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_mldata\n",
    "# mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[61,  0,  0,  0,  1,  0,  0,  0,  0,  0],\n",
       "       [ 0, 63,  0,  0,  0,  0,  0,  0,  2,  0],\n",
       "       [ 0,  0, 77,  0,  0,  0,  0,  0,  1,  0],\n",
       "       [ 0,  0,  0, 65,  0,  1,  0,  3,  6,  1],\n",
       "       [ 0,  1,  0,  0, 66,  0,  0,  1,  0,  1],\n",
       "       [ 0,  2,  0,  0,  0, 69,  0,  0,  0,  2],\n",
       "       [ 0,  0,  0,  0,  1,  1, 72,  0,  1,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0, 69,  1,  4],\n",
       "       [ 0,  5,  0,  1,  0,  1,  0,  0, 61,  0],\n",
       "       [ 0,  4,  0,  2,  1,  1,  0,  0,  2, 68]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some of the images we got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify indices of wrong predictions\n",
    "index = 0\n",
    "false_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_preds = y_test!=preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_preds[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,x in enumerate(false_preds):\n",
    "    if x:\n",
    "        false_index.append(i)\n",
    "\n",
    "len(false_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassifiedIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a10421ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "for index, i in enumerate(false_index[0:5]):\n",
    "    plt.subplot(1,5,index+1)\n",
    "    image = X_test[false_index[index]]\n",
    "    plt.imshow(image.reshape(8,8))\n",
    "    plt.title('actual: {}, predict {}'.format(y_test[i], preds2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6   7   8   9\n",
       "0  61   0   0   0   1   0   0   0   0   0\n",
       "1   0  63   0   0   0   0   0   0   2   0\n",
       "2   0   0  77   0   0   0   0   0   1   0\n",
       "3   0   0   0  65   0   1   0   3   6   1\n",
       "4   0   1   0   0  66   0   0   1   0   1\n",
       "5   0   2   0   0   0  69   0   0   0   2\n",
       "6   0   0   0   0   1   1  72   0   1   0\n",
       "7   0   1   0   0   0   0   0  69   1   4\n",
       "8   0   5   0   1   0   1   0   0  61   0\n",
       "9   0   4   0   2   1   1   0   0   2  68"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class exercise time: \n",
    "\n",
    "Which digit did our model do the best at identifying? What digit was the worst?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    1   2   3   4   5   6   7   8   9\n",
       "0   0   0   0   1   0   0   0   0   0\n",
       "1  63   0   0   0   0   0   0   2   0\n",
       "2   0  77   0   0   0   0   0   1   0\n",
       "3   0   0  65   0   1   0   3   6   1\n",
       "4   1   0   0  66   0   0   1   0   1\n",
       "5   2   0   0   0  69   0   0   0   2\n",
       "6   0   0   0   1   1  72   0   1   0\n",
       "7   1   0   0   0   0   0  69   1   4\n",
       "8   5   0   1   0   1   0   0  61   0\n",
       "9   4   0   2   1   1   0   0   2  68"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits.drop(0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "657"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits.drop(0).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits.drop(1)[:][1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     2\n",
       "2     1\n",
       "3     6\n",
       "4     0\n",
       "5     0\n",
       "6     1\n",
       "7     1\n",
       "8    61\n",
       "9     2\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_digits[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_digits.drop(1, axis=1)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9986111111111111,\n",
       " 1: 0.9791955617198336,\n",
       " 2: 0.9986111111111111,\n",
       " 3: 0.9808219178082191,\n",
       " 4: 0.9916897506925207,\n",
       " 5: 0.9889349930843707,\n",
       " 6: 0.9958448753462604,\n",
       " 7: 0.9862068965517241,\n",
       " 8: 0.9724517906336089,\n",
       " 9: 0.9753086419753086}"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score answer \n",
    "\n",
    "acc_scores = {}\n",
    "\n",
    "for i in range(10):\n",
    "    tp = cm_digits[i][i]\n",
    "    tn = cm_digits.drop(i, axis=1).sum().sum()\n",
    "    fp = cm_digits.drop(i)[:][i].sum()\n",
    "    fn = cm_digits.drop(i, axis=1).iloc[i].sum()\n",
    "    acc_score = (tp+tn)/(tp+tn+fp+fn)\n",
    "    acc_scores[i] = acc_score\n",
    "\n",
    "acc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9986111111111111),\n",
       " (2, 0.9986111111111111),\n",
       " (6, 0.9958448753462604),\n",
       " (4, 0.9916897506925207),\n",
       " (5, 0.9889349930843707),\n",
       " (7, 0.9862068965517241),\n",
       " (3, 0.9808219178082191),\n",
       " (1, 0.9791955617198336),\n",
       " (9, 0.9753086419753086),\n",
       " (8, 0.9724517906336089)]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(acc_scores.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0,\n",
       " 1: 0.8289473684210527,\n",
       " 2: 1.0,\n",
       " 3: 0.9558823529411765,\n",
       " 4: 0.9565217391304348,\n",
       " 5: 0.9452054794520548,\n",
       " 6: 1.0,\n",
       " 7: 0.9452054794520548,\n",
       " 8: 0.8243243243243243,\n",
       " 9: 0.8947368421052632}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision score answer \n",
    "\n",
    "prec_scores = {}\n",
    "\n",
    "for i in range(10):\n",
    "    tp = cm_digits[i][i]\n",
    "#     tn = cm_digits.drop(i, axis=1).sum().sum()\n",
    "    fp = cm_digits.drop(i)[:][i].sum()\n",
    "#     fn = cm_digits.drop(i, axis=1).iloc[i].sum()\n",
    "    prec_score = (tp)/(tp+fp)\n",
    "    prec_scores[i] = prec_score\n",
    "\n",
    "prec_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.0),\n",
       " (2, 1.0),\n",
       " (6, 1.0),\n",
       " (4, 0.9565217391304348),\n",
       " (3, 0.9558823529411765),\n",
       " (5, 0.9452054794520548),\n",
       " (7, 0.9452054794520548),\n",
       " (9, 0.8947368421052632),\n",
       " (1, 0.8289473684210527),\n",
       " (8, 0.8243243243243243)]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(prec_scores.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9838709677419355,\n",
       " 1: 0.9692307692307692,\n",
       " 2: 0.9871794871794872,\n",
       " 3: 0.8552631578947368,\n",
       " 4: 0.9565217391304348,\n",
       " 5: 0.9452054794520548,\n",
       " 6: 0.96,\n",
       " 7: 0.92,\n",
       " 8: 0.8970588235294118,\n",
       " 9: 0.8717948717948718}"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall score answer \n",
    "\n",
    "recall_scores = {}\n",
    "\n",
    "for i in range(10):\n",
    "    tp = cm_digits[i][i]\n",
    "#     tn = cm_digits.drop(i, axis=1).sum().sum()\n",
    "#     fp = cm_digits.drop(i)[:][i].sum()\n",
    "    fn = cm_digits.drop(i, axis=1).iloc[i].sum()\n",
    "    rec_score = (tp)/(tp+fn)\n",
    "    recall_scores[i] = rec_score\n",
    "\n",
    "recall_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.9871794871794872),\n",
       " (0, 0.9838709677419355),\n",
       " (1, 0.9692307692307692),\n",
       " (6, 0.96),\n",
       " (4, 0.9565217391304348),\n",
       " (5, 0.9452054794520548),\n",
       " (7, 0.92),\n",
       " (8, 0.8970588235294118),\n",
       " (9, 0.8717948717948718),\n",
       " (3, 0.8552631578947368)]"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(recall_scores.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Class Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As if often the case in real-world machine learning, there is a huge imbalance between the distribution of class of the variable we're trying to predicting.\n",
    "\n",
    "Class False: 98.23%\n",
    "\n",
    "Class True: 1.77%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrated example\n",
    "![www](http://www.svds.com/wp-content/uploads/2016/08/messy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem that imbalanced machine learning project present that is they render the accuracy score metric almost irrelevant.\n",
    "\n",
    "If our null accuracy is 99.5%, then we don't have much room for improvement.\n",
    "\n",
    "<br>\n",
    "\n",
    "Imagine a confusion with 1000 TNs, 20 FPs, 15 FNs, and 25 TPs. What would the accuracy, precision, and recall scores be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted_False</th>\n",
       "      <th>Predicted_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual_False</th>\n",
       "      <td>1000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual_True</th>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Predicted_False  Predicted_True\n",
       "Actual_False             1000              20\n",
       "Actual_True                15              25"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix form\n",
    "pd.DataFrame(data=[[1000, 20], [15, 25]], \n",
    "             columns=[\"Predicted_False\", \"Predicted_True\"], \n",
    "             index=[\"Actual_False\", \"Actual_True\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign variables\n",
    "\n",
    "tn = 1000.\n",
    "fp = 20.\n",
    "fn = 15.\n",
    "tp = 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9669811320754716"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy score\n",
    "\n",
    "(tp + tn)/(tn + fp + fn + tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good score! Or is it???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622641509433962"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Whats the null accuracy\n",
    "\n",
    "(tn + fp)/(tn + fp + fn + tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hardly beat the null accuracy. Now let's calculate precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "\n",
    "tp/(tp + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall\n",
    "tp/(tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have a good model or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move onto the real thing by modeling credit card fraud data\n",
    "\n",
    "https://www.kaggle.com/dalpozz/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.249999</td>\n",
       "      <td>-1.221637</td>\n",
       "      <td>0.383930</td>\n",
       "      <td>-1.234899</td>\n",
       "      <td>-1.485419</td>\n",
       "      <td>-0.753230</td>\n",
       "      <td>-0.689405</td>\n",
       "      <td>-0.227487</td>\n",
       "      <td>-2.094011</td>\n",
       "      <td>1.323729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231809</td>\n",
       "      <td>-0.483285</td>\n",
       "      <td>0.084668</td>\n",
       "      <td>0.392831</td>\n",
       "      <td>0.161135</td>\n",
       "      <td>-0.354990</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.042422</td>\n",
       "      <td>121.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.752417</td>\n",
       "      <td>0.345485</td>\n",
       "      <td>2.057323</td>\n",
       "      <td>-1.468643</td>\n",
       "      <td>-1.158394</td>\n",
       "      <td>-0.077850</td>\n",
       "      <td>-0.608581</td>\n",
       "      <td>0.003603</td>\n",
       "      <td>-0.436167</td>\n",
       "      <td>0.747731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499625</td>\n",
       "      <td>1.353650</td>\n",
       "      <td>-0.256573</td>\n",
       "      <td>-0.065084</td>\n",
       "      <td>-0.039124</td>\n",
       "      <td>-0.087086</td>\n",
       "      <td>-0.180998</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>15.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.103215</td>\n",
       "      <td>-0.040296</td>\n",
       "      <td>1.267332</td>\n",
       "      <td>1.289091</td>\n",
       "      <td>-0.735997</td>\n",
       "      <td>0.288069</td>\n",
       "      <td>-0.586057</td>\n",
       "      <td>0.189380</td>\n",
       "      <td>0.782333</td>\n",
       "      <td>-0.267975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024612</td>\n",
       "      <td>0.196002</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.103758</td>\n",
       "      <td>0.364298</td>\n",
       "      <td>-0.382261</td>\n",
       "      <td>0.092809</td>\n",
       "      <td>0.037051</td>\n",
       "      <td>12.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.962496</td>\n",
       "      <td>0.328461</td>\n",
       "      <td>-0.171479</td>\n",
       "      <td>2.109204</td>\n",
       "      <td>1.129566</td>\n",
       "      <td>1.696038</td>\n",
       "      <td>0.107712</td>\n",
       "      <td>0.521502</td>\n",
       "      <td>-1.191311</td>\n",
       "      <td>0.724396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143997</td>\n",
       "      <td>0.402492</td>\n",
       "      <td>-0.048508</td>\n",
       "      <td>-1.371866</td>\n",
       "      <td>0.390814</td>\n",
       "      <td>0.199964</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>-0.014605</td>\n",
       "      <td>34.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "12  1.249999 -1.221637  0.383930 -1.234899 -1.485419 -0.753230 -0.689405   \n",
       "15 -0.752417  0.345485  2.057323 -1.468643 -1.158394 -0.077850 -0.608581   \n",
       "16  1.103215 -0.040296  1.267332  1.289091 -0.735997  0.288069 -0.586057   \n",
       "21  0.962496  0.328461 -0.171479  2.109204  1.129566  1.696038  0.107712   \n",
       "\n",
       "          V8        V9       V10  ...         V21       V22       V23  \\\n",
       "0   0.098698  0.363787  0.090794  ...   -0.018307  0.277838 -0.110474   \n",
       "12 -0.227487 -2.094011  1.323729  ...   -0.231809 -0.483285  0.084668   \n",
       "15  0.003603 -0.436167  0.747731  ...    0.499625  1.353650 -0.256573   \n",
       "16  0.189380  0.782333 -0.267975  ...   -0.024612  0.196002  0.013802   \n",
       "21  0.521502 -1.191311  0.724396  ...    0.143997  0.402492 -0.048508   \n",
       "\n",
       "         V24       V25       V26       V27       V28  Amount  Class  \n",
       "0   0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "12  0.392831  0.161135 -0.354990  0.026416  0.042422  121.50      0  \n",
       "15 -0.065084 -0.039124 -0.087086 -0.180998  0.129394   15.99      0  \n",
       "16  0.103758  0.364298 -0.382261  0.092809  0.037051   12.99      0  \n",
       "21 -1.371866  0.390814  0.199964  0.016371 -0.014605   34.09      0  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../../data/fraud.csv\"\n",
    "\n",
    "fraud = pd.read_csv(path, index_col=[0])\n",
    "\n",
    "fraud.drop(\"Time\", axis = 1, inplace = True)\n",
    "fraud.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The V features are principal components which we'll learn about next Thursday, in the meantime think of them as hidden features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see how imbalanced the classes are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64315\n",
       "1      492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Value counts without normalize\n",
    "fraud.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.992408\n",
       "1    0.007592\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Value counts with normalize\n",
    "fraud.Class.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty imbalanced would you say?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0     89.589643\n",
       "1    122.211321\n",
       "Name: Amount, dtype: float64"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick EDA to find relationship amount and fraud status\n",
    "\n",
    "fraud.groupby(\"Class\").mean()[\"Amount\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this say about the relationship between amount of the transaction and fraudelent status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into modeling, what metric should we try to minimize False Negatives or False Positives? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some modeling.\n",
    "\n",
    "Train a logistic regression model and evaluate it on the testing dataset using accuracy, recall, and precision scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign variables\n",
    "\n",
    "X = fraud.drop(\"Class\", axis = 1)\n",
    "\n",
    "y = fraud.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit logistic regression model on training data\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.992979\n",
       "1    0.007021\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null accuracy of testing data\n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981869382401728"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate it on testing set using accuracy score\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9411764705882353"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate it on testing set using precision score\n",
    "\n",
    "precision_score(y_test, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7912087912087912"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate it on testing set using recall score\n",
    "\n",
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25732,     9],\n",
       "       [   38,   144]])"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do these metrics tell us about our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9978551609155503"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(LogisticRegression(), X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9456811815635346"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using precision score\n",
    "\n",
    "cross_val_score(LogisticRegression(), X, y, cv=5, scoring='precision').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7639868068439497"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using precision score\n",
    "\n",
    "cross_val_score(LogisticRegression(), X, y, cv=5, scoring='recall').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747032268272591"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross validate using roc_auc score\n",
    "\n",
    "cross_val_score(LogisticRegression(), X, y, cv=5, scoring='roc_auc').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00062562, 0.00302408, 0.00070984, ..., 0.0009586 , 0.00113733,\n",
       "       0.00120691])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive probabilities of class 1 from the test set\n",
    "test_probabs = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "#Pass in the test_probs variable and the true test labels aka y_test in the roc_curve function\n",
    "fpr, tpr, thres = roc_curve(y_test, test_probabs)\n",
    "\n",
    "#Outputs the fpr, tpr, for varying thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a1c3fc780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot ROC_curve again but this time annotate the curve with the threshold value\n",
    "plt.figure(figsize=(15,11))\n",
    "plt.plot(fpr, tpr, linewidth=12, alpha = .6)\n",
    "plt.plot([0,1], [0,1], \"--\", alpha=.6)\n",
    "for label, x, y in zip(thres[::10], fpr[::10], tpr[::10]):\n",
    "    plt.annotate(\"{0:.2f}\".format(label), xy=(x, y ), size = 15)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret this chart?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced class techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eop](https://chrisalbon.com/images/machine_learning_flashcards/Downsampling_print.png)\n",
    "Source: Chris Albon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead apply down sampling to our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    64315\n",
       "1      492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many true class are there in the training dataset?\n",
    "\n",
    "fraud.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud.Class.value_counts().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign number of fraud class in training data to variable\n",
    "N = fraud.Class.value_counts()[1]\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import resample function from sklearn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "fraud_maj = fraud[fraud.Class==0]\n",
    "fraud_min = fraud[fraud.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_majority_downsampled = resample(fraud_maj, \n",
    "                                 replace=False,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "len(fraud_majority_downsampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "984"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.concat([fraud_majority_downsampled, fraud_min]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0    492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine majority class with upsampled minority class\n",
    "fraud_ds = pd.concat([fraud_majority_downsampled, fraud_min])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_ds.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfectly balanced classes. Let's use cross validate to see how well our model does.\n",
    "\n",
    "Use accuracy, recall, precision, and roc_auc metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9278086992372707"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ds = fraud_ds.drop(\"Class\", axis = 1)\n",
    "y_ds = fraud_ds.Class\n",
    "\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9585630164258212"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8942692228406515"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9588887711778427"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision\n",
    "cross_val_score(LogisticRegression(), X_ds, y_ds, cv = 5, scoring=\"roc_auc\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about our model and our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aw](https://chrisalbon.com/images/machine_learning_flashcards/Upsampling_print.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    64315\n",
       "0    64315\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Number non fraud observations\n",
    "N = fraud.Class.value_counts()[0]\n",
    "\n",
    "# Downsample majority class\n",
    "fraud_minority_upsampled = resample(fraud_min, \n",
    "                                 replace=True,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_us = pd.concat([fraud_minority_upsampled, fraud_maj])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_us.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9464821581279639"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign X and y\n",
    "X_us = fraud_us.drop(\"Class\", axis = 1)\n",
    "y_us = fraud_us.Class\n",
    "\n",
    "#Accuracy score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701457901577344"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Precision score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"precision\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9214180206794683"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Recall\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"recall\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853185069750758"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Roc auc score\n",
    "cross_val_score(LogisticRegression(), X_us, y_us, cv = 5, scoring=\"roc_auc\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both techniques, our interpretation of the accuracy score is more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wee](https://svds.com/wp-content/uploads/2016/08/ImbalancedClasses_fig5.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is an issue here and that is can a model trained on balanced data work well with imbalanced data? Let's find out!\n",
    "\n",
    "\n",
    "We're going to train a logistic regression model on a downsampled training dataset and then apply it to an imbalanced testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reassign variables\n",
    "\n",
    "X = fraud.drop(\"Class\", axis = 1)\n",
    "\n",
    "y = fraud.Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.4, \n",
    "                                                    random_state = 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199907</th>\n",
       "      <td>-2.898534</td>\n",
       "      <td>2.335865</td>\n",
       "      <td>-2.580650</td>\n",
       "      <td>-1.635801</td>\n",
       "      <td>1.004089</td>\n",
       "      <td>0.241421</td>\n",
       "      <td>-1.947226</td>\n",
       "      <td>-11.701934</td>\n",
       "      <td>-0.582457</td>\n",
       "      <td>1.987813</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469586</td>\n",
       "      <td>1.863208</td>\n",
       "      <td>0.496664</td>\n",
       "      <td>0.496874</td>\n",
       "      <td>1.174098</td>\n",
       "      <td>-1.652801</td>\n",
       "      <td>0.379717</td>\n",
       "      <td>-1.024649</td>\n",
       "      <td>-0.939494</td>\n",
       "      <td>64.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108425</th>\n",
       "      <td>0.465652</td>\n",
       "      <td>-2.211063</td>\n",
       "      <td>0.825680</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>-2.161795</td>\n",
       "      <td>0.044469</td>\n",
       "      <td>-0.651862</td>\n",
       "      <td>0.106444</td>\n",
       "      <td>-0.153201</td>\n",
       "      <td>0.413537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.316451</td>\n",
       "      <td>-0.287784</td>\n",
       "      <td>-1.102149</td>\n",
       "      <td>-0.179108</td>\n",
       "      <td>0.580409</td>\n",
       "      <td>-0.197611</td>\n",
       "      <td>0.839903</td>\n",
       "      <td>-0.090394</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>419.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265756</th>\n",
       "      <td>0.127149</td>\n",
       "      <td>0.813936</td>\n",
       "      <td>-0.542322</td>\n",
       "      <td>-1.174614</td>\n",
       "      <td>1.705919</td>\n",
       "      <td>-1.290807</td>\n",
       "      <td>2.004082</td>\n",
       "      <td>-0.921752</td>\n",
       "      <td>-0.128364</td>\n",
       "      <td>-0.129620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091926</td>\n",
       "      <td>0.192409</td>\n",
       "      <td>0.910717</td>\n",
       "      <td>-0.317445</td>\n",
       "      <td>0.785640</td>\n",
       "      <td>-0.424653</td>\n",
       "      <td>-0.031038</td>\n",
       "      <td>-0.304005</td>\n",
       "      <td>-0.241409</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240309</th>\n",
       "      <td>1.997032</td>\n",
       "      <td>0.471299</td>\n",
       "      <td>-2.288758</td>\n",
       "      <td>1.373118</td>\n",
       "      <td>1.010453</td>\n",
       "      <td>-0.673412</td>\n",
       "      <td>0.530564</td>\n",
       "      <td>-0.162376</td>\n",
       "      <td>-0.069638</td>\n",
       "      <td>-0.176525</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258244</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.182688</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>0.568676</td>\n",
       "      <td>0.433247</td>\n",
       "      <td>-0.539873</td>\n",
       "      <td>-0.006559</td>\n",
       "      <td>-0.034073</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230792</th>\n",
       "      <td>-2.396032</td>\n",
       "      <td>-0.553487</td>\n",
       "      <td>1.312400</td>\n",
       "      <td>-1.132354</td>\n",
       "      <td>0.245760</td>\n",
       "      <td>-0.129237</td>\n",
       "      <td>0.889585</td>\n",
       "      <td>-0.015832</td>\n",
       "      <td>0.677545</td>\n",
       "      <td>-0.746701</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.033433</td>\n",
       "      <td>-0.613249</td>\n",
       "      <td>-1.087025</td>\n",
       "      <td>0.617427</td>\n",
       "      <td>-0.476856</td>\n",
       "      <td>0.532635</td>\n",
       "      <td>-0.353114</td>\n",
       "      <td>-0.283374</td>\n",
       "      <td>0.100989</td>\n",
       "      <td>103.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32450</th>\n",
       "      <td>-0.827556</td>\n",
       "      <td>-0.083257</td>\n",
       "      <td>2.062609</td>\n",
       "      <td>-1.916331</td>\n",
       "      <td>-0.998365</td>\n",
       "      <td>0.063317</td>\n",
       "      <td>-0.434585</td>\n",
       "      <td>0.418814</td>\n",
       "      <td>-0.720426</td>\n",
       "      <td>0.376913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>0.481265</td>\n",
       "      <td>1.239019</td>\n",
       "      <td>-0.212110</td>\n",
       "      <td>-0.016285</td>\n",
       "      <td>-0.050586</td>\n",
       "      <td>-0.174210</td>\n",
       "      <td>0.220622</td>\n",
       "      <td>0.151746</td>\n",
       "      <td>39.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207728</th>\n",
       "      <td>2.134170</td>\n",
       "      <td>0.038352</td>\n",
       "      <td>-2.338952</td>\n",
       "      <td>-0.076475</td>\n",
       "      <td>0.711083</td>\n",
       "      <td>-1.310592</td>\n",
       "      <td>0.725398</td>\n",
       "      <td>-0.478574</td>\n",
       "      <td>0.308230</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.228567</td>\n",
       "      <td>0.127905</td>\n",
       "      <td>0.421162</td>\n",
       "      <td>-0.033422</td>\n",
       "      <td>0.616146</td>\n",
       "      <td>0.457346</td>\n",
       "      <td>0.217986</td>\n",
       "      <td>-0.097946</td>\n",
       "      <td>-0.073277</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236892</th>\n",
       "      <td>1.933270</td>\n",
       "      <td>-1.350878</td>\n",
       "      <td>0.652871</td>\n",
       "      <td>1.011541</td>\n",
       "      <td>-1.594781</td>\n",
       "      <td>1.221503</td>\n",
       "      <td>-1.850292</td>\n",
       "      <td>0.425636</td>\n",
       "      <td>1.405681</td>\n",
       "      <td>0.604810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600846</td>\n",
       "      <td>-0.269129</td>\n",
       "      <td>0.228633</td>\n",
       "      <td>0.088670</td>\n",
       "      <td>-0.789299</td>\n",
       "      <td>-0.163106</td>\n",
       "      <td>-0.425544</td>\n",
       "      <td>0.157662</td>\n",
       "      <td>-0.015464</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244943</th>\n",
       "      <td>-1.173946</td>\n",
       "      <td>0.609209</td>\n",
       "      <td>0.741983</td>\n",
       "      <td>-1.809244</td>\n",
       "      <td>0.180076</td>\n",
       "      <td>-0.010626</td>\n",
       "      <td>0.312183</td>\n",
       "      <td>0.399817</td>\n",
       "      <td>-1.805899</td>\n",
       "      <td>-0.311466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.596603</td>\n",
       "      <td>-0.114611</td>\n",
       "      <td>0.072045</td>\n",
       "      <td>-0.516300</td>\n",
       "      <td>0.773349</td>\n",
       "      <td>0.970481</td>\n",
       "      <td>0.701896</td>\n",
       "      <td>-0.285448</td>\n",
       "      <td>-0.050652</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267824</th>\n",
       "      <td>1.844686</td>\n",
       "      <td>-0.619484</td>\n",
       "      <td>-0.394620</td>\n",
       "      <td>0.343100</td>\n",
       "      <td>-0.214237</td>\n",
       "      <td>0.969722</td>\n",
       "      <td>-0.901900</td>\n",
       "      <td>0.434638</td>\n",
       "      <td>1.577066</td>\n",
       "      <td>-0.286928</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.366190</td>\n",
       "      <td>0.265499</td>\n",
       "      <td>1.103009</td>\n",
       "      <td>0.205172</td>\n",
       "      <td>-1.114025</td>\n",
       "      <td>-0.337757</td>\n",
       "      <td>-0.107353</td>\n",
       "      <td>0.090250</td>\n",
       "      <td>-0.050351</td>\n",
       "      <td>9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268131</th>\n",
       "      <td>-0.114616</td>\n",
       "      <td>0.251710</td>\n",
       "      <td>1.536465</td>\n",
       "      <td>1.066808</td>\n",
       "      <td>-0.350781</td>\n",
       "      <td>0.473709</td>\n",
       "      <td>-0.615864</td>\n",
       "      <td>0.260233</td>\n",
       "      <td>0.662910</td>\n",
       "      <td>0.024137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103496</td>\n",
       "      <td>0.173948</td>\n",
       "      <td>0.673010</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>0.674069</td>\n",
       "      <td>-1.146933</td>\n",
       "      <td>-0.080433</td>\n",
       "      <td>0.220144</td>\n",
       "      <td>0.215169</td>\n",
       "      <td>15.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193708</th>\n",
       "      <td>-2.156828</td>\n",
       "      <td>2.678184</td>\n",
       "      <td>-2.279124</td>\n",
       "      <td>-1.087435</td>\n",
       "      <td>0.367839</td>\n",
       "      <td>-0.178373</td>\n",
       "      <td>-0.006520</td>\n",
       "      <td>1.181235</td>\n",
       "      <td>0.395433</td>\n",
       "      <td>0.686286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.611705</td>\n",
       "      <td>-0.434500</td>\n",
       "      <td>-0.915794</td>\n",
       "      <td>0.142096</td>\n",
       "      <td>-0.310162</td>\n",
       "      <td>0.018270</td>\n",
       "      <td>0.150617</td>\n",
       "      <td>0.321682</td>\n",
       "      <td>0.023475</td>\n",
       "      <td>10.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169862</th>\n",
       "      <td>2.017953</td>\n",
       "      <td>0.058252</td>\n",
       "      <td>-1.619452</td>\n",
       "      <td>0.353067</td>\n",
       "      <td>0.315459</td>\n",
       "      <td>-0.681202</td>\n",
       "      <td>0.036451</td>\n",
       "      <td>-0.059286</td>\n",
       "      <td>0.334024</td>\n",
       "      <td>-0.187849</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198141</td>\n",
       "      <td>-0.309022</td>\n",
       "      <td>-0.858695</td>\n",
       "      <td>0.359254</td>\n",
       "      <td>0.649453</td>\n",
       "      <td>-0.338457</td>\n",
       "      <td>0.145044</td>\n",
       "      <td>-0.071171</td>\n",
       "      <td>-0.040289</td>\n",
       "      <td>1.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260085</th>\n",
       "      <td>-0.466681</td>\n",
       "      <td>1.470471</td>\n",
       "      <td>-0.909695</td>\n",
       "      <td>-0.251940</td>\n",
       "      <td>1.616060</td>\n",
       "      <td>-1.919969</td>\n",
       "      <td>1.482073</td>\n",
       "      <td>-0.320694</td>\n",
       "      <td>-0.312395</td>\n",
       "      <td>-2.290189</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076373</td>\n",
       "      <td>-0.009784</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>-0.454421</td>\n",
       "      <td>-0.321732</td>\n",
       "      <td>0.438356</td>\n",
       "      <td>-0.511653</td>\n",
       "      <td>0.140682</td>\n",
       "      <td>0.199139</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8670</th>\n",
       "      <td>0.759117</td>\n",
       "      <td>-0.860906</td>\n",
       "      <td>0.407423</td>\n",
       "      <td>0.343693</td>\n",
       "      <td>-0.512834</td>\n",
       "      <td>0.610339</td>\n",
       "      <td>-0.332852</td>\n",
       "      <td>0.196187</td>\n",
       "      <td>1.863866</td>\n",
       "      <td>-0.725106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172922</td>\n",
       "      <td>-0.047264</td>\n",
       "      <td>-0.076314</td>\n",
       "      <td>-0.162885</td>\n",
       "      <td>-0.240179</td>\n",
       "      <td>0.151794</td>\n",
       "      <td>1.095899</td>\n",
       "      <td>-0.104776</td>\n",
       "      <td>0.009129</td>\n",
       "      <td>200.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189698</th>\n",
       "      <td>1.894893</td>\n",
       "      <td>0.371393</td>\n",
       "      <td>-0.064436</td>\n",
       "      <td>3.607553</td>\n",
       "      <td>0.201358</td>\n",
       "      <td>0.689180</td>\n",
       "      <td>-0.427763</td>\n",
       "      <td>0.120717</td>\n",
       "      <td>-0.600859</td>\n",
       "      <td>1.400583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.156698</td>\n",
       "      <td>0.184745</td>\n",
       "      <td>0.555916</td>\n",
       "      <td>0.187321</td>\n",
       "      <td>0.463481</td>\n",
       "      <td>-0.193536</td>\n",
       "      <td>-0.023847</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>-0.022976</td>\n",
       "      <td>13.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160534</th>\n",
       "      <td>1.979639</td>\n",
       "      <td>-1.029783</td>\n",
       "      <td>-1.444450</td>\n",
       "      <td>-0.694480</td>\n",
       "      <td>-0.671629</td>\n",
       "      <td>-1.360079</td>\n",
       "      <td>-0.017294</td>\n",
       "      <td>-0.465088</td>\n",
       "      <td>-0.517971</td>\n",
       "      <td>0.731342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208411</td>\n",
       "      <td>0.104271</td>\n",
       "      <td>0.070109</td>\n",
       "      <td>0.082159</td>\n",
       "      <td>0.067652</td>\n",
       "      <td>-0.053678</td>\n",
       "      <td>-0.295420</td>\n",
       "      <td>-0.062363</td>\n",
       "      <td>-0.041079</td>\n",
       "      <td>136.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126091</th>\n",
       "      <td>-2.597581</td>\n",
       "      <td>-0.767684</td>\n",
       "      <td>1.272246</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>1.791811</td>\n",
       "      <td>-0.642956</td>\n",
       "      <td>-0.466455</td>\n",
       "      <td>0.810727</td>\n",
       "      <td>-1.083739</td>\n",
       "      <td>-0.859014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303990</td>\n",
       "      <td>0.408754</td>\n",
       "      <td>0.346980</td>\n",
       "      <td>-0.326109</td>\n",
       "      <td>-0.258752</td>\n",
       "      <td>0.533110</td>\n",
       "      <td>-0.390259</td>\n",
       "      <td>-0.029540</td>\n",
       "      <td>-0.320822</td>\n",
       "      <td>9.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34355</th>\n",
       "      <td>-1.414166</td>\n",
       "      <td>-0.426362</td>\n",
       "      <td>0.295761</td>\n",
       "      <td>0.193104</td>\n",
       "      <td>2.475615</td>\n",
       "      <td>-2.339276</td>\n",
       "      <td>0.111606</td>\n",
       "      <td>-0.394224</td>\n",
       "      <td>-0.103002</td>\n",
       "      <td>-0.722651</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393367</td>\n",
       "      <td>-0.030060</td>\n",
       "      <td>0.260830</td>\n",
       "      <td>-0.204197</td>\n",
       "      <td>0.329769</td>\n",
       "      <td>-1.102415</td>\n",
       "      <td>-0.024056</td>\n",
       "      <td>0.131416</td>\n",
       "      <td>-0.047611</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171997</th>\n",
       "      <td>2.166922</td>\n",
       "      <td>-0.753291</td>\n",
       "      <td>-0.887595</td>\n",
       "      <td>-1.106112</td>\n",
       "      <td>-0.636526</td>\n",
       "      <td>-0.684831</td>\n",
       "      <td>-0.650198</td>\n",
       "      <td>-0.096660</td>\n",
       "      <td>-0.626875</td>\n",
       "      <td>0.958453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027292</td>\n",
       "      <td>-0.203135</td>\n",
       "      <td>-0.722754</td>\n",
       "      <td>0.450314</td>\n",
       "      <td>-0.464699</td>\n",
       "      <td>-0.578893</td>\n",
       "      <td>-0.690986</td>\n",
       "      <td>-0.017831</td>\n",
       "      <td>-0.058220</td>\n",
       "      <td>9.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57661</th>\n",
       "      <td>-0.966652</td>\n",
       "      <td>1.478130</td>\n",
       "      <td>0.771943</td>\n",
       "      <td>0.333734</td>\n",
       "      <td>0.621895</td>\n",
       "      <td>0.681178</td>\n",
       "      <td>0.432067</td>\n",
       "      <td>-0.883951</td>\n",
       "      <td>-0.588684</td>\n",
       "      <td>0.376683</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147157</td>\n",
       "      <td>0.851272</td>\n",
       "      <td>-0.120858</td>\n",
       "      <td>-0.176219</td>\n",
       "      <td>-1.035003</td>\n",
       "      <td>-0.085018</td>\n",
       "      <td>-0.395726</td>\n",
       "      <td>-0.337084</td>\n",
       "      <td>0.144295</td>\n",
       "      <td>25.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135738</th>\n",
       "      <td>-0.968716</td>\n",
       "      <td>0.781718</td>\n",
       "      <td>1.227021</td>\n",
       "      <td>0.267244</td>\n",
       "      <td>0.945096</td>\n",
       "      <td>0.043601</td>\n",
       "      <td>0.315519</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>-0.849959</td>\n",
       "      <td>-0.145913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198595</td>\n",
       "      <td>-0.213616</td>\n",
       "      <td>-0.788073</td>\n",
       "      <td>-0.252262</td>\n",
       "      <td>-0.863587</td>\n",
       "      <td>0.129750</td>\n",
       "      <td>0.302896</td>\n",
       "      <td>-0.296841</td>\n",
       "      <td>0.218349</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13767</th>\n",
       "      <td>0.991567</td>\n",
       "      <td>-0.209039</td>\n",
       "      <td>0.765694</td>\n",
       "      <td>0.377316</td>\n",
       "      <td>0.006870</td>\n",
       "      <td>1.156989</td>\n",
       "      <td>-0.509382</td>\n",
       "      <td>0.331841</td>\n",
       "      <td>1.755394</td>\n",
       "      <td>-0.891513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.172627</td>\n",
       "      <td>-0.308394</td>\n",
       "      <td>-0.358728</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>-0.610914</td>\n",
       "      <td>-0.168959</td>\n",
       "      <td>0.886243</td>\n",
       "      <td>-0.019708</td>\n",
       "      <td>-0.004244</td>\n",
       "      <td>34.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137127</th>\n",
       "      <td>-2.240391</td>\n",
       "      <td>1.562074</td>\n",
       "      <td>0.867433</td>\n",
       "      <td>-1.680612</td>\n",
       "      <td>-0.558472</td>\n",
       "      <td>-0.438408</td>\n",
       "      <td>0.095502</td>\n",
       "      <td>0.576837</td>\n",
       "      <td>1.062155</td>\n",
       "      <td>1.103078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401158</td>\n",
       "      <td>-0.191431</td>\n",
       "      <td>-0.229241</td>\n",
       "      <td>0.084992</td>\n",
       "      <td>-0.028267</td>\n",
       "      <td>-0.263373</td>\n",
       "      <td>0.726299</td>\n",
       "      <td>0.374340</td>\n",
       "      <td>0.182592</td>\n",
       "      <td>7.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210631</th>\n",
       "      <td>0.106760</td>\n",
       "      <td>0.949853</td>\n",
       "      <td>-0.535160</td>\n",
       "      <td>-0.745178</td>\n",
       "      <td>1.193639</td>\n",
       "      <td>-0.232937</td>\n",
       "      <td>0.840860</td>\n",
       "      <td>0.085040</td>\n",
       "      <td>-0.286439</td>\n",
       "      <td>-0.672787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>-0.305864</td>\n",
       "      <td>-0.764512</td>\n",
       "      <td>0.049295</td>\n",
       "      <td>0.117717</td>\n",
       "      <td>-0.416061</td>\n",
       "      <td>0.122208</td>\n",
       "      <td>0.220586</td>\n",
       "      <td>0.070412</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149812</th>\n",
       "      <td>0.042436</td>\n",
       "      <td>0.892376</td>\n",
       "      <td>0.410827</td>\n",
       "      <td>-0.379318</td>\n",
       "      <td>0.405166</td>\n",
       "      <td>-1.020157</td>\n",
       "      <td>0.825473</td>\n",
       "      <td>-0.195507</td>\n",
       "      <td>1.417329</td>\n",
       "      <td>-0.755035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.166944</td>\n",
       "      <td>-0.419621</td>\n",
       "      <td>-0.830058</td>\n",
       "      <td>0.099268</td>\n",
       "      <td>-0.113272</td>\n",
       "      <td>-0.567867</td>\n",
       "      <td>0.108790</td>\n",
       "      <td>0.216404</td>\n",
       "      <td>0.087752</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88972</th>\n",
       "      <td>1.232193</td>\n",
       "      <td>-0.229653</td>\n",
       "      <td>-0.332639</td>\n",
       "      <td>0.085176</td>\n",
       "      <td>1.573836</td>\n",
       "      <td>3.959564</td>\n",
       "      <td>-1.130371</td>\n",
       "      <td>1.067693</td>\n",
       "      <td>0.628682</td>\n",
       "      <td>-0.085159</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017532</td>\n",
       "      <td>-0.063580</td>\n",
       "      <td>-0.169507</td>\n",
       "      <td>-0.034198</td>\n",
       "      <td>0.997747</td>\n",
       "      <td>0.552384</td>\n",
       "      <td>-0.384011</td>\n",
       "      <td>0.072874</td>\n",
       "      <td>0.031000</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182871</th>\n",
       "      <td>1.929301</td>\n",
       "      <td>-0.223788</td>\n",
       "      <td>-3.120130</td>\n",
       "      <td>0.292451</td>\n",
       "      <td>2.889450</td>\n",
       "      <td>3.318414</td>\n",
       "      <td>0.070513</td>\n",
       "      <td>0.660561</td>\n",
       "      <td>0.039372</td>\n",
       "      <td>0.259345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115701</td>\n",
       "      <td>0.071827</td>\n",
       "      <td>0.199280</td>\n",
       "      <td>-0.012749</td>\n",
       "      <td>0.709901</td>\n",
       "      <td>0.492767</td>\n",
       "      <td>-0.477041</td>\n",
       "      <td>-0.007833</td>\n",
       "      <td>-0.059462</td>\n",
       "      <td>57.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178562</th>\n",
       "      <td>1.951046</td>\n",
       "      <td>-0.393448</td>\n",
       "      <td>-0.628717</td>\n",
       "      <td>0.241213</td>\n",
       "      <td>-0.578649</td>\n",
       "      <td>-0.208219</td>\n",
       "      <td>-0.899786</td>\n",
       "      <td>0.177414</td>\n",
       "      <td>1.691489</td>\n",
       "      <td>-0.718515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162766</td>\n",
       "      <td>-0.021552</td>\n",
       "      <td>0.042071</td>\n",
       "      <td>0.292816</td>\n",
       "      <td>0.480529</td>\n",
       "      <td>-0.473001</td>\n",
       "      <td>-0.097127</td>\n",
       "      <td>0.037072</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>19.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208441</th>\n",
       "      <td>-2.511998</td>\n",
       "      <td>-2.781748</td>\n",
       "      <td>1.408172</td>\n",
       "      <td>-3.048312</td>\n",
       "      <td>2.863623</td>\n",
       "      <td>-0.482142</td>\n",
       "      <td>-1.593723</td>\n",
       "      <td>0.136738</td>\n",
       "      <td>-1.832276</td>\n",
       "      <td>0.744206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.800161</td>\n",
       "      <td>-0.254756</td>\n",
       "      <td>0.077137</td>\n",
       "      <td>-0.701778</td>\n",
       "      <td>-0.286356</td>\n",
       "      <td>0.508312</td>\n",
       "      <td>-0.168597</td>\n",
       "      <td>0.095949</td>\n",
       "      <td>-0.309488</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152574</th>\n",
       "      <td>1.973531</td>\n",
       "      <td>-0.474499</td>\n",
       "      <td>0.197153</td>\n",
       "      <td>0.549060</td>\n",
       "      <td>-1.020910</td>\n",
       "      <td>-0.398638</td>\n",
       "      <td>-1.011277</td>\n",
       "      <td>-0.017140</td>\n",
       "      <td>2.768881</td>\n",
       "      <td>-0.373530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240604</td>\n",
       "      <td>-0.129143</td>\n",
       "      <td>-0.008698</td>\n",
       "      <td>0.314886</td>\n",
       "      <td>-0.159973</td>\n",
       "      <td>-0.624526</td>\n",
       "      <td>0.451226</td>\n",
       "      <td>-0.044144</td>\n",
       "      <td>-0.045731</td>\n",
       "      <td>15.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241318</th>\n",
       "      <td>-0.074110</td>\n",
       "      <td>-0.347253</td>\n",
       "      <td>1.745622</td>\n",
       "      <td>0.381817</td>\n",
       "      <td>0.094591</td>\n",
       "      <td>3.256371</td>\n",
       "      <td>-1.262272</td>\n",
       "      <td>1.318783</td>\n",
       "      <td>1.921173</td>\n",
       "      <td>-0.724335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460282</td>\n",
       "      <td>0.231311</td>\n",
       "      <td>1.107849</td>\n",
       "      <td>-0.082169</td>\n",
       "      <td>-0.996882</td>\n",
       "      <td>-0.568866</td>\n",
       "      <td>-0.456633</td>\n",
       "      <td>0.201699</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50927</th>\n",
       "      <td>-4.678386</td>\n",
       "      <td>3.451893</td>\n",
       "      <td>-1.946664</td>\n",
       "      <td>0.277297</td>\n",
       "      <td>-3.050028</td>\n",
       "      <td>-0.865884</td>\n",
       "      <td>-2.385780</td>\n",
       "      <td>3.357111</td>\n",
       "      <td>-0.523805</td>\n",
       "      <td>-0.457277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541724</td>\n",
       "      <td>0.198531</td>\n",
       "      <td>-0.418373</td>\n",
       "      <td>0.241571</td>\n",
       "      <td>0.536152</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.160962</td>\n",
       "      <td>-1.251586</td>\n",
       "      <td>-0.306001</td>\n",
       "      <td>12.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187342</th>\n",
       "      <td>2.041429</td>\n",
       "      <td>-1.886582</td>\n",
       "      <td>-1.072220</td>\n",
       "      <td>-1.705269</td>\n",
       "      <td>-1.137088</td>\n",
       "      <td>0.256496</td>\n",
       "      <td>-1.284295</td>\n",
       "      <td>0.065178</td>\n",
       "      <td>-1.243489</td>\n",
       "      <td>1.660271</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157120</td>\n",
       "      <td>-0.057725</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.120146</td>\n",
       "      <td>0.112397</td>\n",
       "      <td>-0.290600</td>\n",
       "      <td>-0.203048</td>\n",
       "      <td>-0.004073</td>\n",
       "      <td>-0.033163</td>\n",
       "      <td>136.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128365</th>\n",
       "      <td>1.231586</td>\n",
       "      <td>-0.413043</td>\n",
       "      <td>1.098834</td>\n",
       "      <td>-0.843303</td>\n",
       "      <td>-1.407439</td>\n",
       "      <td>-0.837886</td>\n",
       "      <td>-0.786304</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>1.810669</td>\n",
       "      <td>-1.003697</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094942</td>\n",
       "      <td>0.267405</td>\n",
       "      <td>0.923146</td>\n",
       "      <td>-0.115746</td>\n",
       "      <td>0.404664</td>\n",
       "      <td>0.482778</td>\n",
       "      <td>-0.550712</td>\n",
       "      <td>0.102953</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265684</th>\n",
       "      <td>-0.484560</td>\n",
       "      <td>0.659441</td>\n",
       "      <td>1.732009</td>\n",
       "      <td>-0.660167</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>-0.396754</td>\n",
       "      <td>0.581202</td>\n",
       "      <td>-0.246530</td>\n",
       "      <td>0.514145</td>\n",
       "      <td>-0.325413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159222</td>\n",
       "      <td>0.174096</td>\n",
       "      <td>0.947562</td>\n",
       "      <td>-0.266288</td>\n",
       "      <td>0.134397</td>\n",
       "      <td>-0.329606</td>\n",
       "      <td>0.504733</td>\n",
       "      <td>0.078024</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167547</th>\n",
       "      <td>-2.210887</td>\n",
       "      <td>-1.344151</td>\n",
       "      <td>-0.064065</td>\n",
       "      <td>0.618148</td>\n",
       "      <td>3.610308</td>\n",
       "      <td>-1.203713</td>\n",
       "      <td>-0.615840</td>\n",
       "      <td>0.371216</td>\n",
       "      <td>-0.271876</td>\n",
       "      <td>-1.329248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796152</td>\n",
       "      <td>0.043285</td>\n",
       "      <td>-0.546567</td>\n",
       "      <td>-0.052014</td>\n",
       "      <td>-0.309504</td>\n",
       "      <td>0.993172</td>\n",
       "      <td>0.013963</td>\n",
       "      <td>-0.037262</td>\n",
       "      <td>0.139016</td>\n",
       "      <td>43.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268760</th>\n",
       "      <td>2.084722</td>\n",
       "      <td>-0.076536</td>\n",
       "      <td>-1.456916</td>\n",
       "      <td>0.055482</td>\n",
       "      <td>0.485344</td>\n",
       "      <td>-0.283433</td>\n",
       "      <td>0.124140</td>\n",
       "      <td>-0.173196</td>\n",
       "      <td>0.218975</td>\n",
       "      <td>0.216269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116798</td>\n",
       "      <td>-0.292322</td>\n",
       "      <td>-0.704764</td>\n",
       "      <td>0.188002</td>\n",
       "      <td>-1.012921</td>\n",
       "      <td>-0.164213</td>\n",
       "      <td>0.238378</td>\n",
       "      <td>-0.070871</td>\n",
       "      <td>-0.077936</td>\n",
       "      <td>5.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197784</th>\n",
       "      <td>2.071716</td>\n",
       "      <td>-1.176000</td>\n",
       "      <td>-1.196195</td>\n",
       "      <td>-1.092389</td>\n",
       "      <td>-0.495480</td>\n",
       "      <td>0.029847</td>\n",
       "      <td>-0.801790</td>\n",
       "      <td>-0.007139</td>\n",
       "      <td>-0.229354</td>\n",
       "      <td>0.885753</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134972</td>\n",
       "      <td>0.228813</td>\n",
       "      <td>0.550809</td>\n",
       "      <td>-0.024919</td>\n",
       "      <td>-1.131703</td>\n",
       "      <td>-0.067649</td>\n",
       "      <td>-0.133243</td>\n",
       "      <td>-0.019360</td>\n",
       "      <td>-0.061658</td>\n",
       "      <td>80.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155257</th>\n",
       "      <td>-0.490327</td>\n",
       "      <td>0.807327</td>\n",
       "      <td>2.399965</td>\n",
       "      <td>-0.103815</td>\n",
       "      <td>0.076465</td>\n",
       "      <td>-0.372893</td>\n",
       "      <td>0.505751</td>\n",
       "      <td>-0.253555</td>\n",
       "      <td>1.576216</td>\n",
       "      <td>-1.124264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033987</td>\n",
       "      <td>-0.313714</td>\n",
       "      <td>-0.465484</td>\n",
       "      <td>-0.299141</td>\n",
       "      <td>-0.068129</td>\n",
       "      <td>0.249814</td>\n",
       "      <td>-0.729107</td>\n",
       "      <td>-0.082298</td>\n",
       "      <td>-0.146533</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142660</th>\n",
       "      <td>-3.110861</td>\n",
       "      <td>0.982156</td>\n",
       "      <td>0.330847</td>\n",
       "      <td>0.829458</td>\n",
       "      <td>-0.242277</td>\n",
       "      <td>-0.856808</td>\n",
       "      <td>-1.567547</td>\n",
       "      <td>-1.927998</td>\n",
       "      <td>-1.061453</td>\n",
       "      <td>-1.129589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477053</td>\n",
       "      <td>-1.479701</td>\n",
       "      <td>-0.464347</td>\n",
       "      <td>-0.523528</td>\n",
       "      <td>0.408897</td>\n",
       "      <td>-0.091166</td>\n",
       "      <td>0.192121</td>\n",
       "      <td>-0.014562</td>\n",
       "      <td>-0.307888</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72505</th>\n",
       "      <td>1.096057</td>\n",
       "      <td>-0.522901</td>\n",
       "      <td>0.875029</td>\n",
       "      <td>0.185963</td>\n",
       "      <td>-0.921279</td>\n",
       "      <td>-0.049623</td>\n",
       "      <td>-0.469502</td>\n",
       "      <td>-0.010056</td>\n",
       "      <td>1.033144</td>\n",
       "      <td>-0.525924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180929</td>\n",
       "      <td>-0.204105</td>\n",
       "      <td>-0.437802</td>\n",
       "      <td>-0.066361</td>\n",
       "      <td>-0.000833</td>\n",
       "      <td>0.217877</td>\n",
       "      <td>0.960943</td>\n",
       "      <td>-0.040551</td>\n",
       "      <td>0.025671</td>\n",
       "      <td>86.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204990</th>\n",
       "      <td>0.022811</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.277257</td>\n",
       "      <td>1.047408</td>\n",
       "      <td>0.871873</td>\n",
       "      <td>-0.544502</td>\n",
       "      <td>0.316577</td>\n",
       "      <td>-0.139794</td>\n",
       "      <td>-0.447481</td>\n",
       "      <td>0.185510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.331010</td>\n",
       "      <td>-0.168732</td>\n",
       "      <td>-0.448164</td>\n",
       "      <td>0.106051</td>\n",
       "      <td>-0.633335</td>\n",
       "      <td>-0.995675</td>\n",
       "      <td>0.592678</td>\n",
       "      <td>0.179258</td>\n",
       "      <td>0.210541</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195383</th>\n",
       "      <td>0.469750</td>\n",
       "      <td>-1.237555</td>\n",
       "      <td>-1.767341</td>\n",
       "      <td>4.833490</td>\n",
       "      <td>-0.268715</td>\n",
       "      <td>-0.512760</td>\n",
       "      <td>1.140149</td>\n",
       "      <td>-0.341273</td>\n",
       "      <td>-1.046351</td>\n",
       "      <td>0.085662</td>\n",
       "      <td>...</td>\n",
       "      <td>1.277315</td>\n",
       "      <td>0.303905</td>\n",
       "      <td>-0.647075</td>\n",
       "      <td>-0.373014</td>\n",
       "      <td>0.260801</td>\n",
       "      <td>-0.496566</td>\n",
       "      <td>-0.245973</td>\n",
       "      <td>-0.117858</td>\n",
       "      <td>0.144774</td>\n",
       "      <td>723.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44696</th>\n",
       "      <td>0.937545</td>\n",
       "      <td>-0.543255</td>\n",
       "      <td>1.027463</td>\n",
       "      <td>0.512110</td>\n",
       "      <td>-0.831667</td>\n",
       "      <td>0.385503</td>\n",
       "      <td>-0.634467</td>\n",
       "      <td>0.131506</td>\n",
       "      <td>0.574428</td>\n",
       "      <td>-0.256355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268107</td>\n",
       "      <td>0.308424</td>\n",
       "      <td>0.748671</td>\n",
       "      <td>-0.192632</td>\n",
       "      <td>-0.330654</td>\n",
       "      <td>0.176630</td>\n",
       "      <td>0.619430</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.047957</td>\n",
       "      <td>137.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31864</th>\n",
       "      <td>1.234139</td>\n",
       "      <td>-0.943517</td>\n",
       "      <td>0.695706</td>\n",
       "      <td>-1.339927</td>\n",
       "      <td>-1.509932</td>\n",
       "      <td>-0.613180</td>\n",
       "      <td>-0.797182</td>\n",
       "      <td>-0.034444</td>\n",
       "      <td>0.574644</td>\n",
       "      <td>-0.304890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.487586</td>\n",
       "      <td>-0.156467</td>\n",
       "      <td>0.248959</td>\n",
       "      <td>-0.058356</td>\n",
       "      <td>0.419196</td>\n",
       "      <td>0.513941</td>\n",
       "      <td>-0.490566</td>\n",
       "      <td>0.114715</td>\n",
       "      <td>0.038206</td>\n",
       "      <td>41.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233909</th>\n",
       "      <td>-1.050381</td>\n",
       "      <td>0.601436</td>\n",
       "      <td>-0.480727</td>\n",
       "      <td>-2.287084</td>\n",
       "      <td>-0.156796</td>\n",
       "      <td>-1.011860</td>\n",
       "      <td>-0.067803</td>\n",
       "      <td>0.660398</td>\n",
       "      <td>-1.750226</td>\n",
       "      <td>-0.312153</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255996</td>\n",
       "      <td>0.262440</td>\n",
       "      <td>0.295577</td>\n",
       "      <td>-0.140619</td>\n",
       "      <td>-0.334669</td>\n",
       "      <td>-0.215177</td>\n",
       "      <td>-0.450262</td>\n",
       "      <td>-0.272015</td>\n",
       "      <td>-0.011373</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183101</th>\n",
       "      <td>0.116849</td>\n",
       "      <td>0.746846</td>\n",
       "      <td>-2.580209</td>\n",
       "      <td>-1.114107</td>\n",
       "      <td>2.711441</td>\n",
       "      <td>3.133278</td>\n",
       "      <td>0.076749</td>\n",
       "      <td>1.214324</td>\n",
       "      <td>-0.321833</td>\n",
       "      <td>-1.183319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075537</td>\n",
       "      <td>0.343801</td>\n",
       "      <td>0.795495</td>\n",
       "      <td>0.046289</td>\n",
       "      <td>0.615515</td>\n",
       "      <td>-0.217676</td>\n",
       "      <td>-0.130093</td>\n",
       "      <td>-0.063341</td>\n",
       "      <td>-0.048145</td>\n",
       "      <td>52.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88265</th>\n",
       "      <td>-1.206488</td>\n",
       "      <td>1.004657</td>\n",
       "      <td>0.152853</td>\n",
       "      <td>-1.962787</td>\n",
       "      <td>-0.656907</td>\n",
       "      <td>-0.706286</td>\n",
       "      <td>-0.466546</td>\n",
       "      <td>0.801169</td>\n",
       "      <td>-1.248699</td>\n",
       "      <td>0.062107</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095808</td>\n",
       "      <td>0.203655</td>\n",
       "      <td>0.396986</td>\n",
       "      <td>-0.160760</td>\n",
       "      <td>-0.571046</td>\n",
       "      <td>0.034573</td>\n",
       "      <td>-0.343876</td>\n",
       "      <td>0.228299</td>\n",
       "      <td>0.120429</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72476</th>\n",
       "      <td>0.683389</td>\n",
       "      <td>-0.940750</td>\n",
       "      <td>-0.032823</td>\n",
       "      <td>0.352668</td>\n",
       "      <td>-0.898172</td>\n",
       "      <td>-1.047873</td>\n",
       "      <td>0.440964</td>\n",
       "      <td>-0.293963</td>\n",
       "      <td>0.178112</td>\n",
       "      <td>-0.269365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533982</td>\n",
       "      <td>-0.045068</td>\n",
       "      <td>-0.877869</td>\n",
       "      <td>-0.160987</td>\n",
       "      <td>0.432490</td>\n",
       "      <td>0.041511</td>\n",
       "      <td>0.783159</td>\n",
       "      <td>-0.148393</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>319.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73886</th>\n",
       "      <td>-1.435844</td>\n",
       "      <td>0.372468</td>\n",
       "      <td>1.526811</td>\n",
       "      <td>-1.628174</td>\n",
       "      <td>-1.265586</td>\n",
       "      <td>0.131469</td>\n",
       "      <td>-0.385453</td>\n",
       "      <td>-2.730749</td>\n",
       "      <td>-1.735721</td>\n",
       "      <td>-0.798762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659205</td>\n",
       "      <td>-1.266709</td>\n",
       "      <td>0.107082</td>\n",
       "      <td>-0.530021</td>\n",
       "      <td>0.391454</td>\n",
       "      <td>-0.107222</td>\n",
       "      <td>-0.586771</td>\n",
       "      <td>0.156965</td>\n",
       "      <td>0.061044</td>\n",
       "      <td>264.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123547</th>\n",
       "      <td>-2.429452</td>\n",
       "      <td>2.028011</td>\n",
       "      <td>-0.981596</td>\n",
       "      <td>-2.806809</td>\n",
       "      <td>1.682207</td>\n",
       "      <td>3.026534</td>\n",
       "      <td>-0.385599</td>\n",
       "      <td>1.459261</td>\n",
       "      <td>1.016029</td>\n",
       "      <td>1.261373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955400</td>\n",
       "      <td>-0.298395</td>\n",
       "      <td>-0.659499</td>\n",
       "      <td>0.028475</td>\n",
       "      <td>1.006401</td>\n",
       "      <td>0.199777</td>\n",
       "      <td>0.767798</td>\n",
       "      <td>0.862653</td>\n",
       "      <td>0.538995</td>\n",
       "      <td>1.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190267</th>\n",
       "      <td>0.108692</td>\n",
       "      <td>0.883450</td>\n",
       "      <td>-0.525120</td>\n",
       "      <td>-0.720752</td>\n",
       "      <td>1.063351</td>\n",
       "      <td>-0.301055</td>\n",
       "      <td>0.760852</td>\n",
       "      <td>0.144138</td>\n",
       "      <td>-0.122277</td>\n",
       "      <td>-0.638261</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037977</td>\n",
       "      <td>-0.315987</td>\n",
       "      <td>-0.842099</td>\n",
       "      <td>0.064093</td>\n",
       "      <td>0.158193</td>\n",
       "      <td>-0.464388</td>\n",
       "      <td>0.121649</td>\n",
       "      <td>0.214521</td>\n",
       "      <td>0.068449</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27586</th>\n",
       "      <td>1.115923</td>\n",
       "      <td>-0.208065</td>\n",
       "      <td>1.272632</td>\n",
       "      <td>1.244066</td>\n",
       "      <td>-1.095039</td>\n",
       "      <td>0.070824</td>\n",
       "      <td>-0.797603</td>\n",
       "      <td>0.346743</td>\n",
       "      <td>0.923676</td>\n",
       "      <td>0.025468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.254451</td>\n",
       "      <td>-0.035142</td>\n",
       "      <td>-0.015426</td>\n",
       "      <td>0.008990</td>\n",
       "      <td>0.274308</td>\n",
       "      <td>0.321104</td>\n",
       "      <td>-0.419127</td>\n",
       "      <td>0.056912</td>\n",
       "      <td>0.023216</td>\n",
       "      <td>5.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42965</th>\n",
       "      <td>-2.588478</td>\n",
       "      <td>-2.640332</td>\n",
       "      <td>2.471179</td>\n",
       "      <td>-1.262654</td>\n",
       "      <td>1.707620</td>\n",
       "      <td>-0.594148</td>\n",
       "      <td>-1.034840</td>\n",
       "      <td>0.517822</td>\n",
       "      <td>1.547498</td>\n",
       "      <td>-1.560564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687919</td>\n",
       "      <td>0.187373</td>\n",
       "      <td>0.092052</td>\n",
       "      <td>0.386099</td>\n",
       "      <td>-0.327056</td>\n",
       "      <td>0.474424</td>\n",
       "      <td>-0.800287</td>\n",
       "      <td>-0.146961</td>\n",
       "      <td>-0.112966</td>\n",
       "      <td>117.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49424</th>\n",
       "      <td>-1.362896</td>\n",
       "      <td>0.784266</td>\n",
       "      <td>2.135114</td>\n",
       "      <td>0.901308</td>\n",
       "      <td>-1.010935</td>\n",
       "      <td>1.006875</td>\n",
       "      <td>0.089636</td>\n",
       "      <td>0.724738</td>\n",
       "      <td>-0.537778</td>\n",
       "      <td>-0.626742</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066826</td>\n",
       "      <td>0.173608</td>\n",
       "      <td>0.545437</td>\n",
       "      <td>0.029496</td>\n",
       "      <td>0.030706</td>\n",
       "      <td>0.201539</td>\n",
       "      <td>-0.257087</td>\n",
       "      <td>-0.042535</td>\n",
       "      <td>-0.033667</td>\n",
       "      <td>132.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116825</th>\n",
       "      <td>-0.444432</td>\n",
       "      <td>0.943937</td>\n",
       "      <td>1.329332</td>\n",
       "      <td>-0.188890</td>\n",
       "      <td>0.385943</td>\n",
       "      <td>-0.169757</td>\n",
       "      <td>0.581994</td>\n",
       "      <td>0.112878</td>\n",
       "      <td>-0.347264</td>\n",
       "      <td>-0.348392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082414</td>\n",
       "      <td>-0.231877</td>\n",
       "      <td>-0.581957</td>\n",
       "      <td>-0.048439</td>\n",
       "      <td>-0.447981</td>\n",
       "      <td>-0.202409</td>\n",
       "      <td>0.132211</td>\n",
       "      <td>0.279578</td>\n",
       "      <td>0.112970</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40971</th>\n",
       "      <td>-1.497122</td>\n",
       "      <td>1.152116</td>\n",
       "      <td>1.661257</td>\n",
       "      <td>1.564480</td>\n",
       "      <td>-0.985292</td>\n",
       "      <td>1.088036</td>\n",
       "      <td>-0.812311</td>\n",
       "      <td>1.394429</td>\n",
       "      <td>0.124257</td>\n",
       "      <td>-0.608783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134520</td>\n",
       "      <td>-0.233120</td>\n",
       "      <td>-0.425322</td>\n",
       "      <td>-0.016851</td>\n",
       "      <td>0.023437</td>\n",
       "      <td>-0.065413</td>\n",
       "      <td>-0.394613</td>\n",
       "      <td>0.078557</td>\n",
       "      <td>0.015980</td>\n",
       "      <td>18.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273455</th>\n",
       "      <td>-0.510381</td>\n",
       "      <td>0.946413</td>\n",
       "      <td>1.925877</td>\n",
       "      <td>-0.538333</td>\n",
       "      <td>0.386981</td>\n",
       "      <td>0.026039</td>\n",
       "      <td>0.710646</td>\n",
       "      <td>-0.164836</td>\n",
       "      <td>0.246040</td>\n",
       "      <td>-0.599644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.312857</td>\n",
       "      <td>-0.247204</td>\n",
       "      <td>-0.310712</td>\n",
       "      <td>-0.420937</td>\n",
       "      <td>-0.672705</td>\n",
       "      <td>0.474619</td>\n",
       "      <td>-0.634045</td>\n",
       "      <td>0.198508</td>\n",
       "      <td>-0.083822</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182867</th>\n",
       "      <td>-0.710812</td>\n",
       "      <td>0.798818</td>\n",
       "      <td>-1.195173</td>\n",
       "      <td>-0.716560</td>\n",
       "      <td>3.372018</td>\n",
       "      <td>3.742349</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>-0.011504</td>\n",
       "      <td>-0.823814</td>\n",
       "      <td>-0.538357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298919</td>\n",
       "      <td>0.747178</td>\n",
       "      <td>-0.292879</td>\n",
       "      <td>-0.413609</td>\n",
       "      <td>0.695876</td>\n",
       "      <td>1.471391</td>\n",
       "      <td>-0.318451</td>\n",
       "      <td>-0.011694</td>\n",
       "      <td>-0.163276</td>\n",
       "      <td>67.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38884 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              V1        V2        V3        V4        V5        V6        V7  \\\n",
       "199907 -2.898534  2.335865 -2.580650 -1.635801  1.004089  0.241421 -1.947226   \n",
       "108425  0.465652 -2.211063  0.825680  0.002524 -2.161795  0.044469 -0.651862   \n",
       "265756  0.127149  0.813936 -0.542322 -1.174614  1.705919 -1.290807  2.004082   \n",
       "240309  1.997032  0.471299 -2.288758  1.373118  1.010453 -0.673412  0.530564   \n",
       "230792 -2.396032 -0.553487  1.312400 -1.132354  0.245760 -0.129237  0.889585   \n",
       "32450  -0.827556 -0.083257  2.062609 -1.916331 -0.998365  0.063317 -0.434585   \n",
       "207728  2.134170  0.038352 -2.338952 -0.076475  0.711083 -1.310592  0.725398   \n",
       "236892  1.933270 -1.350878  0.652871  1.011541 -1.594781  1.221503 -1.850292   \n",
       "244943 -1.173946  0.609209  0.741983 -1.809244  0.180076 -0.010626  0.312183   \n",
       "267824  1.844686 -0.619484 -0.394620  0.343100 -0.214237  0.969722 -0.901900   \n",
       "268131 -0.114616  0.251710  1.536465  1.066808 -0.350781  0.473709 -0.615864   \n",
       "193708 -2.156828  2.678184 -2.279124 -1.087435  0.367839 -0.178373 -0.006520   \n",
       "169862  2.017953  0.058252 -1.619452  0.353067  0.315459 -0.681202  0.036451   \n",
       "260085 -0.466681  1.470471 -0.909695 -0.251940  1.616060 -1.919969  1.482073   \n",
       "8670    0.759117 -0.860906  0.407423  0.343693 -0.512834  0.610339 -0.332852   \n",
       "189698  1.894893  0.371393 -0.064436  3.607553  0.201358  0.689180 -0.427763   \n",
       "160534  1.979639 -1.029783 -1.444450 -0.694480 -0.671629 -1.360079 -0.017294   \n",
       "126091 -2.597581 -0.767684  1.272246  0.336127  1.791811 -0.642956 -0.466455   \n",
       "34355  -1.414166 -0.426362  0.295761  0.193104  2.475615 -2.339276  0.111606   \n",
       "171997  2.166922 -0.753291 -0.887595 -1.106112 -0.636526 -0.684831 -0.650198   \n",
       "57661  -0.966652  1.478130  0.771943  0.333734  0.621895  0.681178  0.432067   \n",
       "135738 -0.968716  0.781718  1.227021  0.267244  0.945096  0.043601  0.315519   \n",
       "13767   0.991567 -0.209039  0.765694  0.377316  0.006870  1.156989 -0.509382   \n",
       "137127 -2.240391  1.562074  0.867433 -1.680612 -0.558472 -0.438408  0.095502   \n",
       "210631  0.106760  0.949853 -0.535160 -0.745178  1.193639 -0.232937  0.840860   \n",
       "149812  0.042436  0.892376  0.410827 -0.379318  0.405166 -1.020157  0.825473   \n",
       "88972   1.232193 -0.229653 -0.332639  0.085176  1.573836  3.959564 -1.130371   \n",
       "182871  1.929301 -0.223788 -3.120130  0.292451  2.889450  3.318414  0.070513   \n",
       "178562  1.951046 -0.393448 -0.628717  0.241213 -0.578649 -0.208219 -0.899786   \n",
       "208441 -2.511998 -2.781748  1.408172 -3.048312  2.863623 -0.482142 -1.593723   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "152574  1.973531 -0.474499  0.197153  0.549060 -1.020910 -0.398638 -1.011277   \n",
       "241318 -0.074110 -0.347253  1.745622  0.381817  0.094591  3.256371 -1.262272   \n",
       "50927  -4.678386  3.451893 -1.946664  0.277297 -3.050028 -0.865884 -2.385780   \n",
       "187342  2.041429 -1.886582 -1.072220 -1.705269 -1.137088  0.256496 -1.284295   \n",
       "128365  1.231586 -0.413043  1.098834 -0.843303 -1.407439 -0.837886 -0.786304   \n",
       "265684 -0.484560  0.659441  1.732009 -0.660167  0.006633 -0.396754  0.581202   \n",
       "167547 -2.210887 -1.344151 -0.064065  0.618148  3.610308 -1.203713 -0.615840   \n",
       "268760  2.084722 -0.076536 -1.456916  0.055482  0.485344 -0.283433  0.124140   \n",
       "197784  2.071716 -1.176000 -1.196195 -1.092389 -0.495480  0.029847 -0.801790   \n",
       "155257 -0.490327  0.807327  2.399965 -0.103815  0.076465 -0.372893  0.505751   \n",
       "142660 -3.110861  0.982156  0.330847  0.829458 -0.242277 -0.856808 -1.567547   \n",
       "72505   1.096057 -0.522901  0.875029  0.185963 -0.921279 -0.049623 -0.469502   \n",
       "204990  0.022811  0.352146  0.277257  1.047408  0.871873 -0.544502  0.316577   \n",
       "195383  0.469750 -1.237555 -1.767341  4.833490 -0.268715 -0.512760  1.140149   \n",
       "44696   0.937545 -0.543255  1.027463  0.512110 -0.831667  0.385503 -0.634467   \n",
       "31864   1.234139 -0.943517  0.695706 -1.339927 -1.509932 -0.613180 -0.797182   \n",
       "233909 -1.050381  0.601436 -0.480727 -2.287084 -0.156796 -1.011860 -0.067803   \n",
       "183101  0.116849  0.746846 -2.580209 -1.114107  2.711441  3.133278  0.076749   \n",
       "88265  -1.206488  1.004657  0.152853 -1.962787 -0.656907 -0.706286 -0.466546   \n",
       "72476   0.683389 -0.940750 -0.032823  0.352668 -0.898172 -1.047873  0.440964   \n",
       "73886  -1.435844  0.372468  1.526811 -1.628174 -1.265586  0.131469 -0.385453   \n",
       "123547 -2.429452  2.028011 -0.981596 -2.806809  1.682207  3.026534 -0.385599   \n",
       "190267  0.108692  0.883450 -0.525120 -0.720752  1.063351 -0.301055  0.760852   \n",
       "27586   1.115923 -0.208065  1.272632  1.244066 -1.095039  0.070824 -0.797603   \n",
       "42965  -2.588478 -2.640332  2.471179 -1.262654  1.707620 -0.594148 -1.034840   \n",
       "49424  -1.362896  0.784266  2.135114  0.901308 -1.010935  1.006875  0.089636   \n",
       "116825 -0.444432  0.943937  1.329332 -0.188890  0.385943 -0.169757  0.581994   \n",
       "40971  -1.497122  1.152116  1.661257  1.564480 -0.985292  1.088036 -0.812311   \n",
       "273455 -0.510381  0.946413  1.925877 -0.538333  0.386981  0.026039  0.710646   \n",
       "182867 -0.710812  0.798818 -1.195173 -0.716560  3.372018  3.742349  0.709788   \n",
       "\n",
       "               V8        V9       V10   ...         V20       V21       V22  \\\n",
       "199907 -11.701934 -0.582457  1.987813   ...   -0.469586  1.863208  0.496664   \n",
       "108425   0.106444 -0.153201  0.413537   ...    0.316451 -0.287784 -1.102149   \n",
       "265756  -0.921752 -0.128364 -0.129620   ...   -0.091926  0.192409  0.910717   \n",
       "240309  -0.162376 -0.069638 -0.176525   ...   -0.258244  0.006095  0.182688   \n",
       "230792  -0.015832  0.677545 -0.746701   ...   -1.033433 -0.613249 -1.087025   \n",
       "32450    0.418814 -0.720426  0.376913   ...    0.027090  0.481265  1.239019   \n",
       "207728  -0.478574  0.308230  0.029776   ...   -0.228567  0.127905  0.421162   \n",
       "236892   0.425636  1.405681  0.604810   ...   -0.600846 -0.269129  0.228633   \n",
       "244943   0.399817 -1.805899 -0.311466   ...   -0.596603 -0.114611  0.072045   \n",
       "267824   0.434638  1.577066 -0.286928   ...   -0.366190  0.265499  1.103009   \n",
       "268131   0.260233  0.662910  0.024137   ...    0.103496  0.173948  0.673010   \n",
       "193708   1.181235  0.395433  0.686286   ...    0.611705 -0.434500 -0.915794   \n",
       "169862  -0.059286  0.334024 -0.187849   ...   -0.198141 -0.309022 -0.858695   \n",
       "260085  -0.320694 -0.312395 -2.290189   ...   -0.076373 -0.009784  0.003451   \n",
       "8670     0.196187  1.863866 -0.725106   ...    0.172922 -0.047264 -0.076314   \n",
       "189698   0.120717 -0.600859  1.400583   ...   -0.156698  0.184745  0.555916   \n",
       "160534  -0.465088 -0.517971  0.731342   ...    0.208411  0.104271  0.070109   \n",
       "126091   0.810727 -1.083739 -0.859014   ...    0.303990  0.408754  0.346980   \n",
       "34355   -0.394224 -0.103002 -0.722651   ...   -0.393367 -0.030060  0.260830   \n",
       "171997  -0.096660 -0.626875  0.958453   ...   -0.027292 -0.203135 -0.722754   \n",
       "57661   -0.883951 -0.588684  0.376683   ...   -0.147157  0.851272 -0.120858   \n",
       "135738   0.050625 -0.849959 -0.145913   ...    0.198595 -0.213616 -0.788073   \n",
       "13767    0.331841  1.755394 -0.891513   ...   -0.172627 -0.308394 -0.358728   \n",
       "137127   0.576837  1.062155  1.103078   ...    0.401158 -0.191431 -0.229241   \n",
       "210631   0.085040 -0.286439 -0.672787   ...    0.042659 -0.305864 -0.764512   \n",
       "149812  -0.195507  1.417329 -0.755035   ...   -0.166944 -0.419621 -0.830058   \n",
       "88972    1.067693  0.628682 -0.085159   ...   -0.017532 -0.063580 -0.169507   \n",
       "182871   0.660561  0.039372  0.259345   ...   -0.115701  0.071827  0.199280   \n",
       "178562   0.177414  1.691489 -0.718515   ...   -0.162766 -0.021552  0.042071   \n",
       "208441   0.136738 -1.832276  0.744206   ...   -0.800161 -0.254756  0.077137   \n",
       "...           ...       ...       ...   ...         ...       ...       ...   \n",
       "152574  -0.017140  2.768881 -0.373530   ...   -0.240604 -0.129143 -0.008698   \n",
       "241318   1.318783  1.921173 -0.724335   ...   -0.460282  0.231311  1.107849   \n",
       "50927    3.357111 -0.523805 -0.457277   ...   -0.541724  0.198531 -0.418373   \n",
       "187342   0.065178 -1.243489  1.660271   ...   -0.157120 -0.057725  0.016847   \n",
       "128365   0.001675  1.810669 -1.003697   ...   -0.094942  0.267405  0.923146   \n",
       "265684  -0.246530  0.514145 -0.325413   ...    0.159222  0.174096  0.947562   \n",
       "167547   0.371216 -0.271876 -1.329248   ...    0.796152  0.043285 -0.546567   \n",
       "268760  -0.173196  0.218975  0.216269   ...   -0.116798 -0.292322 -0.704764   \n",
       "197784  -0.007139 -0.229354  0.885753   ...    0.134972  0.228813  0.550809   \n",
       "155257  -0.253555  1.576216 -1.124264   ...    0.033987 -0.313714 -0.465484   \n",
       "142660  -1.927998 -1.061453 -1.129589   ...    0.477053 -1.479701 -0.464347   \n",
       "72505   -0.010056  1.033144 -0.525924   ...    0.180929 -0.204105 -0.437802   \n",
       "204990  -0.139794 -0.447481  0.185510   ...    0.331010 -0.168732 -0.448164   \n",
       "195383  -0.341273 -1.046351  0.085662   ...    1.277315  0.303905 -0.647075   \n",
       "44696    0.131506  0.574428 -0.256355   ...    0.268107  0.308424  0.748671   \n",
       "31864   -0.034444  0.574644 -0.304890   ...   -0.487586 -0.156467  0.248959   \n",
       "233909   0.660398 -1.750226 -0.312153   ...   -0.255996  0.262440  0.295577   \n",
       "183101   1.214324 -0.321833 -1.183319   ...   -0.075537  0.343801  0.795495   \n",
       "88265    0.801169 -1.248699  0.062107   ...    0.095808  0.203655  0.396986   \n",
       "72476   -0.293963  0.178112 -0.269365   ...    0.533982 -0.045068 -0.877869   \n",
       "73886   -2.730749 -1.735721 -0.798762   ...    0.659205 -1.266709  0.107082   \n",
       "123547   1.459261  1.016029  1.261373   ...    0.955400 -0.298395 -0.659499   \n",
       "190267   0.144138 -0.122277 -0.638261   ...   -0.037977 -0.315987 -0.842099   \n",
       "27586    0.346743  0.923676  0.025468   ...   -0.254451 -0.035142 -0.015426   \n",
       "42965    0.517822  1.547498 -1.560564   ...    0.687919  0.187373  0.092052   \n",
       "49424    0.724738 -0.537778 -0.626742   ...   -0.066826  0.173608  0.545437   \n",
       "116825   0.112878 -0.347264 -0.348392   ...    0.082414 -0.231877 -0.581957   \n",
       "40971    1.394429  0.124257 -0.608783   ...   -0.134520 -0.233120 -0.425322   \n",
       "273455  -0.164836  0.246040 -0.599644   ...    0.312857 -0.247204 -0.310712   \n",
       "182867  -0.011504 -0.823814 -0.538357   ...   -0.298919  0.747178 -0.292879   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \n",
       "199907  0.496874  1.174098 -1.652801  0.379717 -1.024649 -0.939494   64.99  \n",
       "108425 -0.179108  0.580409 -0.197611  0.839903 -0.090394  0.082187  419.27  \n",
       "265756 -0.317445  0.785640 -0.424653 -0.031038 -0.304005 -0.241409    3.90  \n",
       "240309  0.012547  0.568676  0.433247 -0.539873 -0.006559 -0.034073    1.00  \n",
       "230792  0.617427 -0.476856  0.532635 -0.353114 -0.283374  0.100989  103.99  \n",
       "32450  -0.212110 -0.016285 -0.050586 -0.174210  0.220622  0.151746   39.00  \n",
       "207728 -0.033422  0.616146  0.457346  0.217986 -0.097946 -0.073277   15.33  \n",
       "236892  0.088670 -0.789299 -0.163106 -0.425544  0.157662 -0.015464   40.00  \n",
       "244943 -0.516300  0.773349  0.970481  0.701896 -0.285448 -0.050652   11.50  \n",
       "267824  0.205172 -1.114025 -0.337757 -0.107353  0.090250 -0.050351    9.99  \n",
       "268131  0.014641  0.674069 -1.146933 -0.080433  0.220144  0.215169   15.40  \n",
       "193708  0.142096 -0.310162  0.018270  0.150617  0.321682  0.023475   10.71  \n",
       "169862  0.359254  0.649453 -0.338457  0.145044 -0.071171 -0.040289    1.98  \n",
       "260085 -0.454421 -0.321732  0.438356 -0.511653  0.140682  0.199139    5.00  \n",
       "8670   -0.162885 -0.240179  0.151794  1.095899 -0.104776  0.009129  200.00  \n",
       "189698  0.187321  0.463481 -0.193536 -0.023847  0.007603 -0.022976   13.92  \n",
       "160534  0.082159  0.067652 -0.053678 -0.295420 -0.062363 -0.041079  136.47  \n",
       "126091 -0.326109 -0.258752  0.533110 -0.390259 -0.029540 -0.320822    9.49  \n",
       "34355  -0.204197  0.329769 -1.102415 -0.024056  0.131416 -0.047611    0.76  \n",
       "171997  0.450314 -0.464699 -0.578893 -0.690986 -0.017831 -0.058220    9.89  \n",
       "57661  -0.176219 -1.035003 -0.085018 -0.395726 -0.337084  0.144295   25.00  \n",
       "135738 -0.252262 -0.863587  0.129750  0.302896 -0.296841  0.218349    0.89  \n",
       "13767   0.253165 -0.610914 -0.168959  0.886243 -0.019708 -0.004244   34.56  \n",
       "137127  0.084992 -0.028267 -0.263373  0.726299  0.374340  0.182592    7.68  \n",
       "210631  0.049295  0.117717 -0.416061  0.122208  0.220586  0.070412    3.59  \n",
       "149812  0.099268 -0.113272 -0.567867  0.108790  0.216404  0.087752    2.28  \n",
       "88972  -0.034198  0.997747  0.552384 -0.384011  0.072874  0.031000    1.18  \n",
       "182871 -0.012749  0.709901  0.492767 -0.477041 -0.007833 -0.059462   57.78  \n",
       "178562  0.292816  0.480529 -0.473001 -0.097127  0.037072  0.001485   19.99  \n",
       "208441 -0.701778 -0.286356  0.508312 -0.168597  0.095949 -0.309488   12.00  \n",
       "...          ...       ...       ...       ...       ...       ...     ...  \n",
       "152574  0.314886 -0.159973 -0.624526  0.451226 -0.044144 -0.045731   15.95  \n",
       "241318 -0.082169 -0.996882 -0.568866 -0.456633  0.201699  0.007315    1.00  \n",
       "50927   0.241571  0.536152  0.228046  0.160962 -1.251586 -0.306001   12.23  \n",
       "187342  0.120146  0.112397 -0.290600 -0.203048 -0.004073 -0.033163  136.00  \n",
       "128365 -0.115746  0.404664  0.482778 -0.550712  0.102953  0.041372    8.00  \n",
       "265684 -0.266288  0.134397 -0.329606  0.504733  0.078024  0.006248   11.50  \n",
       "167547 -0.052014 -0.309504  0.993172  0.013963 -0.037262  0.139016   43.55  \n",
       "268760  0.188002 -1.012921 -0.164213  0.238378 -0.070871 -0.077936    5.48  \n",
       "197784 -0.024919 -1.131703 -0.067649 -0.133243 -0.019360 -0.061658   80.50  \n",
       "155257 -0.299141 -0.068129  0.249814 -0.729107 -0.082298 -0.146533    2.12  \n",
       "142660 -0.523528  0.408897 -0.091166  0.192121 -0.014562 -0.307888    3.27  \n",
       "72505  -0.066361 -0.000833  0.217877  0.960943 -0.040551  0.025671   86.80  \n",
       "204990  0.106051 -0.633335 -0.995675  0.592678  0.179258  0.210541    6.75  \n",
       "195383 -0.373014  0.260801 -0.496566 -0.245973 -0.117858  0.144774  723.21  \n",
       "44696  -0.192632 -0.330654  0.176630  0.619430  0.013191  0.047957  137.95  \n",
       "31864  -0.058356  0.419196  0.513941 -0.490566  0.114715  0.038206   41.95  \n",
       "233909 -0.140619 -0.334669 -0.215177 -0.450262 -0.272015 -0.011373    4.13  \n",
       "183101  0.046289  0.615515 -0.217676 -0.130093 -0.063341 -0.048145   52.00  \n",
       "88265  -0.160760 -0.571046  0.034573 -0.343876  0.228299  0.120429    5.00  \n",
       "72476  -0.160987  0.432490  0.041511  0.783159 -0.148393  0.058986  319.00  \n",
       "73886  -0.530021  0.391454 -0.107222 -0.586771  0.156965  0.061044  264.39  \n",
       "123547  0.028475  1.006401  0.199777  0.767798  0.862653  0.538995    1.54  \n",
       "190267  0.064093  0.158193 -0.464388  0.121649  0.214521  0.068449    0.89  \n",
       "27586   0.008990  0.274308  0.321104 -0.419127  0.056912  0.023216    5.50  \n",
       "42965   0.386099 -0.327056  0.474424 -0.800287 -0.146961 -0.112966  117.99  \n",
       "49424   0.029496  0.030706  0.201539 -0.257087 -0.042535 -0.033667  132.27  \n",
       "116825 -0.048439 -0.447981 -0.202409  0.132211  0.279578  0.112970    1.79  \n",
       "40971  -0.016851  0.023437 -0.065413 -0.394613  0.078557  0.015980   18.90  \n",
       "273455 -0.420937 -0.672705  0.474619 -0.634045  0.198508 -0.083822    1.00  \n",
       "182867 -0.413609  0.695876  1.471391 -0.318451 -0.011694 -0.163276   67.77  \n",
       "\n",
       "[38884 rows x 29 columns]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the two training datasets\n",
    "train = X_train.copy()\n",
    "train[\"Class\"] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    38588\n",
       "1      296\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Class count\n",
    "train.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    296\n",
       "0    296\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = train.Class.value_counts()[1]\n",
    "\n",
    "fraud_maj = train[train.Class==0]\n",
    "fraud_min = train[train.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_majority_downsampled = resample(fraud_maj, \n",
    "                                 replace=False,     # Do not sample with replacement\n",
    "                                 n_samples=N,    # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_ds = pd.concat([fraud_majority_downsampled, fraud_min])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_ds.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train Logistic Regression on downsampled data and evaluate it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign X and y\n",
    "fraud_ds_X = fraud_ds.drop(\"Class\", axis = 1)\n",
    "fraud_ds_y = fraud_ds.Class\n",
    "#Intialize\n",
    "lr = LogisticRegression()\n",
    "lr.fit(fraud_ds_X, fraud_ds_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.992439\n",
       "1    0.007561\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null accuracy \n",
    "\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554063958646761"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on testing dataset\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13414634146341464"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision\n",
    "\n",
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979591836734694"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall\n",
    "\n",
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24591,  1136],\n",
       "       [   20,   176]])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your interpretation now?\n",
    "\n",
    "<br>\n",
    "Let's the upsampling technique to see if that produces a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot sample 38588 out of arrays with dim 296 when replace is False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-447-730d00ad16be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Downsample majority class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfraud_minority_upsampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfraud_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Combine majority class with upsampled minority class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError(\"Cannot sample %d out of arrays with dim %d \"\n\u001b[1;32m    255\u001b[0m                          \"when replace is False\" % (max_n_samples,\n\u001b[0;32m--> 256\u001b[0;31m                                                     n_samples))\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot sample 38588 out of arrays with dim 296 when replace is False"
     ]
    }
   ],
   "source": [
    "N = train.Class.value_counts()[0]\n",
    "\n",
    "fraud_maj = train[train.Class==0]\n",
    "fraud_min = train[train.Class==1]\n",
    " \n",
    "# Downsample majority class\n",
    "fraud_minority_upsampled = resample(fraud_min, replace=False, n_samples=N, random_state=123)\n",
    " \n",
    "# Combine majority class with upsampled minority class\n",
    "fraud_us = pd.concat([fraud_minority_upsampled, fraud_maj])\n",
    " \n",
    "# Display new class counts\n",
    "fraud_us.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign X and y\n",
    "fraud_us_X = fraud_ds.drop(\"Class\", axis = 1)\n",
    "fraud_us_y = fraud_ds.Class\n",
    "#Intialize\n",
    "lr = LogisticRegression()\n",
    "lr.fit(fraud_us_X, fraud_us_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9554063958646761"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate on testing dataset\n",
    "\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13414634146341464"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#precision\n",
    "\n",
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8979591836734694"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#recall\n",
    "\n",
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24591,  1136],\n",
       "       [   20,   176]])"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion matrix\n",
    "confusion_matrix(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we make of these results??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the good news. We can set a class_weight setting in our models to be assigned to \"balanced\".\n",
    "\n",
    "From sklearn:\n",
    "\n",
    "\"The 'balanced' mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll cross validate with Logistic Regression, Decision Trees, and RandomForest models with the class_weight parameter set to \"balanced.\"\n",
    "\n",
    "But first let's calculate those weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 0 weight\n",
    "(y.shape[0])/float((2*y.value_counts()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class 1 weight\n",
    "(y.shape[0])/float((2*y.value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple class counts by weights\n",
    "64315*0.5038249242011972"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "65*492"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression \n",
    "\n",
    "cross_val_score(LogisticRegression(class_weight=\"balanced\"), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree with max depth = 12\n",
    "\n",
    "cross_val_score(DecisionTreeClassifier(class_weight=\"balanced\", max_depth=14), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest with n_estimators = 40\n",
    "\n",
    "cross_val_score(RandomForestClassifier(class_weight=\"balanced\", n_estimators=40), \n",
    "                X, y, cv = 5, scoring=\"accuracy\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "MNIST:\n",
    "\n",
    "- https://github.com/grfiv/MNIST/blob/master/knn/knn.ipynb\n",
    "- https://github.com/monsta-hd/ml-mnist\n",
    "- https://www.youtube.com/watch?v=aZsZrkIgan0\n",
    "- http://joshmontague.com/posts/2016/mnist-scikit-learn/\n",
    "\n",
    "Multi-class:\n",
    "\n",
    "- https://gallery.cortanaintelligence.com/Competition/Tutorial-Competition-Iris-Multiclass-Classification-2\n",
    "-https://www.youtube.com/watch?v=6kzvrq-MIO0\n",
    "\n",
    "\n",
    "Imbalanced classes:\n",
    "\n",
    "- https://towardsdatascience.com/what-metrics-should-we-use-on-imbalanced-data-set-precision-recall-roc-e2e79252aeba\n",
    "- https://svds.com/learning-imbalanced-classes/\n",
    "- https://www.youtube.com/watch?v=X9MZtvvQDR4\n",
    "- https://elitedatascience.com/imbalanced-classes\n",
    "- https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "Grid Search and Pipelines\n",
    "\n",
    "- https://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html\n",
    "- https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "- https://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html\n",
    "- https://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html\n",
    "- https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "- https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Gridsearch with Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  MEDV  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston[\"data\"])\n",
    "df.columns = boston[\"feature_names\"]\n",
    "df[\"MEDV\"] = boston[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = df.drop(\"MEDV\", axis =1)\n",
    "y = df.MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Pipeline Class instead of function to establish pipeline\n",
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                           ('linearregression', LinearRegression())])                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a few features from X\n",
    "XX = X[[\"RM\", \"DIS\", \"NOX\", \"CRIM\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize range values for poly\n",
    "poly_range = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "#Intialize grid dictionary\n",
    "param_grid_poly = {}\n",
    "\n",
    "#Input grid values\n",
    "param_grid_poly[\"polynomialfeatures__degree\"] = poly_range\n",
    "\n",
    "#Establish the grid\n",
    "poly_grid = GridSearchCV(pipe_poly, \n",
    "                         param_grid = param_grid_poly, cv=5, \n",
    "                         scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'polynomialfeatures__degree': [1, 2, 3, 4, 5, 6, 7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True,\n",
       "       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_grid.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('polynomialfeatures', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('ridgeregression', Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001))]),\n",
       "          fit_params={}, iid=True, n_iter=5, n_jobs=1,\n",
       "          param_distributions={'polynomialfeatures__degree': [1, 2, 3, 4, 5], 'ridgeregression__alpha': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                            ('ridgeregression', Ridge())]) \n",
    "\n",
    "param_grid_ridge = {'polynomialfeatures__degree': [1, 2, 3, 4, 5],\n",
    "              'ridgeregression__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_ridge = RandomizedSearchCV(pipe_poly, param_distributions=param_grid_ridge, \n",
    "                                n_iter = 5 , cv = 5, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ridgeregression__alpha': 0.01, 'polynomialfeatures__degree': 2} -41.51075524490348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (grid_ridge.best_params_, grid_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
