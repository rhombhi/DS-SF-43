{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "\n",
    "- Basics of NLP: tokenization, stopwords, POS tagging, stemming/lematization\n",
    "- TextBlob library. How to process text with it and do sentiment analysis\n",
    "- Text classification in sklearn: vectorizing text, modeling with naive bayes, and model optimization with grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is NLP?\n",
    "\n",
    "- Using computers to process (analyze, understand, generate) natural human languages\n",
    "- Most knowledge created by humans is unstructured text, and we need a way to make sense of it\n",
    "- Build probabilistic model using data about a language\n",
    "- Also referred to as machine learning with text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "- **Information retrieval**: Find relevant results and similar results\n",
    "    - [Google](https://www.google.com/)\n",
    "- **Information extraction**: Structured information from unstructured documents\n",
    "    - [Events from Gmail](https://support.google.com/calendar/answer/6084018?hl=en)\n",
    "- **Machine translation**: One language to another\n",
    "    - [Google Translate](https://translate.google.com/)\n",
    "- **Text simplification**: Preserve the meaning of text, but simplify the grammar and vocabulary\n",
    "    - [Rewordify](https://rewordify.com/)\n",
    "    - [Simple English Wikipedia](https://simple.wikipedia.org/wiki/Main_Page)\n",
    "- **Predictive text input**: Faster or easier typing\n",
    "    - [A friend's application](https://justmarkham.shinyapps.io/textprediction/)\n",
    "    - [A much better application](https://farsite.shinyapps.io/swiftkey-cap/)\n",
    "- **Sentiment analysis**: Attitude of speaker\n",
    "    - [Hater News](http://haternews.herokuapp.com/)\n",
    "- **Automatic summarization**: Extractive or abstractive summarization\n",
    "    - [autotldr](https://www.reddit.com/r/technology/comments/35brc8/21_million_people_still_use_aol_dialup/cr2zzj0)\n",
    "- **Natural Language Generation**: Generate text from data\n",
    "    - [How a computer describes a sports match](http://www.bbc.com/news/technology-34204052)\n",
    "    - [Publishers withdraw more than 120 gibberish papers](http://www.nature.com/news/publishers-withdraw-more-than-120-gibberish-papers-1.14763)\n",
    "- **Speech recognition and generation**: Speech-to-text, text-to-speech\n",
    "    - [Google's Web Speech API demo](https://www.google.com/intl/en/chrome/demos/speech.html)\n",
    "    - [Vocalware Text-to-Speech demo](https://www.vocalware.com/index/demo)\n",
    "- **Question answering**: Determine the intent of the question, match query with knowledge base, evaluate hypotheses\n",
    "    - [How did supercomputer Watson beat Jeopardy champion Ken Jennings?](http://blog.ted.com/how-did-supercomputer-watson-beat-jeopardy-champion-ken-jennings-experts-discuss/)\n",
    "    - [IBM's Watson Trivia Challenge](http://www.nytimes.com/interactive/2010/06/16/magazine/watson-trivia-game.html)\n",
    "    - [The AI Behind Watson](http://www.aaai.org/Magazine/Watson/watson.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Tools\n",
    "\n",
    "- **Tokenization**: breaking text into tokens (words, sentences, n-grams)\n",
    "- **Stopword removal**: a/an/the\n",
    "- **Stemming and lemmatization**: root word\n",
    "- **TF-IDF**: word importance\n",
    "- **Part-of-speech tagging**: noun/verb/adjective\n",
    "- **Named entity recognition**: person/organization/location\n",
    "- **Spelling correction**: \"New Yrok City\"\n",
    "- **Word sense disambiguation**: \"buy a mouse\"\n",
    "- **Segmentation**: \"New York City subway\"\n",
    "- **Language detection**: \"translate this page\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP is hard! Here's why\n",
    "\n",
    "- **Ambiguity**:\n",
    "    - Hospitals are Sued by 7 Foot Doctors\n",
    "    - Juvenile Court to Try Shooting Defendant\n",
    "    - Local High School Dropouts Cut in Half\n",
    "- **Non-standard English**: text messages, \"y r u\" vs \"why are you\"\n",
    "- **Idioms**: \"throw in the towel\"\n",
    "- **Newly coined words**: \"retweet\", \"clickbait\", \"fleek\"\n",
    "- **Tricky entity names**: \"Where is A Bug's Life playing?\"\n",
    "- **World knowledge**: \"Mary and Sue are sisters\", \"Mary and Sue are mothers\"\n",
    "- **Texts with the same words and phrases can having different meanings **: \n",
    "State farm commercial where two different people say \"Is this my car? What? This is ridiculous! This can't be happening! Shut up! Ahhhh!!!\"\n",
    "\n",
    "\n",
    "NLP requires an understanding of the **language** and the **world**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with the NLTK library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point NLTK should be installed and its additional materials should be downloaded as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, LancasterStemmer, PorterStemmer, WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloads the nltk data\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "- **What:** Separate text into units such as sentences or words\n",
    "- **Why:** Gives structure to previously unstructured text\n",
    "- **Notes:** Relatively easy with English language text, not easy with some languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello.',\n",
       " 'How are you, dear Mr. Sir?',\n",
       " 'Are you well?',\n",
       " 'Here: drink this!',\n",
       " 'It will make you feel better.',\n",
       " \"I mean, it won't make you feel worse!\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Hello. How are you, dear Mr. Sir? Are you well?\n",
    "          Here: drink this! It will make you feel better.\n",
    "          I mean, it won't make you feel worse!\"\"\"\n",
    "\n",
    "\n",
    "#Tokenize text using sent_tokenize function\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the output, can you figure out the rules of tokenization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean, it won't make you feel worse!\n",
      "['I', 'mean', ',', 'it', 'wo', \"n't\", 'make', 'you', 'feel', 'worse', '!']\n"
     ]
    }
   ],
   "source": [
    "#Assign last sentence in sentences to sentence\n",
    "\n",
    "sentence = sentences[5]\n",
    "\n",
    "#Word tokenize using one of the sentences from sentences\n",
    "#Assumes that input has already been tokenized into sentences\n",
    "\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(sentence)\n",
    "\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '.', 'How', 'are', 'you', ',', 'dear', 'Mr.', 'Sir', '?', 'Are', 'you', 'well', '?', 'Here', ':', 'drink', 'this', '!', 'It', 'will', 'make', 'you', 'feel', 'better', '.', 'I', 'mean', ',', 'it', 'wo', \"n't\", 'make', 'you', 'feel', 'worse', '!']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the word_tokenize function work? Let's try the wordpunct_tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'mean', ',', 'it', 'won', \"'\", 't', 'make', 'you', 'feel', 'worse', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pass sentence into wordpunct_tokenize function\n",
    "wordpunct_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whats the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMG', '!!', 'How', 'are', 'you', '??', '..']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize('OMG!! How are you?? ..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OMG', '!', '!', 'How', 'are', 'you', '?', '?', '..']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize('OMG!! How are you?? ..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online demo of various tokenizers: http://text-processing.com/demo/tokenize/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging\n",
    "\n",
    "<br>\n",
    "\n",
    "\"The process of assigning one of the parts of speech to the given word is called Parts Of Speech tagging. It is commonly referred to as POS tagging. Parts of speech include nouns, verbs, adverbs, adjectives, pronouns, conjunction and their sub-categories.\"\n",
    "\n",
    "http://language.worldofcomputing.net/pos-tagging/parts-of-speech-tagging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('assigning', 'VBG'),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('parts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('speech', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('given', 'VBN'),\n",
       " ('word', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('called', 'VBN'),\n",
       " ('Parts', 'NNS'),\n",
       " ('Of', 'IN'),\n",
       " ('Speech', 'NNP'),\n",
       " ('tagging', 'VBG')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text for POS tagging\n",
    "text = \"\"\"The process of assigning one of \n",
    "the parts of speech to the given word is called Parts Of Speech tagging\"\"\"\n",
    "\n",
    "#Tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# tokens\n",
    "#Pass tokens into pos_tag function\n",
    "pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is tuple pairings of tokens with their POS tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some of POS tags: \n",
    "WP: wh-pronoun (\"who\", \"what\")  \n",
    "VBZ: verb, 3rd person sing. present (\"takes\")  \n",
    "VBG: verb, gerund/present participle (\"taking\")  \n",
    "TO: to (\"to go\", \"to him\")   \n",
    "DT: determiner (\"the\", \"this\")  \n",
    "NN: noun, singular or mass (\"door\")  \n",
    "\n",
    "All tags: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Common words that will likely appear in any text. Anything that can appears in a poem, rap lyric, or medical research paper is most likely a stopword. In most NLP contexts, we remove the stopwords because they don't tell you much about your text, they have no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Intialize the list of stopwords \n",
    "\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View list of punctuation characters\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add them to the sw list\n",
    "\n",
    "sw += punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove stopwords and punctuation from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = \"\"\"Sony Michel's touchdown in double-overtime gave \n",
    "Georgia a 54-48 Rose Bowl win over Oklahoma and \n",
    "made up for a late fumble that resulted in six points for the Sooners.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sony', 'Michel', \"'\", 's', 'touchdown', 'in', 'double', '-', 'overtime', 'gave', 'Georgia', 'a', '54', '-', '48', 'Rose', 'Bowl', 'win', 'over', 'Oklahoma', 'and', 'made', 'up', 'for', 'a', 'late', 'fumble', 'that', 'resulted', 'in', 'six', 'points', 'for', 'the', 'Sooners', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize text\n",
    "\n",
    "tokens = wordpunct_tokenize(corpus)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sony',\n",
       " 'Michel',\n",
       " 'touchdown',\n",
       " 'double',\n",
       " 'overtime',\n",
       " 'gave',\n",
       " 'Georgia',\n",
       " '54',\n",
       " '48',\n",
       " 'Rose',\n",
       " 'Bowl',\n",
       " 'win',\n",
       " 'Oklahoma',\n",
       " 'made',\n",
       " 'late',\n",
       " 'fumble',\n",
       " 'resulted',\n",
       " 'six',\n",
       " 'points',\n",
       " 'Sooners']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean up tokens by removing stopwords and punctuation characters\n",
    "\n",
    "clean_tokens = [x for x in tokens if x not in sw]\n",
    "\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "<br>\n",
    "\n",
    "**Stemming:**\n",
    "\n",
    "- **What:** Reduce a word to its base/stem/root form\n",
    "- **Why:** Often makes sense to treat related words the same way\n",
    "- **Notes:**\n",
    "    - Uses a \"simple\" and fast rule-based approach\n",
    "    - Stemmed words are usually not shown to users (used for analysis/indexing)\n",
    "    - Some search engines treat words with the same stem as synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize stemmer object\n",
    "\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Derive stems from random words\n",
    "\n",
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absolut'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('absolutely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mad'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('madness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'forgav'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('forgave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ran'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('ran')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rain'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rain'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('rained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'student'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('student')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cours'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roma'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('Roma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer('hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gave'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('gave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('give')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sony', 'Michel', 'touchdown', 'double', 'overtime', 'gave', 'Georgia', '54', '48', 'Rose', 'Bowl', 'win', 'Oklahoma', 'made', 'late', 'fumble', 'resulted', 'six', 'points', 'Sooners']\n",
      "['soni', 'michel', 'touchdown', 'doubl', 'overtim', 'gave', 'georgia', '54', '48', 'rose', 'bowl', 'win', 'oklahoma', 'made', 'late', 'fumbl', 'result', 'six', 'point', 'sooner']\n"
     ]
    }
   ],
   "source": [
    "#Derive the stems of every token in clean tokens\n",
    "\n",
    "stems = [stemmer.stem(token) for token in clean_tokens]\n",
    "\n",
    "print(clean_tokens)\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the results of the stemming process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"Omg, Natural Language Processing is so cool and I'm really loving this workshop!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Omg',\n",
       " ',',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'is',\n",
       " 'so',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'really',\n",
       " 'loving',\n",
       " 'this',\n",
       " 'workshop',\n",
       " '!']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytokens = word_tokenize(mytext)\n",
    "mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', ',', 'nat', 'langu', 'process', 'is', 'so', 'cool', 'and', 'i', \"'m\", 'real', 'lov', 'thi', 'workshop', '!']\n"
     ]
    }
   ],
   "source": [
    "lop = [LancasterStemmer().stem(mytoken) for mytoken in mytokens]\n",
    "print(lop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', ',', 'natur', 'languag', 'process', 'is', 'so', 'cool', 'and', 'I', \"'m\", 'realli', 'love', 'thi', 'workshop', '!']\n"
     ]
    }
   ],
   "source": [
    "pop = [PorterStemmer().stem(mytoken) for mytoken in mytokens]\n",
    "print(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['omg', ',', 'natur', 'languag', 'process', 'is', 'so', 'cool', 'and', 'i', \"'m\", 'realli', 'love', 'this', 'workshop', '!']\n"
     ]
    }
   ],
   "source": [
    "sop = [SnowballStemmer('english').stem(mytoken) for mytoken in mytokens]\n",
    "print(sop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization**\n",
    "\n",
    "- **What:** Derive the canonical form ('lemma') of a word\n",
    "- **Why:** Can be better than stemming\n",
    "- **Notes:** Uses a dictionary-based approach (slower than stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast the stems and lemmatization of certain words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopi'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stem of octopi (plural of octopus)\n",
    "stemmer.stem('octopi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'octopus'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize lemmatization object\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatize octopi\n",
    "\n",
    "lem.lemmatize('octopi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? Try it again with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indic'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stem\n",
    "\n",
    "stemmer.stem('indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'index'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemon\n",
    "lem.lemmatize('indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the lemons of clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sony', 'Michel', 'touchdown', 'double', 'overtime', 'gave', 'Georgia', '54', '48', 'Rose', 'Bowl', 'win', 'Oklahoma', 'made', 'late', 'fumble', 'resulted', 'six', 'point', 'Sooners']\n"
     ]
    }
   ],
   "source": [
    "#Lemmative the clean tokens and set pos = v\n",
    "\n",
    "lemons = [lem.lemmatize(token) for token in clean_tokens ]\n",
    "\n",
    "print(lemons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams\n",
    "\n",
    "Collections of adjacent words, number of words in each collection is determined by N. \n",
    "\n",
    "Bigrams = Two-word phrases\n",
    "\n",
    "Trigrams = Three-word phrases\n",
    "\n",
    "http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sony', 'Michel'),\n",
       " ('Michel', 'touchdown'),\n",
       " ('touchdown', 'double'),\n",
       " ('double', 'overtime'),\n",
       " ('overtime', 'gave'),\n",
       " ('gave', 'Georgia'),\n",
       " ('Georgia', '54'),\n",
       " ('54', '48'),\n",
       " ('48', 'Rose'),\n",
       " ('Rose', 'Bowl'),\n",
       " ('Bowl', 'win'),\n",
       " ('win', 'Oklahoma'),\n",
       " ('Oklahoma', 'made'),\n",
       " ('made', 'late'),\n",
       " ('late', 'fumble'),\n",
       " ('fumble', 'resulted'),\n",
       " ('resulted', 'six'),\n",
       " ('six', 'points'),\n",
       " ('points', 'Sooners')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Set N to 2 for bigrams\n",
    "N = 2\n",
    "\n",
    "#Make bigrams from clean_tokens\n",
    "bigrams = ngrams(clean_tokens, 2)\n",
    "\n",
    "list(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Sony', 'Michel', 'touchdown'),\n",
       " ('Michel', 'touchdown', 'double'),\n",
       " ('touchdown', 'double', 'overtime'),\n",
       " ('double', 'overtime', 'gave'),\n",
       " ('overtime', 'gave', 'Georgia'),\n",
       " ('gave', 'Georgia', '54'),\n",
       " ('Georgia', '54', '48'),\n",
       " ('54', '48', 'Rose'),\n",
       " ('48', 'Rose', 'Bowl'),\n",
       " ('Rose', 'Bowl', 'win'),\n",
       " ('Bowl', 'win', 'Oklahoma'),\n",
       " ('win', 'Oklahoma', 'made'),\n",
       " ('Oklahoma', 'made', 'late'),\n",
       " ('made', 'late', 'fumble'),\n",
       " ('late', 'fumble', 'resulted'),\n",
       " ('fumble', 'resulted', 'six'),\n",
       " ('resulted', 'six', 'points'),\n",
       " ('six', 'points', 'Sooners')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Set N to 3 for trigrams\n",
    "N = 3\n",
    "\n",
    "#Make bigrams from clean_tokens\n",
    "trigrams = ngrams(clean_tokens,3)\n",
    "\n",
    "list(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "<br>\n",
    "\n",
    "Python library for processing simple NLP tasks.\n",
    "\n",
    "\n",
    "You may need to download the corpora in textblob. Type into command line:\n",
    "\n",
    "python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text for using TextBlob\n",
    "\n",
    "corpus = \"\"\"\n",
    "Mr. Persson, 35, sits in front of four computer screens,\n",
    "one displaying the loader he steers as it lifts freshly blasted rock containing silver,\n",
    "zinc and lead. If he were down in the mine shaft operating the loader manually,\n",
    "he would be inhaling dust and exhaust fumes. \n",
    "Instead, he reclines in an office chair while using a joystick to control the machine.\n",
    "\"\"\"\n",
    "\n",
    "#Pass in text into textblob\n",
    "blob = TextBlob(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the capabilities of textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mr', 'Persson', '35', 'sits', 'in', 'front', 'of', 'four', 'computer', 'screens', 'one', 'displaying', 'the', 'loader', 'he', 'steers', 'as', 'it', 'lifts', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fumes', 'Instead', 'he', 'reclines', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine'])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenized words\n",
    "\n",
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " Mr. Persson, 35, sits in front of four computer screens,\n",
       " one displaying the loader he steers as it lifts freshly blasted rock containing silver,\n",
       " zinc and lead.\"),\n",
       " Sentence(\"If he were down in the mine shaft operating the loader manually,\n",
       " he would be inhaling dust and exhaust fumes.\"),\n",
       " Sentence(\"Instead, he reclines in an office chair while using a joystick to control the machine.\")]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentences\n",
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"a.b.c.d.e.f hddkj.\"),\n",
       " Sentence(\"sjhdjhd.\"),\n",
       " Sentence(\"aaa.\"),\n",
       " Sentence(\"aaaaaab.\"),\n",
       " Sentence(\"sc.\"),\n",
       " Sentence(\"d. e. ff.\"),\n",
       " Sentence(\"gg.\")]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = TextBlob('a.b.c.d.e.f hddkj. sjhdjhd. aaa. aaaaaab. sc. d. e. ff. gg.')\n",
    "b.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'35': 1,\n",
       "             'a': 1,\n",
       "             'an': 1,\n",
       "             'and': 2,\n",
       "             'as': 1,\n",
       "             'be': 1,\n",
       "             'blasted': 1,\n",
       "             'chair': 1,\n",
       "             'computer': 1,\n",
       "             'containing': 1,\n",
       "             'control': 1,\n",
       "             'displaying': 1,\n",
       "             'down': 1,\n",
       "             'dust': 1,\n",
       "             'exhaust': 1,\n",
       "             'four': 1,\n",
       "             'freshly': 1,\n",
       "             'front': 1,\n",
       "             'fumes': 1,\n",
       "             'he': 4,\n",
       "             'if': 1,\n",
       "             'in': 3,\n",
       "             'inhaling': 1,\n",
       "             'instead': 1,\n",
       "             'it': 1,\n",
       "             'joystick': 1,\n",
       "             'lead': 1,\n",
       "             'lifts': 1,\n",
       "             'loader': 2,\n",
       "             'machine': 1,\n",
       "             'manually': 1,\n",
       "             'mine': 1,\n",
       "             'mr': 1,\n",
       "             'of': 1,\n",
       "             'office': 1,\n",
       "             'one': 1,\n",
       "             'operating': 1,\n",
       "             'persson': 1,\n",
       "             'reclines': 1,\n",
       "             'rock': 1,\n",
       "             'screens': 1,\n",
       "             'shaft': 1,\n",
       "             'silver': 1,\n",
       "             'sits': 1,\n",
       "             'steers': 1,\n",
       "             'the': 4,\n",
       "             'to': 1,\n",
       "             'using': 1,\n",
       "             'were': 1,\n",
       "             'while': 1,\n",
       "             'would': 1,\n",
       "             'zinc': 1})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Word counts\n",
    "\n",
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mr.', 'NNP'),\n",
       " ('Persson', 'NNP'),\n",
       " ('35', 'CD'),\n",
       " ('sits', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('front', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('four', 'CD'),\n",
       " ('computer', 'NN'),\n",
       " ('screens', 'NNS'),\n",
       " ('one', 'CD'),\n",
       " ('displaying', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('loader', 'NN'),\n",
       " ('he', 'PRP'),\n",
       " ('steers', 'VBZ'),\n",
       " ('as', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('lifts', 'VBZ'),\n",
       " ('freshly', 'RB'),\n",
       " ('blasted', 'VBN'),\n",
       " ('rock', 'NN'),\n",
       " ('containing', 'VBG'),\n",
       " ('silver', 'NN'),\n",
       " ('zinc', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('lead', 'NN'),\n",
       " ('If', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('were', 'VBD'),\n",
       " ('down', 'RB'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mine', 'NN'),\n",
       " ('shaft', 'NN'),\n",
       " ('operating', 'VBG'),\n",
       " ('the', 'DT'),\n",
       " ('loader', 'NN'),\n",
       " ('manually', 'RB'),\n",
       " ('he', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('inhaling', 'VBG'),\n",
       " ('dust', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('exhaust', 'JJ'),\n",
       " ('fumes', 'NNS'),\n",
       " ('Instead', 'RB'),\n",
       " ('he', 'PRP'),\n",
       " ('reclines', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('office', 'NN'),\n",
       " ('chair', 'NN'),\n",
       " ('while', 'IN'),\n",
       " ('using', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('joystick', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('control', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('machine', 'NN')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pos tags\n",
    "\n",
    "blob.pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['mr. persson', 'computer screens', 'mine shaft', 'exhaust fumes', 'office chair'])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Noun phrases\n",
    "\n",
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Singularize words\n",
    "blob.words.singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mrs', 'Perssons', '35s', 'sitss', 'ins', 'fronts', 'ofs', 'fours', 'computers', 'screenss', 'ones', 'displayings', 'thes', 'loaders', 'they', 'steerss', 'ass', 'they', 'liftss', 'freshlies', 'blasteds', 'rocks', 'containings', 'silvers', 'zincs', 'ands', 'leads', 'Ifs', 'they', 'weres', 'downs', 'ins', 'thes', 'ours', 'shafts', 'operatings', 'thes', 'loaders', 'manuallies', 'they', 'woulds', 'bes', 'inhalings', 'dusts', 'ands', 'exhausts', 'fumess', 'Insteads', 'they', 'recliness', 'ins', 'some', 'offices', 'chairs', 'whiles', 'usings', 'some', 'joysticks', 'toes', 'controls', 'thes', 'machines'])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pluralize words\n",
    "blob.words.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Mr', 'Persson', '35', 'sits', 'in', 'front', 'of', 'four', 'computer', 'screen', 'one', 'displaying', 'the', 'loader', 'he', 'steer', 'a', 'it', 'lift', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fume', 'Instead', 'he', 'reclines', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine'])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "blob.words.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', 'Persson', '35', 'sits', 'in', 'front', 'of', 'four', 'computer', 'screen', 'one', 'displaying', 'the', 'loader', 'he', 'steer', 'a', 'it', 'lift', 'freshly', 'blasted', 'rock', 'containing', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'were', 'down', 'in', 'the', 'mine', 'shaft', 'operating', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhaling', 'dust', 'and', 'exhaust', 'fume', 'Instead', 'he', 'reclines', 'in', 'an', 'office', 'chair', 'while', 'using', 'a', 'joystick', 'to', 'control', 'the', 'machine']\n"
     ]
    }
   ],
   "source": [
    "#or\n",
    "# print([word.lemmatize for word in blob.words])\n",
    "print([word.lemmatize() for word in blob.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr', 'Persson', '35', 'sit', 'in', 'front', 'of', 'four', 'computer', 'screen', 'one', 'display', 'the', 'loader', 'he', 'steer', 'as', 'it', 'lift', 'freshly', 'blast', 'rock', 'contain', 'silver', 'zinc', 'and', 'lead', 'If', 'he', 'be', 'down', 'in', 'the', 'mine', 'shaft', 'operate', 'the', 'loader', 'manually', 'he', 'would', 'be', 'inhale', 'dust', 'and', 'exhaust', 'fume', 'Instead', 'he', 'recline', 'in', 'an', 'office', 'chair', 'while', 'use', 'a', 'joystick', 'to', 'control', 'the', 'machine']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization with verbs\n",
    "print([word.lemmatize(pos='v') for word in blob.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Mr', 'Persson']),\n",
       " WordList(['Persson', '35']),\n",
       " WordList(['35', 'sits']),\n",
       " WordList(['sits', 'in']),\n",
       " WordList(['in', 'front']),\n",
       " WordList(['front', 'of']),\n",
       " WordList(['of', 'four']),\n",
       " WordList(['four', 'computer']),\n",
       " WordList(['computer', 'screens']),\n",
       " WordList(['screens', 'one']),\n",
       " WordList(['one', 'displaying']),\n",
       " WordList(['displaying', 'the']),\n",
       " WordList(['the', 'loader']),\n",
       " WordList(['loader', 'he']),\n",
       " WordList(['he', 'steers']),\n",
       " WordList(['steers', 'as']),\n",
       " WordList(['as', 'it']),\n",
       " WordList(['it', 'lifts']),\n",
       " WordList(['lifts', 'freshly']),\n",
       " WordList(['freshly', 'blasted']),\n",
       " WordList(['blasted', 'rock']),\n",
       " WordList(['rock', 'containing']),\n",
       " WordList(['containing', 'silver']),\n",
       " WordList(['silver', 'zinc']),\n",
       " WordList(['zinc', 'and']),\n",
       " WordList(['and', 'lead']),\n",
       " WordList(['lead', 'If']),\n",
       " WordList(['If', 'he']),\n",
       " WordList(['he', 'were']),\n",
       " WordList(['were', 'down']),\n",
       " WordList(['down', 'in']),\n",
       " WordList(['in', 'the']),\n",
       " WordList(['the', 'mine']),\n",
       " WordList(['mine', 'shaft']),\n",
       " WordList(['shaft', 'operating']),\n",
       " WordList(['operating', 'the']),\n",
       " WordList(['the', 'loader']),\n",
       " WordList(['loader', 'manually']),\n",
       " WordList(['manually', 'he']),\n",
       " WordList(['he', 'would']),\n",
       " WordList(['would', 'be']),\n",
       " WordList(['be', 'inhaling']),\n",
       " WordList(['inhaling', 'dust']),\n",
       " WordList(['dust', 'and']),\n",
       " WordList(['and', 'exhaust']),\n",
       " WordList(['exhaust', 'fumes']),\n",
       " WordList(['fumes', 'Instead']),\n",
       " WordList(['Instead', 'he']),\n",
       " WordList(['he', 'reclines']),\n",
       " WordList(['reclines', 'in']),\n",
       " WordList(['in', 'an']),\n",
       " WordList(['an', 'office']),\n",
       " WordList(['office', 'chair']),\n",
       " WordList(['chair', 'while']),\n",
       " WordList(['while', 'using']),\n",
       " WordList(['using', 'a']),\n",
       " WordList(['a', 'joystick']),\n",
       " WordList(['joystick', 'to']),\n",
       " WordList(['to', 'control']),\n",
       " WordList(['control', 'the']),\n",
       " WordList(['the', 'machine'])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bigrams\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "TextBlob uses an algorithm to rate text on subjectivity and polarity. Subjectivity measures how opinonated a text is on a scale from 0.0-1.0 and polarity measures how happy or mad or a text is on a scale from -1.0-1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.445, subjectivity=0.43)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text for sentiment analysis\n",
    "raw_text = \"I love learning about data science, it is very fun.\"\n",
    "\n",
    "#Pass in raw_text into textblob\n",
    "blob = TextBlob(raw_text)\n",
    "\n",
    "#Derive scores\n",
    "blob.sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Polarity score\n",
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Subjectivity score\n",
    "blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"it's so awesome\").sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I love this course.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.5, subjectivity=0.6)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Oh my god I love this course.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=1.0, subjectivity=1.0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"it's so awesome.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.8, subjectivity=0.9)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I hate cupcakes.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.0, subjectivity=0.0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"i have no opinions about the matter\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyze the sentiment of yelp reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in yelp review data\n",
    "\n",
    "path = \"../../data/NLP_data/yelp.csv\"\n",
    "\n",
    "yelp = pd.read_csv(path, encoding='unicode-escape')\n",
    "\n",
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\r\\n\\r\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\r\\n\\r\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\r\\n\\r\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read first review\n",
    "\n",
    "review = yelp.text[0]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.40246913580246907, subjectivity=0.6591122868900646)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Textblob review and get its sentiments scores\n",
    "\n",
    "blob = TextBlob(review)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the scores? Are they too high or low?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate polarity and subjectivity scores for entire corpus\n",
    "# by applying polarity and sentiment over yelp reviews df\n",
    "\n",
    "yelp[\"polarity\"] = yelp.text.apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "yelp[\"subjectivity\"] = yelp.text.apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most negative and positives reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust settings\n",
    "# pd.set_option('max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773     This was absolutely horrible. I got the supreme pizza with the mystery meats.  I threw it in the trash. I will wait until I get to my destination to eat. Horrible!!!\n",
       "1517                                                                                                                                      Nasty workers and over priced trash\n",
       "3266                                                                                                         Absolutely awful... these guys have NO idea what they are doing!\n",
       "4766                                                                                                                                                           Very bad food!\n",
       "5812                                                                                                                            I wouldn't send my worst enemy to this place.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most negative\n",
    "\n",
    "yelp[yelp.polarity==-1].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254                                                                                                                                                                             Our server Gary was awesome. Food was amazing...an experience.\n",
       "347                                                                                                                                                           3 syllables for this place. \\r\\nA-MAZ-ING!\\r\\n\\r\\nThe best Phoenix has to offer.\n",
       "420                                                                                                                                                                                                                          LOVE the food!!!!\n",
       "459    Love it!!! Wish we still lived in Arizona as Chino is the one thing we miss. Every time I think about Chino Bandido my mouth starts watering. If I am ever in the state again I will drive out of my way just to go to it again. YUMMY!\n",
       "679                                                                                                                                                                                                                           Excellent burger\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Most positive\n",
    "\n",
    "yelp[yelp.polarity==1].text.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there reviews with 5 stars but low polarity scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287    Obsessed. Like, I've-got-the-Twangy-Tart-withdrawal-shakes level of addiction to this place. Please make one in Arcadia! Pleeeaaassse.\n",
       "6726                                                   Brown bag chicken sammich, mac n cheese, fried okra, and the bourbon drink.  Nuff said.\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# yelp[(yelp.polarity==-1) & (yelp.stars==5)].text.head()\n",
    "\n",
    "yelp[(yelp.polarity<=-0.6) & (yelp.stars==5)].text.head()\n",
    "\n",
    "# yelp[(yelp.polarity==-1) & (yelp.stars==5)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8833    The owner has changed hands & this place isn't what it used to be.  If you want up to date paper & quality product...go to Scrap Happy OR Crop Girls!\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One star reviews with high polarity scores\n",
    "\n",
    "yelp[(yelp.polarity==1) & (yelp.stars==1)].text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a221484e0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22bcb278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of polarity scores\n",
    "\n",
    "yelp.polarity.hist(bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a22ec09b0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22bc75f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Histogram of subjectivity scores\n",
    "yelp.subjectivity.plot(kind='hist', bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot scatter plot of polarity vs subjectivity scores\n",
    "\n",
    "plt.xlabel(\"Polarity Scores\")\n",
    "plt.ylabel(\"Subjectivity Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a22ab4748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot boxplots of the polarity by yelp stars\n",
    "# yelp.polarity.plot.box(by='stars');\n",
    "yelp.boxplot(column='polarity',by='stars');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are you thoughts on the plots? Do they make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train a machine learning algorithm to classify yelp reviews as either five or one stars. But first we need to transform or \"vectorize\" our raw text before make any classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer: How to turn text into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame called yelp_best_worst that only contains the 5-star and 1-star reviews\n",
    "yelp_best_worst = yelp[(yelp.stars==5) | (yelp.stars==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    3337\n",
      "1     749\n",
      "Name: stars, dtype: int64\n",
      "5    0.816691\n",
      "1    0.183309\n",
      "Name: stars, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# define X and y\n",
    "X = yelp_best_worst.text\n",
    "y = yelp_best_worst.stars\n",
    "\n",
    "#Null accuracy\n",
    "print(y.value_counts())\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't pass in raw text into an algorithm, first we have to vectorize it, which means converting a collection of text documents to a matrix of token counts.\n",
    "\n",
    "<br>\n",
    "\n",
    "We will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cab', 'call', 'me', 'please', 'tonight', 'you']\n",
      "[[0 1 0 0 1 1]\n",
      " [1 1 1 0 0 0]\n",
      " [0 1 1 2 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency\n",
    "\n",
    "vect = CountVectorizer()\n",
    "dtm = vect.fit_transform(simple_train)\n",
    "\n",
    "# print(vect.get_params())\n",
    "print(vect.get_feature_names())\n",
    "print(dtm.toarray())\n",
    "\n",
    "tf = pd.DataFrame(data=dtm.toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cab  call  me  please  tonight  you\n",
      "0    0     1   0       1        0    0\n",
      "   call  please  taxi  yourself\n",
      "0     1       1     1         1\n"
     ]
    }
   ],
   "source": [
    "# transforming a new sentence\n",
    "new_sentence = ['please call yourself a taxi']\n",
    "\n",
    "# vect = CountVectorizer()\n",
    "print(pd.DataFrame(data=vect.transform(new_sentence).toarray(), columns=vect.get_feature_names()))\n",
    "print(pd.DataFrame(data=vect.fit_transform(new_sentence).toarray(), columns=vect.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? How come the two dataframes have the same features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use CountVectorizer to create document-term matrices from X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize vectorizer object\n",
    "vect = CountVectorizer()\n",
    "\n",
    "#Fit and transform with training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "#Transform the testing data\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3064, 16825)\n",
      "(1022, 16825)\n"
     ]
    }
   ],
   "source": [
    "#Vectorized data shapes\n",
    "\n",
    "print(X_train_dtm.shape)\n",
    "print(X_test_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '00a', '00am', '00pm', '01', '02', '03', '03342', '04', '05', '06', '07', '09', '0buxoc0crqjpvkezo3bqog', '0l', '10', '100', '1000', '1000x', '1001', '100th', '101', '102', '105', '1070', '108', '10am', '10ish', '10min', '10mins', '10minutes', '10pm', '10th', '10x', '11', '110', '1100', '111', '111th', '112', '115th', '118', '11a', '11am', '11p', '11pm', '12', '120', '128i']\n"
     ]
    }
   ],
   "source": [
    "# first 50 features\n",
    "# X_train_dtm.toarray()[:50]\n",
    "\n",
    "print(vect.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vect.max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scollops', 'scone', 'scones', 'scoop', 'scooped', 'scoops', 'scooter', 'scope', 'scorching', 'score', 'scored', 'scorpion', 'scorpions', 'scotch', 'scothes', 'scott', 'scottsadale', 'scottsdale', 'scottsdaley', 'scottsdalian', 'scottsdalish', 'scout', 'scow', 'scowl', 'scraggly', 'scramble', 'scrambled', 'scrambler', 'scramblers', 'scrambles', 'scrambling', 'scraped', 'scraping', 'scraps', 'scrapy', 'scratch', 'scratched', 'scratcher', 'scream', 'screamed', 'screaming', 'screams', 'screen', 'screening', 'screens', 'screw', 'screwed', 'screwing', 'screws', 'scrounge']\n"
     ]
    }
   ],
   "source": [
    "# Random selection of 50 features\n",
    "print(vect.get_feature_names()[13000:13050])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put it in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00a</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>03342</th>\n",
       "      <th>04</th>\n",
       "      <th>...</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zzed</th>\n",
       "      <th>clairs</th>\n",
       "      <th>cole</th>\n",
       "      <th>m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3036</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3042</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3043</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3046</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3047</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3050</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3054</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3064 rows  16825 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      00  000  00a  00am  00pm  01  02  03  03342  04 ...  zucchini  zuchinni  \\\n",
       "0      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "1      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "2      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "4      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "5      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "6      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "7      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "8      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "9      0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "10     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "11     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "12     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "13     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "14     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "15     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "16     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "17     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "18     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "19     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "20     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "21     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "22     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "23     0    0    0     1     1   0   0   0      0   0 ...         0         0   \n",
       "24     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "25     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "26     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "27     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "28     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "29     0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "...   ..  ...  ...   ...   ...  ..  ..  ..    ...  .. ...       ...       ...   \n",
       "3034   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3035   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3036   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3037   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3038   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3039   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3040   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3041   0    0    0     0     0   0   0   0      0   0 ...         1         0   \n",
       "3042   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3043   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3044   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3045   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3046   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3047   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3048   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3049   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3050   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3051   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3052   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3053   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3054   4    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3055   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3056   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3057   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3058   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3059   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3060   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3061   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3062   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "3063   0    0    0     0     0   0   0   0      0   0 ...         0         0   \n",
       "\n",
       "      zumba  zupa  zuzu  zwiebel  zzed  clairs  cole  m  \n",
       "0         0     0     0        0     0        0      0   0  \n",
       "1         0     0     0        0     0        0      0   0  \n",
       "2         0     0     0        0     0        0      0   0  \n",
       "3         0     0     0        0     0        0      0   0  \n",
       "4         0     0     0        0     0        0      0   0  \n",
       "5         0     0     0        0     0        0      0   0  \n",
       "6         0     0     0        0     0        0      0   0  \n",
       "7         0     0     0        0     0        0      0   0  \n",
       "8         0     0     0        0     0        0      0   0  \n",
       "9         0     0     0        0     0        0      0   0  \n",
       "10        0     0     0        0     0        0      0   0  \n",
       "11        0     0     0        0     0        0      0   0  \n",
       "12        0     0     0        0     0        0      0   0  \n",
       "13        0     0     0        0     0        0      0   0  \n",
       "14        0     0     0        0     0        0      0   0  \n",
       "15        0     0     0        0     0        0      0   0  \n",
       "16        0     0     0        0     0        0      0   0  \n",
       "17        0     0     0        0     0        0      0   0  \n",
       "18        0     0     0        0     0        0      0   0  \n",
       "19        0     0     0        0     0        0      0   0  \n",
       "20        0     0     0        0     0        0      0   0  \n",
       "21        0     0     0        0     0        0      0   0  \n",
       "22        0     0     0        0     0        0      0   0  \n",
       "23        0     0     0        0     0        0      0   0  \n",
       "24        0     0     0        0     0        0      0   0  \n",
       "25        0     0     0        0     0        0      0   0  \n",
       "26        0     0     0        0     0        0      0   0  \n",
       "27        0     0     0        0     0        0      0   0  \n",
       "28        0     0     0        0     0        0      0   0  \n",
       "29        0     0     0        0     0        0      0   0  \n",
       "...     ...   ...   ...      ...   ...      ...    ...  ..  \n",
       "3034      0     0     0        0     0        0      0   0  \n",
       "3035      0     0     0        0     0        0      0   0  \n",
       "3036      0     0     0        0     0        0      0   0  \n",
       "3037      0     0     0        0     0        0      0   0  \n",
       "3038      0     0     0        0     0        0      0   0  \n",
       "3039      0     0     0        0     0        0      0   0  \n",
       "3040      0     0     0        0     0        0      0   0  \n",
       "3041      0     0     0        0     0        0      0   0  \n",
       "3042      0     0     0        0     0        0      0   0  \n",
       "3043      0     0     0        0     0        0      0   0  \n",
       "3044      0     0     0        0     0        0      0   0  \n",
       "3045      0     0     0        0     0        0      0   0  \n",
       "3046      0     0     0        0     0        0      0   0  \n",
       "3047      0     0     0        0     0        0      0   0  \n",
       "3048      0     0     0        0     0        0      0   0  \n",
       "3049      0     0     0        0     0        0      0   0  \n",
       "3050      0     0     0        0     0        0      0   0  \n",
       "3051      0     0     0        0     0        0      0   0  \n",
       "3052      0     0     0        0     0        0      0   0  \n",
       "3053      0     0     0        0     0        0      0   0  \n",
       "3054      0     0     0        0     0        0      0   0  \n",
       "3055      0     0     0        0     0        0      0   0  \n",
       "3056      0     0     0        0     0        0      0   0  \n",
       "3057      0     0     0        0     0        0      0   0  \n",
       "3058      0     0     0        0     0        0      0   0  \n",
       "3059      0     0     0        0     0        0      0   0  \n",
       "3060      0     0     0        0     0        0      0   0  \n",
       "3061      0     0     0        0     0        0      0   0  \n",
       "3062      0     0     0        0     0        0      0   0  \n",
       "3063      0     0     0        0     0        0      0   0  \n",
       "\n",
       "[3064 rows x 16825 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train_dtm.toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to configure vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show vectorizer options\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **lowercase:** boolean, True by default\n",
    "- Convert all characters to lowercase before tokenizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 20838)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a count vectorizer that doesn't lowercase the words\n",
    "vect = CountVectorizer(lowercase=False)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape # has more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n)\n",
    "- The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 439573)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect = CountVectorizer(ngram_range=(2,3))\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00 25', '00 25 00', '00 am', '00 am like', '00 am on', '00 am they', '00 amazing', '00 amazing appetizer', '00 and', '00 and dinner', '00 and that', '00 and the', '00 and they', '00 at', '00 at 30', '00 before', '00 before tip', '00 bill', '00 bill they', '00 can', '00 can wait', '00 charge', '00 charge but', '00 cover', '00 cover charge', '00 credit', '00 credit for', '00 did', '00 did not', '00 did say', '00 each', '00 each they', '00 entree', '00 entree everything', '00 etc', '00 etc was', '00 everyday', '00 everyday except', '00 extra', '00 extra of', '00 food', '00 food drinks', '00 for', '00 for 24', '00 for 50', '00 for all', '00 for glass', '00 for lb', '00 for mediocre', '00 for show']\n"
     ]
    }
   ],
   "source": [
    "# last 50 features\n",
    "print(vect.get_feature_names()[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "- If 'english', a built-in stop word list for English is used.\n",
    "- If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. This allows you use to use your own custom stopwords list. Great for corpus-specific stopwords, that words that aren't regular stopwords but become stopwords depending on the context.\n",
    "- If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 16528)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with stop_words to english\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset({'therein', 'without', 'cant', 'also', 'yours', 'than', 'forty', 'there', 'something', 'seeming', 'everyone', 'another', 'these', 'find', 'wherever', 'hasnt', 'what', 'your', 'all', 'those', 'sincere', 'at', 'full', 'go', 'six', 'detail', 'will', 'before', 'meanwhile', 'again', 'never', 'own', 'became', 'she', 'nine', 'thick', 'third', 'as', 'already', 'get', 'keep', 'into', 'sixty', 'enough', 'noone', 'still', 'among', 'rather', 're', 'such', 'me', 'while', 'formerly', 'you', 'should', 'somewhere', 'empty', 'him', 'cry', 'nevertheless', 'then', 'beside', 'he', 'though', 'however', 'top', 'fill', 'whither', 'that', 'yet', 'often', 'might', 'found', 'amongst', 'was', 'afterwards', 'were', 'its', 'throughout', 'now', 'un', 'anywhere', 'mostly', 'anything', 'couldnt', 'always', 'ten', 'amount', 'behind', 'de', 'himself', 'this', 'when', 'after', 'whom', 'with', 'wherein', 'alone', 'in', 'anyway', 'across', 'myself', 'further', 'along', 'for', 'hereby', 'anyone', 'how', 'mine', 'why', 'her', 'call', 'per', 'seems', 'from', 'see', 'done', 'hers', 'anyhow', 'first', 'name', 'well', 'our', 'ours', 'some', 'move', 'if', 'latter', 'between', 'please', 'give', 'every', 'or', 'amoungst', 'latterly', 'seem', 'three', 'towards', 'describe', 'although', 'been', 'it', 'due', 'thence', 'sometimes', 'together', 'herself', 'part', 'whereby', 'whereafter', 'least', 'above', 'namely', 'few', 'everywhere', 'ltd', 'up', 'perhaps', 'thru', 'become', 'much', 'beforehand', 'fifteen', 'system', 'upon', 'thin', 'any', 'same', 'serious', 'else', 'put', 'the', 'out', 'them', 'itself', 'do', 'over', 'until', 'mill', 'are', 'hereafter', 'hereupon', 'i', 'side', 'almost', 'bill', 'indeed', 'otherwise', 'whatever', 'etc', 'several', 'has', 'too', 'thereupon', 'toward', 'down', 'we', 'whose', 'here', 'more', 'not', 'a', 'yourselves', 'take', 'by', 'below', 'nowhere', 'be', 'ourselves', 'to', 'whether', 'whoever', 'where', 'an', 'less', 'front', 'last', 'us', 'whereupon', 'none', 'their', 'former', 'his', 'since', 'eight', 'twenty', 'thereby', 'whenever', 'within', 'during', 'either', 'of', 'becomes', 'each', 'about', 'whereas', 'seemed', 'against', 'cannot', 'under', 'becoming', 'hundred', 'except', 'can', 'moreover', 'via', 'once', 'had', 'only', 'five', 'most', 'through', 'whence', 'very', 'back', 'nothing', 'bottom', 'even', 'may', 'my', 'themselves', 'others', 'besides', 'interest', 'so', 'four', 'would', 'being', 'many', 'someone', 'yourself', 'two', 'which', 'sometime', 'elsewhere', 'on', 'fire', 'one', 'somehow', 'who', 'thereafter', 'but', 'herein', 'they', 'twelve', 'am', 'eleven', 'both', 'con', 'nor', 'show', 'because', 'have', 'fifty', 'beyond', 'must', 'next', 'and', 'no', 'neither', 'other', 'inc', 'onto', 'everything', 'is', 'eg', 'ever', 'ie', 'made', 'hence', 'around', 'nobody', 'could', 'thus', 'therefore', 'co', 'whole', 'off'})\n"
     ]
    }
   ],
   "source": [
    "#Show the stopwords used\n",
    "print(vect.get_stop_words())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_features:** int or None, default=None\n",
    "- If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "- When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 2000)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with max_features to 2000\n",
    "vect = CountVectorizer(max_features=2000)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 4340)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with min_df to 5\n",
    "vect = CountVectorizer(min_df=5)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 3)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=1000, stop_words='english')\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['food', 'great', 'place']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1661, 1570, 1870], dtype=int64)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dtm.toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect.max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 128)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set vectorizer with min_df to 0.1\n",
    "vect = CountVectorizer(min_df=0.1)\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'after', 'all', 'also', 'always', 'am', 'amazing', 'an', 'and', 'any', 'are', 'as', 'at', 'back', 'be', 'because', 'been', 'best', 'better', 'but', 'by', 'can', 'could', 'delicious', 'did', 'do', 'don', 'eat', 'even', 'ever', 'every', 'everything', 'experience', 'few', 'first', 'food', 'for', 'fresh', 'friendly', 'from', 'get', 'go', 'going', 'good', 'got', 'great', 'had', 'has', 'have', 'he', 'here', 'how', 'if', 'in', 'is', 'it', 'just', 'know', 'like', 'little', 'love', 'made', 'make', 'me', 'menu', 'more', 'much', 'my', 'never', 'nice', 'no', 'not', 'now', 'of', 'off', 'on', 'one', 'only', 'or', 'order', 'ordered', 'other', 'our', 'out', 'over', 'people', 'place', 'really', 'restaurant', 'right', 'say', 'service', 'so', 'some', 'staff', 'take', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'they', 'this', 'time', 'to', 'too', 'try', 'up', 'us', 've', 'very', 'was', 'way', 'we', 'well', 'went', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'would', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "#What are the words that show up in at least 10 percent of documents\n",
    "\n",
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What:** Computes \"relative frequency\" that a word appears in a document compared to its frequency across all documents\n",
    "- **Why:** More useful than \"term frequency\" for identifying \"important\" words in each document (high frequency in that document, low frequency in other documents). Court, ball, shooting, passing will show up frequently in a basketball corpus, but essentially add no meaning. Corpus-specific stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source: Ultra Violet Analytics](http://www.ultravioletanalytics.com/2016/11/18/tf-idf-basics-with-pandas-scikit-learn/)\n",
    "\n",
    "\"Tf-idf is a very common technique for determining roughly what each document in a set of documents is about. It cleverly accomplishes this by looking at two simple metrics: tf (term frequency) and idf (inverse document frequency). Term frequency is the proportion of occurrences of a specific term to total number of terms in a document. Inverse document frequency is the inverse of the proportion of documents that contain that word/phrase. The general idea is that if a specific phrase appears a lot of times in a given document, but it doesnt appear in many other documents, then we have a good idea that the phrase is important in distinguishing that document from all the others.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example documents\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency with CountVectorizer\n",
    "\n",
    "# vect = CountVectorizer(stop_words='english')\n",
    "vect = CountVectorizer()\n",
    "tf = pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary = True assigns a 1 if a word is present irregardless of count, and 0 for absent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2 1 1 1]\n",
      "[[1 3 2 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    1     3   2       1        1    1"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize vectorizer with binary = true\n",
    "vect = CountVectorizer(binary=True)\n",
    "\n",
    "#Fit and transform the text and sum up the counts\n",
    "df= vect.fit_transform(simple_train).toarray().sum(axis=0)\n",
    "\n",
    "print(df)\n",
    "\n",
    "print(df.reshape(1,6))\n",
    "\n",
    "#Put results into dataframe\n",
    "pd.DataFrame(df.reshape(1,6), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many documents each word appears in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF (simple version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide tf by df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab      call   me  please  tonight  you\n",
       "0  0.0  0.333333  0.0     0.0      1.0  1.0\n",
       "1  1.0  0.333333  0.5     0.0      0.0  0.0\n",
       "2  0.0  0.333333  0.5     2.0      0.0  0.0"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf/df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the sklearn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652491</td>\n",
       "      <td>0.652491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266075</td>\n",
       "      <td>0.342620</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cab      call        me    please   tonight       you\n",
       "0  0.000000  0.385372  0.000000  0.000000  0.652491  0.652491\n",
       "1  0.720333  0.425441  0.547832  0.000000  0.000000  0.000000\n",
       "2  0.000000  0.266075  0.342620  0.901008  0.000000  0.000000"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize vectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "#Fit and transform using tfidf and input results into dataframe\n",
    "pd.DataFrame(vect.fit_transform(simple_train).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem covers the probabilistic relationship between multiple variables, and specifically allows us to define one conditional in terms of the underlying probabilities and the inverse condition. Specifically, it can be defined as:\n",
    "\n",
    "$$P(y|x) = P(y)P(x|y)/P(x)$$\n",
    "\n",
    "This means the probability of y given x condition equals the probability of y times the probability of x given y condition divided by the probability of x.\n",
    "\n",
    "This theorem can be extended to when x is a vector (containing the multiple x variables used as inputs for the model) to:\n",
    "\n",
    "$$P(y|x_1,...,x_n) = P(y)P(x_1,...,x_n|y)/P(x_1,...,x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretend we have an email with three words: \"Send money now.\" We'll use Naive Bayes to classify it as **ham or spam.**\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) = \\frac {P(\\text{send money now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "By assuming that the features (the words) are **conditionally independent**, we can simplify the likelihood function:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {P(\\text{send} \\ | \\ spam) \\times P(\\text{money} \\ | \\ spam) \\times P(\\text{now} \\ | \\ spam) \\times P(spam)} {P(\\text{send money now})}$$\n",
    "\n",
    "We can calculate all of the values in the numerator by examining a corpus of **spam email**:\n",
    "\n",
    "$$P(spam \\ | \\ \\text{send money now}) \\approx \\frac {0.2 \\times 0.1 \\times 0.1 \\times 0.9} {P(\\text{send money now})} = \\frac {0.0018} {P(\\text{send money now})}$$\n",
    "\n",
    "We would repeat this process with a corpus of **ham email**:\n",
    "\n",
    "$$P(ham \\ | \\ \\text{send money now}) \\approx \\frac {0.05 \\times 0.01 \\times 0.1 \\times 0.1} {P(\\text{send money now})} = \\frac {0.000005} {P(\\text{send money now})}$$\n",
    "\n",
    "All we care about is whether spam or ham has the **higher probability**, and so we predict that the email is **spam**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "\n",
    "- The **\"naive\" assumption** of Naive Bayes (that the features are conditionally independent) is critical to making these calculations simple.\n",
    "- The **normalization constant** (the denominator) can be ignored since it's the same for all classes.\n",
    "- The **prior probability** is much less relevant once you have a lot of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pros</b>: \n",
    "- Very fast. Adept at handling tens of thousands of features which is why it's used for text classification\n",
    "- Works well with a small number of observations\n",
    "- Isn't negatively affected by \"noise\"\n",
    "\n",
    "<b>Cons</b>:\n",
    "- Useless for probabilities. Most of the time assigns probabilites that are close to zero or one\n",
    "- It is literally \"naive\". Meaning it assumes features are independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the whole corpus. Remove stop words.\n",
    "\n",
    "#Intialize vectorizer\n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "#fit and transform data\n",
    "X_dtm = vect.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747919725893294"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "\n",
    "#Fit and score model\n",
    "nb.fit(X_dtm, y)\n",
    "\n",
    "nb.score(X_dtm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but let's try it on a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    0.816691\n",
       "1    0.183309\n",
       "Name: stars, dtype: float64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null accuracy of testing set\n",
    "\n",
    "y.value_counts()\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758485639686684\n",
      "0.9158512720156555\n"
     ]
    }
   ],
   "source": [
    "#Intialize vectorizer \n",
    "vect = CountVectorizer(stop_words='english')\n",
    "\n",
    "#Fit and transform on the training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "#Transform the testing data witht the vectorizer\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "#Fit it on training data\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Score it on training and testing data\n",
    "print(nb.score(X_train_dtm, y_train))\n",
    "print(nb.score(X_test_dtm, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you assess this model? \n",
    "\n",
    "<br>\n",
    "\n",
    "Let's try it on some new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[[0.03322787 0.96677213]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on new text\n",
    "new_text = [\"I had a decent time at this restaurant. \\\n",
    "The food was delicious but the service was very poor. \\\n",
    "I recommend the salad but do not eat the french fries.\"]\n",
    "new_text_transform = vect.transform(new_text)\n",
    "\n",
    "#Predict class\n",
    "print(nb.predict(new_text_transform))\n",
    "\n",
    "#Class probabilities\n",
    "print(nb.predict_proba(new_text_transform))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again with the tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8204960835509139\n",
      "0.8199608610567515\n"
     ]
    }
   ],
   "source": [
    "#Intialize vectorizer \n",
    "vect = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Fit and transform on the training data\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "#Transform the testing data witht the vectorizer\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "\n",
    "#Intialize model\n",
    "nb = MultinomialNB()\n",
    "#Fit it on training data\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "\n",
    "#Score it on training and testing data\n",
    "print(nb.score(X_train_dtm, y_train))\n",
    "print(nb.score(X_test_dtm, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on the results? Did you expect the scores to be lower than the Countvectorizer ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cross validate with pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171812425011602"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create pipeline with tfidf vectorizer with max_features = 1000 and lowercase = true\n",
    "\n",
    "# pipe = make_pipeline(TfidfVectorizer(max_features=1000, lowercase=False), MultinomialNB())\n",
    "\n",
    "pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# pipe\n",
    "\n",
    "#Cross validate with the pipeline and use the full raw text\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search time. We could spend a whole bunch of time testing various combinations of parameters, so instead of doing that, let's use grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for countvectorizer and naive bayes model\n",
    "pipe_cv = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for count vectorizer\n",
    "param_grid_cv = {}\n",
    "param_grid_cv['countvectorizer__max_features']=[1000, 2500, 5000, 7500, 1000]\n",
    "param_grid_cv['countvectorizer__lowercase'] = [True, False]\n",
    "param_grid_cv['countvectorizer__ngram_range']=[(1,1),(2,2), (1,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make pipeline for tfidfvectorizer and naive bayes model\n",
    "pipe_tf = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "#Intialize parameters for tfidf vectorizer\n",
    "param_grid_tf = {}\n",
    "param_grid_tf['tfidfvectorizer__max_features']=[1000, 2500, 5000, 7500, 10000]\n",
    "param_grid_tf['tfidfvectorizer__lowercase']=[True, False]\n",
    "param_grid_tf['tfidfvectorizer__ngram_range']= [(1,1),(1,2),(2,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's import time to see how long it takes\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132.38375997543335\n"
     ]
    }
   ],
   "source": [
    "#Grid search for the count vectorizer\n",
    "\n",
    "grid_cv = GridSearchCV(pipe_cv, param_grid_cv, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_cv.fit(X, y)\n",
    "#Print time elapsed\n",
    "print( time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__lowercase': True,\n",
       " 'countvectorizer__max_features': 7500,\n",
       " 'countvectorizer__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters\n",
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9319627998042095"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133.71612787246704\n"
     ]
    }
   ],
   "source": [
    "#Grid search for the tfidf vectorizer\n",
    "grid_tf = GridSearchCV(pipe_tf, param_grid_tf, cv=5, scoring='accuracy')\n",
    "\n",
    "#intialize time stamp\n",
    "t = time()\n",
    "#fit grid search object\n",
    "grid_tf.fit(X, y)\n",
    "#Print time elapsed\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__lowercase': True,\n",
       " 'tfidfvectorizer__max_features': 2500,\n",
       " 'tfidfvectorizer__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best parameters\n",
    "grid_tf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8773861967694567"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Best score\n",
    "grid_tf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Search option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.75383806228638\n"
     ]
    }
   ],
   "source": [
    "#Intialize randomized grid search\n",
    "randsearch_cv = RandomizedSearchCV(pipe_cv, param_grid_cv, cv=5, scoring='accuracy')\n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "randsearch_cv.fit(X,y)\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the \"spaminess\" of a token\n",
    "\n",
    "This is a really helpful technique to find the words most associated with either class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                       message  \n",
       "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                                Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n",
       "3                                                                                                            U dun say so early hor... U c already then say...  \n",
       "4                                                                                                Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in ham or spam text dataset\n",
    "df = pd.read_table(\"../../data/NLP_data/sms.tsv\",encoding=\"utf-8\", names= [\"label\", \"message\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at null accuracy\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935391241923905"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign X and y\n",
    "X = df.message\n",
    "y = df.label\n",
    "\n",
    "#Intialize vectorizer with default settings\n",
    "vect = CountVectorizer()\n",
    "#Fit and transform X\n",
    "Xdtm = vect.fit_transform(X)\n",
    "#Intialize, fit, and score model on training data\n",
    "nb = MultinomialNB()\n",
    "nb.fit(Xdtm,y)\n",
    "nb.score(Xdtm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8713"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Assign list of features to tokens variable\n",
    "tokens = vect.get_feature_names()\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['account', 'accounting', 'accounts', 'accumulation', 'achan', 'ache', 'achieve', 'acid', 'acknowledgement', 'acl03530150pm', 'acnt', 'aco', 'across', 'act', 'acted', 'actin', 'acting', 'action', 'activ8', 'activate', 'active', 'activities', 'actor', 'actual', 'actually', 'ad', 'adam', 'add', 'addamsfa', 'added', 'addicted', 'addie', 'adding', 'address', 'adds', 'adewale', 'adi', 'adjustable', 'admin', 'administrator', 'admirer', 'admission', 'admit', 'adore', 'adoring', 'adp', 'adress', 'adrian', 'adrink', 'ads', 'adsense', 'adult', 'adults', 'advance', 'adventure', 'adventuring', 'advice', 'advise', 'advising', 'advisors', 'aeronautics', 'aeroplane', 'afew', 'affair', 'affairs', 'affection', 'affectionate', 'affections', 'affidavit', 'afford', 'afghanistan', 'afraid', 'africa', 'african', 'aft', 'after', 'afternon', 'afternoon', 'afternoons', 'afterwards']\n"
     ]
    }
   ],
   "source": [
    "#Print random slice of features\n",
    "print(tokens[900:980])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1., ...,  1.,  0.,  1.],\n",
       "       [10., 29.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many times does a word appear in each class\n",
    "nb.feature_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 8713)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape\n",
    "nb.feature_count_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns out counts of each word in documents marked \"ham\"\n",
    "ham_token_count = nb.feature_count_[0]\n",
    "ham_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10., 29.,  0., ...,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns out counts of each word in documents marked \"spam\"\n",
    "spam_token_count = nb.feature_count_[1]\n",
    "spam_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>weddin</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gautham</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salmon</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live</th>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>memories</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aproach</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37819</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>algarve</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versus</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ham  spam\n",
       "token               \n",
       "weddin     1.0   0.0\n",
       "gautham    3.0   0.0\n",
       "lambda     1.0   0.0\n",
       "salmon     1.0   0.0\n",
       "live      17.0  29.0\n",
       "memories   1.0   0.0\n",
       "aproach    2.0   0.0\n",
       "37819      0.0   1.0\n",
       "algarve    0.0   2.0\n",
       "versus     1.0   0.0"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "df_tokens = pd.DataFrame({'token':tokens, \n",
    "                          'ham':ham_token_count, \n",
    "                          'spam':spam_token_count}).set_index('token')\n",
    "\n",
    "#Randomly data \n",
    "df_tokens.sample(10, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "df_tokens['ham'] = df_tokens.ham+1\n",
    "df_tokens['spam'] = df_tokens.spam+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token\n",
       "00              1.0\n",
       "000             1.0\n",
       "000pes          2.0\n",
       "008704050406    1.0\n",
       "0089            1.0\n",
       "Name: ham, dtype: float64"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.ham.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4825.,  747.])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the ham and spam counts into frequencies\n",
    "df_tokens['ham'] = df_tokens.ham/nb.class_count_[0]\n",
    "df_tokens['spam'] = df_tokens.spam/nb.class_count_[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ratio of spam-to-ham for each token\n",
    "df_tokens['spam_ratio'] = df_tokens.spam/df_tokens.ham\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "token\n",
       "00               71.050870\n",
       "000             193.775100\n",
       "000pes            3.229585\n",
       "008704050406     19.377510\n",
       "0089             12.918340\n",
       "Name: spam_ratio, dtype: float64"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokens.spam_ratio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gt</th>\n",
       "      <td>0.066114</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.020248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0.065699</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.020376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.048083</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.027841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>she</th>\n",
       "      <td>0.034819</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.038447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lor</th>\n",
       "      <td>0.033782</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.039627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0.031295</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.042776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>later</th>\n",
       "      <td>0.028187</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.047494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask</th>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.071769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.018653</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.071769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doing</th>\n",
       "      <td>0.018446</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.072575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ham      spam  spam_ratio\n",
       "token                                \n",
       "gt     0.066114  0.001339    0.020248\n",
       "lt     0.065699  0.001339    0.020376\n",
       "he     0.048083  0.001339    0.027841\n",
       "she    0.034819  0.001339    0.038447\n",
       "lor    0.033782  0.001339    0.039627\n",
       "da     0.031295  0.001339    0.042776\n",
       "later  0.028187  0.001339    0.047494\n",
       "ask    0.018653  0.001339    0.071769\n",
       "said   0.018653  0.001339    0.071769\n",
       "doing  0.018446  0.001339    0.072575"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "df_tokens.sort_values(ascending=True, by='spam_ratio').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, the top ten \"spammiest\" words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Tokenization:\n",
    "- http://text-processing.com/demo/tokenize/\n",
    "- https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "\n",
    "\n",
    "POS tagging:\n",
    "- https://nlp.stanford.edu/software/tagger.shtml\n",
    "- http://language.worldofcomputing.net/pos-tagging/parts-of-speech-tagging.html\n",
    "- https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/\n",
    "\n",
    "NLTK:\n",
    "- https://likegeeks.com/nlp-tutorial-using-python-nltk/\n",
    "- http://billchambers.me/tutorials/2015/01/14/python-nlp-cheatsheet-nltk-scikit-learn.html\n",
    "\n",
    "TextBlob:\n",
    "- http://textblob.readthedocs.io/en/dev/quickstart.html\n",
    "- http://rwet.decontextualize.com/book/textblob/\n",
    "- http://text-analytics101.rxnlp.com/2014/11/what-are-n-grams.html\n",
    "\n",
    "Stemming and Lemmatization:\n",
    "- http://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization\n",
    "- https://stackoverflow.com/questions/1787110/what-is-the-true-difference-between-lemmatization-vs-stemming\n",
    "\n",
    "Vectorizating Text:\n",
    "- https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/\n",
    "- http://planspace.org/20150524-tfidf_is_about_what_matters/\n",
    "- http://www.tfidf.com/\n",
    "- http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "\n",
    "Text classification:\n",
    "- https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "- https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "- https://www.dataquest.io/blog/natural-language-processing-with-python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab time\n",
    "- There are three other datasets pitchfork album reviews, fake/real news, and political lean.\n",
    "- Pick one of those three datasets and try to build a model that differentiate between good/bad review, real/fake news, or liberal/conservative leaning. Make sure to examine the false positives and the false negatives texts. Use the \"spamminess\" technique on the corpus as well. \n",
    "- Use both count and tfidf vectorizers. Use textblob to determine sentiment and polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
